{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "9779e5d5",
      "metadata": {
        "id": "9779e5d5"
      },
      "source": [
        "<div align=center>\n",
        "\n",
        "<font size=5>\n",
        "    In the Name of God\n",
        "<font/>\n",
        "<br/>\n",
        "<br/>\n",
        "<font>\n",
        "    Sharif University of Technology - Departmenet of Electrical Engineering\n",
        "</font>\n",
        "<br/>\n",
        "<font>\n",
        "    Introducing with Machine Learing - Dr. S. Amini\n",
        "</font>\n",
        "<br/>\n",
        "<br/>\n",
        "Spring 2023\n",
        "\n",
        "</div>\n",
        "\n",
        "<hr/>\n",
        "<div align=center>\n",
        "<font size=6>\n",
        "    Neural Networks Practical Assignment\n",
        "    \n",
        "    Question 2\n",
        "</font>\n",
        "<br/>\n",
        "<font size=4>\n",
        "<br/>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d55ee53",
      "metadata": {
        "id": "1d55ee53"
      },
      "source": [
        "# Personal Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40e950d8",
      "metadata": {
        "id": "40e950d8"
      },
      "outputs": [],
      "source": [
        "# Set your student number\n",
        "student_number = 98102165\n",
        "Name = 'Mohammad Amin'\n",
        "Last_Name = 'Keivanrad'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2edb1e44",
      "metadata": {
        "id": "2edb1e44"
      },
      "source": [
        "# Rules\n",
        "- You are not allowed to add or remove cells. You **must use the provided space to write your code**. If you don't follow this rule, **your Practical Assignment won't be graded**.  \n",
        "\n",
        "- Collaboration and using the internet is allowed, but your code **must be written by yourself**. **Copying code** from each other or from available resources will result in a **zero score for the assignment**.\n",
        "\n",
        "- In this question, you are allowed to use `torch.optim`, `torch.nn` or any other submodule you need.\n",
        "- You **are not allowed** to use the models already implemented or the pretrained models in torch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a15a195",
      "metadata": {
        "id": "7a15a195",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "535a5844-518c-4ca3-c67a-506f861a387c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.22.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.40.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.4)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (8.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.15.2+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.27.1)\n",
            "Requirement already satisfied: torch==2.0.1 in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.0.1+cu118)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (8.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision) (4.6.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1->torchvision) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1->torchvision) (16.0.6)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.1->torchvision) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.1->torchvision) (1.3.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.6.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install numpy\n",
        "!pip install matplotlib\n",
        "!pip install torchvision\n",
        "!pip install torch"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "85c3b02e",
      "metadata": {
        "id": "85c3b02e"
      },
      "source": [
        "## Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6246005",
      "metadata": {
        "id": "b6246005"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchvision.datasets import FashionMNIST\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from typing import Dict\n",
        "import torchvision"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c89754b",
      "metadata": {
        "id": "2c89754b"
      },
      "source": [
        "## Datasets and Dataloaders\n",
        "\n",
        "You should load the train and test set of the `CIFAR10` dataset and apply the desired transforms on it. Then, you should create the loaders for these `test` and `train` sets.\n",
        "- **Hint:** You can use `torchvision.datasets` to easily load the CIFAR10 dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40e10458",
      "metadata": {
        "id": "40e10458",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4ca6231-8176-4daa-d0e0-d12c5f65b227"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:03<00:00, 47771555.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "## FILL HERE\n",
        "batch_size = 5\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,\n",
        "                                          shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size,\n",
        "                                         shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_dataset.classes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7u31ruNOcgsx",
        "outputId": "6b8ab89b-9451-4d03-a3b6-268b2057891b"
      },
      "id": "7u31ruNOcgsx",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ee9a3be8",
      "metadata": {
        "id": "ee9a3be8"
      },
      "source": [
        "## Visualization\n",
        "\n",
        "Visualize 1 random image from each class\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1b039d1",
      "metadata": {
        "id": "a1b039d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a334e190-8677-4508-d8d3-6bf9fe7df284"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 500x5000 with 10 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "## FILL HERE\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "classes = train_dataset.classes\n",
        "\n",
        "random_images = {}\n",
        "\n",
        "for data, target in train_dataset:\n",
        "    if classes[target] not in random_images:\n",
        "        random_images[classes[target]] = data\n",
        "\n",
        "fig, axs = plt.subplots(nrows=len(random_images), figsize=(5, 5*len(random_images)))\n",
        "\n",
        "for i, (class_name, image) in enumerate(random_images.items()):\n",
        "    axs[i].imshow(image.permute(2, 1, 0).numpy())\n",
        "    axs[i].set_title(class_name)\n",
        "    axs[i].axis('off')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1221642e",
      "metadata": {
        "id": "1221642e"
      },
      "source": [
        "## Defining a Convolutional Neural Network\n",
        "\n",
        "In this section, you should define a class called `Net` which represents the model for your convolutional neural network.\n",
        "- **Hint:** You can use `torch.nn` to add the required layers to your model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ba6a266",
      "metadata": {
        "id": "0ba6a266"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        ## FILL HERE\n",
        "        self.conv1 = nn.Conv2d(3, 16, 3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(32, 64, 3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(64 * 4 * 4, 512)\n",
        "        self.fc2 = nn.Linear(512, 64)\n",
        "        self.fc3 = nn.Linear(64, 10)\n",
        "        self.dropout = nn.Dropout(p=.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = self.pool(F.relu(self.conv3(x)))\n",
        "        x = x.view(-1, 64 * 4 * 4)\n",
        "        x = self.dropout(F.relu(self.fc1(x)))\n",
        "        x = self.dropout(F.relu(self.fc2(x)))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "net = Net()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e3c27c9f",
      "metadata": {
        "id": "e3c27c9f"
      },
      "source": [
        "## Define a Loss function and optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8f1eadd",
      "metadata": {
        "id": "a8f1eadd"
      },
      "source": [
        "In this section, you simply define your loss function and optimizer.\n",
        "- **Hint:** You can use `torch.optim` and implemented loss functions in torch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79068600",
      "metadata": {
        "id": "79068600"
      },
      "outputs": [],
      "source": [
        "## FILL HERE\n",
        "learning_rate = 0.005\n",
        "loss = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(net.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b526ee28",
      "metadata": {
        "id": "b526ee28"
      },
      "source": [
        "## Training the model\n",
        "\n",
        "You should train your model for multiple epochs. You are free to choose the number of epochs. However, you should reach an acceptable accuracy at the end."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fbf5c8d2",
      "metadata": {
        "id": "fbf5c8d2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f92fb0f3-59bf-44c9-ff15-4a2eed95818f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "for batch5001 loss is 0.13266441226005554\n",
            "for batch5002 loss is 5.388187219068641e-06\n",
            "for batch5003 loss is 0.06601733714342117\n",
            "for batch5004 loss is 0.0008048736490309238\n",
            "for batch5005 loss is 0.0038754616398364305\n",
            "for batch5006 loss is 0.2600037157535553\n",
            "for batch5007 loss is 0.08833803981542587\n",
            "for batch5008 loss is 0.05978935956954956\n",
            "for batch5009 loss is 0.2178652286529541\n",
            "for batch5010 loss is 0.07201292365789413\n",
            "for batch5011 loss is 0.037319522351026535\n",
            "for batch5012 loss is 0.05448231101036072\n",
            "for batch5013 loss is 0.0025526750832796097\n",
            "for batch5014 loss is 0.30966442823410034\n",
            "for batch5015 loss is 0.049338165670633316\n",
            "for batch5016 loss is 0.9305360913276672\n",
            "for batch5017 loss is 1.7105776071548462\n",
            "for batch5018 loss is 0.02911512553691864\n",
            "for batch5019 loss is 0.0008293332648463547\n",
            "for batch5020 loss is 0.04019875451922417\n",
            "for batch5021 loss is 0.004202274139970541\n",
            "for batch5022 loss is 0.023973770439624786\n",
            "for batch5023 loss is 0.3559282720088959\n",
            "for batch5024 loss is 0.11269627511501312\n",
            "for batch5025 loss is 0.04352475702762604\n",
            "for batch5026 loss is 0.7097606658935547\n",
            "for batch5027 loss is 0.0021614371798932552\n",
            "for batch5028 loss is 0.5129435062408447\n",
            "for batch5029 loss is 0.9262956380844116\n",
            "for batch5030 loss is 0.003383791074156761\n",
            "for batch5031 loss is 0.00204587634652853\n",
            "for batch5032 loss is 0.006406728178262711\n",
            "for batch5033 loss is 0.08951248228549957\n",
            "for batch5034 loss is 0.0035889442078769207\n",
            "for batch5035 loss is 0.0036419432144612074\n",
            "for batch5036 loss is 0.2885860800743103\n",
            "for batch5037 loss is 0.09042831510305405\n",
            "for batch5038 loss is 0.45374250411987305\n",
            "for batch5039 loss is 0.09407033026218414\n",
            "for batch5040 loss is 0.31540799140930176\n",
            "for batch5041 loss is 0.0022163870744407177\n",
            "for batch5042 loss is 0.6430410742759705\n",
            "for batch5043 loss is 0.16300037503242493\n",
            "for batch5044 loss is 1.0013389328378253e-05\n",
            "for batch5045 loss is 0.021117156371474266\n",
            "for batch5046 loss is 0.0038100131787359715\n",
            "for batch5047 loss is 0.3044368028640747\n",
            "for batch5048 loss is 0.15043509006500244\n",
            "for batch5049 loss is 0.153152734041214\n",
            "for batch5050 loss is 0.00039735701284371316\n",
            "for batch5051 loss is 0.0021186547819525003\n",
            "for batch5052 loss is 0.11203684657812119\n",
            "for batch5053 loss is 0.0016202731058001518\n",
            "for batch5054 loss is 0.14996841549873352\n",
            "for batch5055 loss is 0.004503217525780201\n",
            "for batch5056 loss is 0.0009294120827689767\n",
            "for batch5057 loss is 0.20771567523479462\n",
            "for batch5058 loss is 0.17071731388568878\n",
            "for batch5059 loss is 0.20825842022895813\n",
            "for batch5060 loss is 0.030940279364585876\n",
            "for batch5061 loss is 0.045233968645334244\n",
            "for batch5062 loss is 0.08315230906009674\n",
            "for batch5063 loss is 0.03571035712957382\n",
            "for batch5064 loss is 0.053691327571868896\n",
            "for batch5065 loss is 0.049037910997867584\n",
            "for batch5066 loss is 0.27537351846694946\n",
            "for batch5067 loss is 0.04821380600333214\n",
            "for batch5068 loss is 0.043511856347322464\n",
            "for batch5069 loss is 0.001959011424332857\n",
            "for batch5070 loss is 1.012691617012024\n",
            "for batch5071 loss is 0.1767122745513916\n",
            "for batch5072 loss is 3.29014937960892e-06\n",
            "for batch5073 loss is 0.1908458173274994\n",
            "for batch5074 loss is 0.3067537248134613\n",
            "for batch5075 loss is 0.23868119716644287\n",
            "for batch5076 loss is 0.6237391829490662\n",
            "for batch5077 loss is 4.053096290590474e-06\n",
            "for batch5078 loss is 0.06301210820674896\n",
            "for batch5079 loss is 0.011529308743774891\n",
            "for batch5080 loss is 5.984218205412617e-06\n",
            "for batch5081 loss is 0.029919970780611038\n",
            "for batch5082 loss is 0.28913038969039917\n",
            "for batch5083 loss is 0.024818047881126404\n",
            "for batch5084 loss is 0.3305920958518982\n",
            "for batch5085 loss is 0.0006647511618211865\n",
            "for batch5086 loss is 0.14931541681289673\n",
            "for batch5087 loss is 0.3050302565097809\n",
            "for batch5088 loss is 0.11753551661968231\n",
            "for batch5089 loss is 0.11599968373775482\n",
            "for batch5090 loss is 0.41013678908348083\n",
            "for batch5091 loss is 0.5560742616653442\n",
            "for batch5092 loss is 0.0017717608716338873\n",
            "for batch5093 loss is 4.248175901011564e-05\n",
            "for batch5094 loss is 3.576276412786683e-07\n",
            "for batch5095 loss is 0.05033203214406967\n",
            "for batch5096 loss is 0.0004747650236822665\n",
            "for batch5097 loss is 0.19638630747795105\n",
            "for batch5098 loss is 0.695896327495575\n",
            "for batch5099 loss is 0.16745522618293762\n",
            "for batch5100 loss is 0.13369975984096527\n",
            "for batch5101 loss is 0.15222002565860748\n",
            "for batch5102 loss is 0.04233162850141525\n",
            "for batch5103 loss is 0.17775902152061462\n",
            "for batch5104 loss is 0.021903064101934433\n",
            "for batch5105 loss is 0.0022659183014184237\n",
            "for batch5106 loss is 0.44613200426101685\n",
            "for batch5107 loss is 0.3204987645149231\n",
            "for batch5108 loss is 0.0007193409837782383\n",
            "for batch5109 loss is 0.23799702525138855\n",
            "for batch5110 loss is 0.10188065469264984\n",
            "for batch5111 loss is 0.16460449993610382\n",
            "for batch5112 loss is 0.2059057503938675\n",
            "for batch5113 loss is 0.03767472505569458\n",
            "for batch5114 loss is 0.023412445560097694\n",
            "for batch5115 loss is 0.34381893277168274\n",
            "for batch5116 loss is 0.011653808876872063\n",
            "for batch5117 loss is 1.0464317798614502\n",
            "for batch5118 loss is 0.04696452245116234\n",
            "for batch5119 loss is 0.1346747726202011\n",
            "for batch5120 loss is 0.1532871425151825\n",
            "for batch5121 loss is 3.3758067729650065e-05\n",
            "for batch5122 loss is 0.12353622913360596\n",
            "for batch5123 loss is 0.27383333444595337\n",
            "for batch5124 loss is 0.01635970175266266\n",
            "for batch5125 loss is 0.2794519066810608\n",
            "for batch5126 loss is 2.2362622985383496e-05\n",
            "for batch5127 loss is 0.02549394965171814\n",
            "for batch5128 loss is 0.002819058019667864\n",
            "for batch5129 loss is 0.01320252101868391\n",
            "for batch5130 loss is 0.20528709888458252\n",
            "for batch5131 loss is 0.008668146096169949\n",
            "for batch5132 loss is 0.25624483823776245\n",
            "for batch5133 loss is 0.008624810725450516\n",
            "for batch5134 loss is 0.02224479243159294\n",
            "for batch5135 loss is 0.000563346198759973\n",
            "for batch5136 loss is 0.30585211515426636\n",
            "for batch5137 loss is 0.2272830754518509\n",
            "for batch5138 loss is 1.9734256267547607\n",
            "for batch5139 loss is 0.2706783711910248\n",
            "for batch5140 loss is 0.11629869043827057\n",
            "for batch5141 loss is 0.0563172921538353\n",
            "for batch5142 loss is 0.001946298056282103\n",
            "for batch5143 loss is 0.0005057848175056279\n",
            "for batch5144 loss is 0.008219381794333458\n",
            "for batch5145 loss is 0.6774517893791199\n",
            "for batch5146 loss is 0.0016955338651314378\n",
            "for batch5147 loss is 0.20410490036010742\n",
            "for batch5148 loss is 1.2153503894805908\n",
            "for batch5149 loss is 0.00022221702965907753\n",
            "for batch5150 loss is 0.2588687241077423\n",
            "for batch5151 loss is 0.07756976783275604\n",
            "for batch5152 loss is 0.018964173272252083\n",
            "for batch5153 loss is 0.0009940678719431162\n",
            "for batch5154 loss is 0.0011544942390173674\n",
            "for batch5155 loss is 0.08569493144750595\n",
            "for batch5156 loss is 0.04454178363084793\n",
            "for batch5157 loss is 0.07862908393144608\n",
            "for batch5158 loss is 0.03442489355802536\n",
            "for batch5159 loss is 0.0361381396651268\n",
            "for batch5160 loss is 0.6390222311019897\n",
            "for batch5161 loss is 0.004256739281117916\n",
            "for batch5162 loss is 0.018958551809191704\n",
            "for batch5163 loss is 0.004068118054419756\n",
            "for batch5164 loss is 0.014197571203112602\n",
            "for batch5165 loss is 0.056744419038295746\n",
            "for batch5166 loss is 0.28013044595718384\n",
            "for batch5167 loss is 0.0688280537724495\n",
            "for batch5168 loss is 0.011853300966322422\n",
            "for batch5169 loss is 0.002180594950914383\n",
            "for batch5170 loss is 0.09477515518665314\n",
            "for batch5171 loss is 0.2669227421283722\n",
            "for batch5172 loss is 0.0006559593020938337\n",
            "for batch5173 loss is 0.2286820411682129\n",
            "for batch5174 loss is 3.5762466268352e-06\n",
            "for batch5175 loss is 0.14980874955654144\n",
            "for batch5176 loss is 0.03644872084259987\n",
            "for batch5177 loss is 0.1356610655784607\n",
            "for batch5178 loss is 8.857678039930761e-05\n",
            "for batch5179 loss is 0.12911760807037354\n",
            "for batch5180 loss is 0.2905691862106323\n",
            "for batch5181 loss is 0.27857446670532227\n",
            "for batch5182 loss is 0.07832507789134979\n",
            "for batch5183 loss is 0.007368461228907108\n",
            "for batch5184 loss is 1.115013837814331\n",
            "for batch5185 loss is 0.01379074715077877\n",
            "for batch5186 loss is 0.13455374538898468\n",
            "for batch5187 loss is 0.006800954230129719\n",
            "for batch5188 loss is 0.045764677226543427\n",
            "for batch5189 loss is 0.004091067239642143\n",
            "for batch5190 loss is 0.006022328976541758\n",
            "for batch5191 loss is 0.2450789511203766\n",
            "for batch5192 loss is 0.9659616351127625\n",
            "for batch5193 loss is 0.06961876153945923\n",
            "for batch5194 loss is 0.00434973556548357\n",
            "for batch5195 loss is 0.026696324348449707\n",
            "for batch5196 loss is 0.175394207239151\n",
            "for batch5197 loss is 2.6105131837539375e-05\n",
            "for batch5198 loss is 0.00017194740939885378\n",
            "for batch5199 loss is 0.0035525269340723753\n",
            "for batch5200 loss is 0.0011740198824554682\n",
            "for batch5201 loss is 0.09405593574047089\n",
            "for batch5202 loss is 0.005064420402050018\n",
            "for batch5203 loss is 0.834638237953186\n",
            "for batch5204 loss is 0.0005288914544507861\n",
            "for batch5205 loss is 0.15582630038261414\n",
            "for batch5206 loss is 3.456770718912594e-05\n",
            "for batch5207 loss is 0.014061564579606056\n",
            "for batch5208 loss is 0.0438833087682724\n",
            "for batch5209 loss is 0.060406558215618134\n",
            "for batch5210 loss is 0.015059813857078552\n",
            "for batch5211 loss is 0.2560117840766907\n",
            "for batch5212 loss is 0.009616275317966938\n",
            "for batch5213 loss is 0.0003627409168984741\n",
            "for batch5214 loss is 0.37021511793136597\n",
            "for batch5215 loss is 0.4391269087791443\n",
            "for batch5216 loss is 0.47744959592819214\n",
            "for batch5217 loss is 0.0012905844487249851\n",
            "for batch5218 loss is 0.021268296986818314\n",
            "for batch5219 loss is 1.349403646599967e-05\n",
            "for batch5220 loss is 1.679430603981018\n",
            "for batch5221 loss is 3.468840441200882e-05\n",
            "for batch5222 loss is 0.023820599541068077\n",
            "for batch5223 loss is 0.24643683433532715\n",
            "for batch5224 loss is 0.00024222648062277585\n",
            "for batch5225 loss is 0.0002598655701149255\n",
            "for batch5226 loss is 0.12798766791820526\n",
            "for batch5227 loss is 0.9321244955062866\n",
            "for batch5228 loss is 0.008096983656287193\n",
            "for batch5229 loss is 0.3194888234138489\n",
            "for batch5230 loss is 0.011829724535346031\n",
            "for batch5231 loss is 0.04300063103437424\n",
            "for batch5232 loss is 3.504513006191701e-05\n",
            "for batch5233 loss is 0.23232798278331757\n",
            "for batch5234 loss is 0.00138255232013762\n",
            "for batch5235 loss is 0.2493763267993927\n",
            "for batch5236 loss is 0.0657254308462143\n",
            "for batch5237 loss is 0.15113213658332825\n",
            "for batch5238 loss is 0.022725099697709084\n",
            "for batch5239 loss is 0.05180706828832626\n",
            "for batch5240 loss is 0.005070147570222616\n",
            "for batch5241 loss is 0.0004124882980249822\n",
            "for batch5242 loss is 0.367611825466156\n",
            "for batch5243 loss is 0.16323800384998322\n",
            "for batch5244 loss is 7.32333428459242e-05\n",
            "for batch5245 loss is 0.40660014748573303\n",
            "for batch5246 loss is 0.19477958977222443\n",
            "for batch5247 loss is 0.184663325548172\n",
            "for batch5248 loss is 0.0008073888020589948\n",
            "for batch5249 loss is 0.055972903966903687\n",
            "for batch5250 loss is 0.011562632396817207\n",
            "for batch5251 loss is 0.008840547874569893\n",
            "for batch5252 loss is 0.7518177032470703\n",
            "for batch5253 loss is 0.5737524628639221\n",
            "for batch5254 loss is 0.7596558332443237\n",
            "for batch5255 loss is 0.0033410184551030397\n",
            "for batch5256 loss is 0.02744118496775627\n",
            "for batch5257 loss is 0.1251380741596222\n",
            "for batch5258 loss is 2.622455940581858e-05\n",
            "for batch5259 loss is 0.08960671722888947\n",
            "for batch5260 loss is 0.003939801827073097\n",
            "for batch5261 loss is 0.030229365453124046\n",
            "for batch5262 loss is 6.283850962063298e-05\n",
            "for batch5263 loss is 0.5468491315841675\n",
            "for batch5264 loss is 0.07937809079885483\n",
            "for batch5265 loss is 0.11495423316955566\n",
            "for batch5266 loss is 2.384185648907078e-08\n",
            "for batch5267 loss is 0.019367868080735207\n",
            "for batch5268 loss is 0.00043900226592086256\n",
            "for batch5269 loss is 0.00300420681014657\n",
            "for batch5270 loss is 0.16377830505371094\n",
            "for batch5271 loss is 0.07817443460226059\n",
            "for batch5272 loss is 0.14630739390850067\n",
            "for batch5273 loss is 0.0016953082522377372\n",
            "for batch5274 loss is 0.0017703030025586486\n",
            "for batch5275 loss is 0.0022874074056744576\n",
            "for batch5276 loss is 0.0003494701231829822\n",
            "for batch5277 loss is 0.16246795654296875\n",
            "for batch5278 loss is 0.03272520750761032\n",
            "for batch5279 loss is 0.0021212720312178135\n",
            "for batch5280 loss is 0.014414019882678986\n",
            "for batch5281 loss is 0.06531012058258057\n",
            "for batch5282 loss is 0.043971240520477295\n",
            "for batch5283 loss is 0.00027594994753599167\n",
            "for batch5284 loss is 0.04457993805408478\n",
            "for batch5285 loss is 0.022231686860322952\n",
            "for batch5286 loss is 2.861021641820116e-07\n",
            "for batch5287 loss is 0.07755966484546661\n",
            "for batch5288 loss is 0.00202638260088861\n",
            "for batch5289 loss is 0.3917497396469116\n",
            "for batch5290 loss is 0.07337205111980438\n",
            "for batch5291 loss is 0.15617863833904266\n",
            "for batch5292 loss is 0.00510932644829154\n",
            "for batch5293 loss is 0.00032673226087354124\n",
            "for batch5294 loss is 0.06738149374723434\n",
            "for batch5295 loss is 0.5365921854972839\n",
            "for batch5296 loss is 0.10382363945245743\n",
            "for batch5297 loss is 0.04645073786377907\n",
            "for batch5298 loss is 0.1579994559288025\n",
            "for batch5299 loss is 1.907348234908568e-07\n",
            "for batch5300 loss is 0.02595597878098488\n",
            "for batch5301 loss is 0.009509162977337837\n",
            "for batch5302 loss is 0.14203497767448425\n",
            "for batch5303 loss is 0.2203449308872223\n",
            "for batch5304 loss is 0.14659109711647034\n",
            "for batch5305 loss is 0.0004921818617731333\n",
            "for batch5306 loss is 0.0011656966526061296\n",
            "for batch5307 loss is 0.015534316189587116\n",
            "for batch5308 loss is 1.208754539489746\n",
            "for batch5309 loss is 0.03289565443992615\n",
            "for batch5310 loss is 4.327045826357789e-05\n",
            "for batch5311 loss is 1.835738839872647e-05\n",
            "for batch5312 loss is 0.22773198783397675\n",
            "for batch5313 loss is 2.66301904048305e-05\n",
            "for batch5314 loss is 0.10871545970439911\n",
            "for batch5315 loss is 0.07842426002025604\n",
            "for batch5316 loss is 2.384185648907078e-08\n",
            "for batch5317 loss is 0.0006303146365098655\n",
            "for batch5318 loss is 0.07136733829975128\n",
            "for batch5319 loss is 0.06480730324983597\n",
            "for batch5320 loss is 0.0068513317964971066\n",
            "for batch5321 loss is 0.009230919182300568\n",
            "for batch5322 loss is 0.002616551239043474\n",
            "for batch5323 loss is 0.010435102507472038\n",
            "for batch5324 loss is 0.004048504866659641\n",
            "for batch5325 loss is 0.205853670835495\n",
            "for batch5326 loss is 0.005204149521887302\n",
            "for batch5327 loss is 0.514714241027832\n",
            "for batch5328 loss is 0.007148708216845989\n",
            "for batch5329 loss is 0.011052625253796577\n",
            "for batch5330 loss is 0.05633360892534256\n",
            "for batch5331 loss is 0.4634982943534851\n",
            "for batch5332 loss is 0.09104491025209427\n",
            "for batch5333 loss is 0.9057532548904419\n",
            "for batch5334 loss is 0.07874282449483871\n",
            "for batch5335 loss is 0.010680118575692177\n",
            "for batch5336 loss is 0.0006223982200026512\n",
            "for batch5337 loss is 0.9384254217147827\n",
            "for batch5338 loss is 0.07349896430969238\n",
            "for batch5339 loss is 0.1905716061592102\n",
            "for batch5340 loss is 0.1601656973361969\n",
            "for batch5341 loss is 0.16263969242572784\n",
            "for batch5342 loss is 0.02049022726714611\n",
            "for batch5343 loss is 0.00018686370458453894\n",
            "for batch5344 loss is 0.05737666040658951\n",
            "for batch5345 loss is 0.06541411578655243\n",
            "for batch5346 loss is 0.005799414590001106\n",
            "for batch5347 loss is 0.0\n",
            "for batch5348 loss is 0.008101466111838818\n",
            "for batch5349 loss is 0.3792211413383484\n",
            "for batch5350 loss is 0.3787010610103607\n",
            "for batch5351 loss is 0.13124558329582214\n",
            "for batch5352 loss is 0.022806700319051743\n",
            "for batch5353 loss is 0.09194387495517731\n",
            "for batch5354 loss is 0.024422215297818184\n",
            "for batch5355 loss is 0.0004603105189744383\n",
            "for batch5356 loss is 0.1538453996181488\n",
            "for batch5357 loss is 0.033561259508132935\n",
            "for batch5358 loss is 0.14681626856327057\n",
            "for batch5359 loss is 0.03984016925096512\n",
            "for batch5360 loss is 0.0020483003463596106\n",
            "for batch5361 loss is 0.022086288779973984\n",
            "for batch5362 loss is 0.032079290598630905\n",
            "for batch5363 loss is 0.03258124366402626\n",
            "for batch5364 loss is 0.19013960659503937\n",
            "for batch5365 loss is 0.051413655281066895\n",
            "for batch5366 loss is 0.03170701861381531\n",
            "for batch5367 loss is 0.0034770979546010494\n",
            "for batch5368 loss is 0.029640858992934227\n",
            "for batch5369 loss is 0.14262139797210693\n",
            "for batch5370 loss is 0.006470100022852421\n",
            "for batch5371 loss is 0.0040727704763412476\n",
            "for batch5372 loss is 0.00020290531392674893\n",
            "for batch5373 loss is 0.08782888203859329\n",
            "for batch5374 loss is 0.09177567064762115\n",
            "for batch5375 loss is 0.015755917876958847\n",
            "for batch5376 loss is 7.271635695360601e-06\n",
            "for batch5377 loss is 0.001198835321702063\n",
            "for batch5378 loss is 0.009999791160225868\n",
            "for batch5379 loss is 0.25815239548683167\n",
            "for batch5380 loss is 0.06561098247766495\n",
            "for batch5381 loss is 1.9632740020751953\n",
            "for batch5382 loss is 0.29876649379730225\n",
            "for batch5383 loss is 0.07255025207996368\n",
            "for batch5384 loss is 0.13974708318710327\n",
            "for batch5385 loss is 0.043673135340213776\n",
            "for batch5386 loss is 0.0775667056441307\n",
            "for batch5387 loss is 0.010973694734275341\n",
            "for batch5388 loss is 0.004138207528740168\n",
            "for batch5389 loss is 2.299497604370117\n",
            "for batch5390 loss is 0.00518463458865881\n",
            "for batch5391 loss is 0.04055820778012276\n",
            "for batch5392 loss is 0.00015539243759121746\n",
            "for batch5393 loss is 0.8662294149398804\n",
            "for batch5394 loss is 0.04645803943276405\n",
            "for batch5395 loss is 0.0010232054628431797\n",
            "for batch5396 loss is 0.0198085717856884\n",
            "for batch5397 loss is 1.9073479506914737e-07\n",
            "for batch5398 loss is 7.030051347101107e-05\n",
            "for batch5399 loss is 0.005015519913285971\n",
            "for batch5400 loss is 0.009183408692479134\n",
            "for batch5401 loss is 0.0015416837995871902\n",
            "for batch5402 loss is 0.3625723719596863\n",
            "for batch5403 loss is 0.6400775909423828\n",
            "for batch5404 loss is 0.2278825342655182\n",
            "for batch5405 loss is 0.22259275615215302\n",
            "for batch5406 loss is 0.019976291805505753\n",
            "for batch5407 loss is 0.11589176952838898\n",
            "for batch5408 loss is 0.25360220670700073\n",
            "for batch5409 loss is 0.07710181176662445\n",
            "for batch5410 loss is 1.0013554856413975e-06\n",
            "for batch5411 loss is 0.0019569392316043377\n",
            "for batch5412 loss is 0.009862677194178104\n",
            "for batch5413 loss is 0.0016423470806330442\n",
            "for batch5414 loss is 0.20735450088977814\n",
            "for batch5415 loss is 0.011176486499607563\n",
            "for batch5416 loss is 0.10951337963342667\n",
            "for batch5417 loss is 0.0005518941907212138\n",
            "for batch5418 loss is 0.026167038828134537\n",
            "for batch5419 loss is 0.04025329276919365\n",
            "for batch5420 loss is 0.1337687075138092\n",
            "for batch5421 loss is 0.027231624349951744\n",
            "for batch5422 loss is 0.8738002777099609\n",
            "for batch5423 loss is 0.06617406755685806\n",
            "for batch5424 loss is 0.8324180841445923\n",
            "for batch5425 loss is 0.13854768872261047\n",
            "for batch5426 loss is 0.02573562227189541\n",
            "for batch5427 loss is 0.09206061065196991\n",
            "for batch5428 loss is 0.7528373003005981\n",
            "for batch5429 loss is 0.1640203893184662\n",
            "for batch5430 loss is 0.0536077618598938\n",
            "for batch5431 loss is 0.08889973908662796\n",
            "for batch5432 loss is 0.22765609622001648\n",
            "for batch5433 loss is 0.10235436260700226\n",
            "for batch5434 loss is 0.0032898865174502134\n",
            "for batch5435 loss is 0.013442784547805786\n",
            "for batch5436 loss is 0.030365368351340294\n",
            "for batch5437 loss is 0.10814984142780304\n",
            "for batch5438 loss is 0.001141676097176969\n",
            "for batch5439 loss is 0.0010619102977216244\n",
            "for batch5440 loss is 0.0038543653208762407\n",
            "for batch5441 loss is 0.025914356112480164\n",
            "for batch5442 loss is 0.14750778675079346\n",
            "for batch5443 loss is 0.0015964271733537316\n",
            "for batch5444 loss is 0.0034433603286743164\n",
            "for batch5445 loss is 0.19320693612098694\n",
            "for batch5446 loss is 0.034521959722042084\n",
            "for batch5447 loss is 0.05443283170461655\n",
            "for batch5448 loss is 0.10080734640359879\n",
            "for batch5449 loss is 0.1652708798646927\n",
            "for batch5450 loss is 0.12266838550567627\n",
            "for batch5451 loss is 0.05446697399020195\n",
            "for batch5452 loss is 0.021343357861042023\n",
            "for batch5453 loss is 0.0031922992784529924\n",
            "for batch5454 loss is 0.07006482034921646\n",
            "for batch5455 loss is 0.07956673949956894\n",
            "for batch5456 loss is 0.009904850274324417\n",
            "for batch5457 loss is 0.4034990668296814\n",
            "for batch5458 loss is 0.06103023886680603\n",
            "for batch5459 loss is 0.0034078904427587986\n",
            "for batch5460 loss is 0.2745433747768402\n",
            "for batch5461 loss is 0.0034289658069610596\n",
            "for batch5462 loss is 0.002852795412763953\n",
            "for batch5463 loss is 0.08135297894477844\n",
            "for batch5464 loss is 0.1516036093235016\n",
            "for batch5465 loss is 0.004742087796330452\n",
            "for batch5466 loss is 0.9997378587722778\n",
            "for batch5467 loss is 0.11217395216226578\n",
            "for batch5468 loss is 0.0004690856148954481\n",
            "for batch5469 loss is 0.004052760545164347\n",
            "for batch5470 loss is 0.5523694753646851\n",
            "for batch5471 loss is 0.6961264610290527\n",
            "for batch5472 loss is 0.004382660612463951\n",
            "for batch5473 loss is 0.4203945994377136\n",
            "for batch5474 loss is 0.002079067286103964\n",
            "for batch5475 loss is 0.12478891760110855\n",
            "for batch5476 loss is 4.963609899277799e-05\n",
            "for batch5477 loss is 0.013062164187431335\n",
            "for batch5478 loss is 0.0012394462246447802\n",
            "for batch5479 loss is 0.07536940276622772\n",
            "for batch5480 loss is 0.0033103935420513153\n",
            "for batch5481 loss is 9.053089161170647e-05\n",
            "for batch5482 loss is 0.00985739752650261\n",
            "for batch5483 loss is 0.14368751645088196\n",
            "for batch5484 loss is 0.12396136671304703\n",
            "for batch5485 loss is 0.008228952065110207\n",
            "for batch5486 loss is 0.00021222361829131842\n",
            "for batch5487 loss is 0.5757512450218201\n",
            "for batch5488 loss is 0.5310593247413635\n",
            "for batch5489 loss is 0.10673253238201141\n",
            "for batch5490 loss is 0.19684961438179016\n",
            "for batch5491 loss is 0.007928656414151192\n",
            "for batch5492 loss is 0.01770765893161297\n",
            "for batch5493 loss is 0.0005477649392560124\n",
            "for batch5494 loss is 0.07126972824335098\n",
            "for batch5495 loss is 0.06792774051427841\n",
            "for batch5496 loss is 0.18445409834384918\n",
            "for batch5497 loss is 0.053554754704236984\n",
            "for batch5498 loss is 0.07694171369075775\n",
            "for batch5499 loss is 0.0012340083485469222\n",
            "for batch5500 loss is 0.061973780393600464\n",
            "for batch5501 loss is 0.24439451098442078\n",
            "for batch5502 loss is 0.02203477919101715\n",
            "for batch5503 loss is 1.362975835800171\n",
            "for batch5504 loss is 0.009508563205599785\n",
            "for batch5505 loss is 0.008377927355468273\n",
            "for batch5506 loss is 0.04669296741485596\n",
            "for batch5507 loss is 0.021760346367955208\n",
            "for batch5508 loss is 0.017517682164907455\n",
            "for batch5509 loss is 0.27179810404777527\n",
            "for batch5510 loss is 0.07606470584869385\n",
            "for batch5511 loss is 3.382996510481462e-05\n",
            "for batch5512 loss is 2.3888118448667228e-05\n",
            "for batch5513 loss is 0.03288023918867111\n",
            "for batch5514 loss is 0.002668608445674181\n",
            "for batch5515 loss is 0.047505248337984085\n",
            "for batch5516 loss is 0.01917162723839283\n",
            "for batch5517 loss is 0.041198574006557465\n",
            "for batch5518 loss is 0.06348279863595963\n",
            "for batch5519 loss is 0.2601202130317688\n",
            "for batch5520 loss is 0.019286613911390305\n",
            "for batch5521 loss is 0.8278900384902954\n",
            "for batch5522 loss is 2.622602437440946e-07\n",
            "for batch5523 loss is 0.0174016784876585\n",
            "for batch5524 loss is 1.5878151316428557e-05\n",
            "for batch5525 loss is 0.009705115109682083\n",
            "for batch5526 loss is 0.10582330077886581\n",
            "for batch5527 loss is 0.9312111735343933\n",
            "for batch5528 loss is 0.023753631860017776\n",
            "for batch5529 loss is 0.0011448045261204243\n",
            "for batch5530 loss is 0.15395203232765198\n",
            "for batch5531 loss is 1.5329821109771729\n",
            "for batch5532 loss is 0.009460283443331718\n",
            "for batch5533 loss is 3.743145953194471e-06\n",
            "for batch5534 loss is 0.001929220394231379\n",
            "for batch5535 loss is 0.33161526918411255\n",
            "for batch5536 loss is 0.00184548064135015\n",
            "for batch5537 loss is 0.04110182076692581\n",
            "for batch5538 loss is 0.058372922241687775\n",
            "for batch5539 loss is 0.006217651069164276\n",
            "for batch5540 loss is 0.5471146106719971\n",
            "for batch5541 loss is 0.2585163116455078\n",
            "for batch5542 loss is 1.049024285748601e-05\n",
            "for batch5543 loss is 0.04390363767743111\n",
            "for batch5544 loss is 0.00858444906771183\n",
            "for batch5545 loss is 0.4646422266960144\n",
            "for batch5546 loss is 0.005081227980554104\n",
            "for batch5547 loss is 0.007498548831790686\n",
            "for batch5548 loss is 0.005942511837929487\n",
            "for batch5549 loss is 0.055736590176820755\n",
            "for batch5550 loss is 0.002134446520358324\n",
            "for batch5551 loss is 0.05934716388583183\n",
            "for batch5552 loss is 0.0019785258919000626\n",
            "for batch5553 loss is 0.030781934037804604\n",
            "for batch5554 loss is 0.06612016260623932\n",
            "for batch5555 loss is 0.0019772315863519907\n",
            "for batch5556 loss is 0.14644145965576172\n",
            "for batch5557 loss is 0.18578685820102692\n",
            "for batch5558 loss is 0.05346465855836868\n",
            "for batch5559 loss is 0.7326314449310303\n",
            "for batch5560 loss is 0.028373444452881813\n",
            "for batch5561 loss is 1.6474132280563936e-05\n",
            "for batch5562 loss is 0.011974124237895012\n",
            "for batch5563 loss is 0.004538351204246283\n",
            "for batch5564 loss is 0.18824471533298492\n",
            "for batch5565 loss is 0.13975176215171814\n",
            "for batch5566 loss is 0.015293982811272144\n",
            "for batch5567 loss is 0.13575690984725952\n",
            "for batch5568 loss is 0.0004253257066011429\n",
            "for batch5569 loss is 0.0007718213018961251\n",
            "for batch5570 loss is 0.07962299138307571\n",
            "for batch5571 loss is 0.2652899920940399\n",
            "for batch5572 loss is 0.17890198528766632\n",
            "for batch5573 loss is 2.3497397899627686\n",
            "for batch5574 loss is 0.031758420169353485\n",
            "for batch5575 loss is 0.0017588833579793572\n",
            "for batch5576 loss is 0.14963343739509583\n",
            "for batch5577 loss is 7.1525511202708e-07\n",
            "for batch5578 loss is 0.14974524080753326\n",
            "for batch5579 loss is 0.5859821438789368\n",
            "for batch5580 loss is 0.1404469907283783\n",
            "for batch5581 loss is 0.5433927774429321\n",
            "for batch5582 loss is 0.06146051734685898\n",
            "for batch5583 loss is 0.00022196979261934757\n",
            "for batch5584 loss is 0.00014376206672750413\n",
            "for batch5585 loss is 0.006388233043253422\n",
            "for batch5586 loss is 0.4374117851257324\n",
            "for batch5587 loss is 0.26062268018722534\n",
            "for batch5588 loss is 0.031455934047698975\n",
            "for batch5589 loss is 0.000342104904120788\n",
            "for batch5590 loss is 0.09011821448802948\n",
            "for batch5591 loss is 0.062056250870227814\n",
            "for batch5592 loss is 0.039332129061222076\n",
            "for batch5593 loss is 1.0361088514328003\n",
            "for batch5594 loss is 0.7275146245956421\n",
            "for batch5595 loss is 0.4052846431732178\n",
            "for batch5596 loss is 0.43005308508872986\n",
            "for batch5597 loss is 0.04480065777897835\n",
            "for batch5598 loss is 0.005600763019174337\n",
            "for batch5599 loss is 0.004581803921610117\n",
            "for batch5600 loss is 0.35989800095558167\n",
            "for batch5601 loss is 0.20167312026023865\n",
            "for batch5602 loss is 1.002600073814392\n",
            "for batch5603 loss is 0.3343892991542816\n",
            "for batch5604 loss is 0.17974430322647095\n",
            "for batch5605 loss is 0.00013666233280673623\n",
            "for batch5606 loss is 2.4318571831827285e-06\n",
            "for batch5607 loss is 0.058226604014635086\n",
            "for batch5608 loss is 0.08616611361503601\n",
            "for batch5609 loss is 0.26421302556991577\n",
            "for batch5610 loss is 0.013240039348602295\n",
            "for batch5611 loss is 0.11379822343587875\n",
            "for batch5612 loss is 0.5745040774345398\n",
            "for batch5613 loss is 0.000684022787027061\n",
            "for batch5614 loss is 1.1500980854034424\n",
            "for batch5615 loss is 0.08080486953258514\n",
            "for batch5616 loss is 0.30659350752830505\n",
            "for batch5617 loss is 2.334895372390747\n",
            "for batch5618 loss is 0.2750527560710907\n",
            "for batch5619 loss is 0.13769584894180298\n",
            "for batch5620 loss is 0.262916624546051\n",
            "for batch5621 loss is 0.004576667211949825\n",
            "for batch5622 loss is 0.9317150115966797\n",
            "for batch5623 loss is 0.5955281853675842\n",
            "for batch5624 loss is 0.0331135131418705\n",
            "for batch5625 loss is 3.480881787254475e-06\n",
            "for batch5626 loss is 0.3552810847759247\n",
            "for batch5627 loss is 0.007450110279023647\n",
            "for batch5628 loss is 0.010440399870276451\n",
            "for batch5629 loss is 0.6672190427780151\n",
            "for batch5630 loss is 0.052468616515398026\n",
            "for batch5631 loss is 0.00031276355730369687\n",
            "for batch5632 loss is 0.1356554925441742\n",
            "for batch5633 loss is 1.4183123111724854\n",
            "for batch5634 loss is 0.6741241216659546\n",
            "for batch5635 loss is 0.19981397688388824\n",
            "for batch5636 loss is 0.01065840758383274\n",
            "for batch5637 loss is 0.022676195949316025\n",
            "for batch5638 loss is 0.08870334178209305\n",
            "for batch5639 loss is 0.2605496048927307\n",
            "for batch5640 loss is 0.0693449005484581\n",
            "for batch5641 loss is 0.3387845456600189\n",
            "for batch5642 loss is 0.2952825427055359\n",
            "for batch5643 loss is 0.6619322896003723\n",
            "for batch5644 loss is 0.011261487379670143\n",
            "for batch5645 loss is 0.059930652379989624\n",
            "for batch5646 loss is 0.022059842944145203\n",
            "for batch5647 loss is 0.12630942463874817\n",
            "for batch5648 loss is 0.309537798166275\n",
            "for batch5649 loss is 0.28920164704322815\n",
            "for batch5650 loss is 0.3399946987628937\n",
            "for batch5651 loss is 0.7404563426971436\n",
            "for batch5652 loss is 0.008456534706056118\n",
            "for batch5653 loss is 0.07879123836755753\n",
            "for batch5654 loss is 0.4464260935783386\n",
            "for batch5655 loss is 0.0058578443713486195\n",
            "for batch5656 loss is 0.004417850635945797\n",
            "for batch5657 loss is 0.018381400033831596\n",
            "for batch5658 loss is 0.05806436017155647\n",
            "for batch5659 loss is 0.16468545794487\n",
            "for batch5660 loss is 0.008116809651255608\n",
            "for batch5661 loss is 0.19829198718070984\n",
            "for batch5662 loss is 0.0058109695091843605\n",
            "for batch5663 loss is 0.005947847850620747\n",
            "for batch5664 loss is 0.02595778927206993\n",
            "for batch5665 loss is 0.03125043585896492\n",
            "for batch5666 loss is 0.001222837483510375\n",
            "for batch5667 loss is 1.0039594173431396\n",
            "for batch5668 loss is 0.09026534855365753\n",
            "for batch5669 loss is 0.08976467698812485\n",
            "for batch5670 loss is 0.1556169092655182\n",
            "for batch5671 loss is 0.5819817781448364\n",
            "for batch5672 loss is 0.021852727979421616\n",
            "for batch5673 loss is 0.3150009512901306\n",
            "for batch5674 loss is 0.33647507429122925\n",
            "for batch5675 loss is 0.00044233151129446924\n",
            "for batch5676 loss is 0.08879408985376358\n",
            "for batch5677 loss is 0.030192801728844643\n",
            "for batch5678 loss is 0.0\n",
            "for batch5679 loss is 0.20184406638145447\n",
            "for batch5680 loss is 0.012167307548224926\n",
            "for batch5681 loss is 0.025536853820085526\n",
            "for batch5682 loss is 0.0006022002198733389\n",
            "for batch5683 loss is 0.013882987201213837\n",
            "for batch5684 loss is 0.7374390363693237\n",
            "for batch5685 loss is 0.0\n",
            "for batch5686 loss is 0.08501456677913666\n",
            "for batch5687 loss is 0.035238225013017654\n",
            "for batch5688 loss is 0.08841145783662796\n",
            "for batch5689 loss is 2.0742311335197883e-06\n",
            "for batch5690 loss is 0.004489189945161343\n",
            "for batch5691 loss is 0.03838959336280823\n",
            "for batch5692 loss is 0.009314613416790962\n",
            "for batch5693 loss is 0.003653125138953328\n",
            "for batch5694 loss is 0.027882084250450134\n",
            "for batch5695 loss is 0.1210470050573349\n",
            "for batch5696 loss is 0.17219771444797516\n",
            "for batch5697 loss is 0.31720319390296936\n",
            "for batch5698 loss is 0.233854740858078\n",
            "for batch5699 loss is 0.00019995374896097928\n",
            "for batch5700 loss is 1.71180199686205e-05\n",
            "for batch5701 loss is 0.14438477158546448\n",
            "for batch5702 loss is 0.1541157215833664\n",
            "for batch5703 loss is 0.26519492268562317\n",
            "for batch5704 loss is 0.003349809441715479\n",
            "for batch5705 loss is 1.6069656610488892\n",
            "for batch5706 loss is 2.908685928559862e-06\n",
            "for batch5707 loss is 0.48535117506980896\n",
            "for batch5708 loss is 0.022219672799110413\n",
            "for batch5709 loss is 0.5067375898361206\n",
            "for batch5710 loss is 0.00901259109377861\n",
            "for batch5711 loss is 0.1600279062986374\n",
            "for batch5712 loss is 0.0916559100151062\n",
            "for batch5713 loss is 0.05053677409887314\n",
            "for batch5714 loss is 4.243805506121134e-06\n",
            "for batch5715 loss is 0.0006518731242977083\n",
            "for batch5716 loss is 0.06163681671023369\n",
            "for batch5717 loss is 0.12777003645896912\n",
            "for batch5718 loss is 0.1414712369441986\n",
            "for batch5719 loss is 0.013929511420428753\n",
            "for batch5720 loss is 0.12978684902191162\n",
            "for batch5721 loss is 0.3568899631500244\n",
            "for batch5722 loss is 0.048608992248773575\n",
            "for batch5723 loss is 0.21389761567115784\n",
            "for batch5724 loss is 0.3440065085887909\n",
            "for batch5725 loss is 1.016359806060791\n",
            "for batch5726 loss is 0.0003225139807909727\n",
            "for batch5727 loss is 1.432845056115184e-05\n",
            "for batch5728 loss is 0.00031630476587451994\n",
            "for batch5729 loss is 0.21837034821510315\n",
            "for batch5730 loss is 0.09568329900503159\n",
            "for batch5731 loss is 0.014670467004179955\n",
            "for batch5732 loss is 0.8219712972640991\n",
            "for batch5733 loss is 0.028539802879095078\n",
            "for batch5734 loss is 0.003604354802519083\n",
            "for batch5735 loss is 0.0027394252829253674\n",
            "for batch5736 loss is 0.41830167174339294\n",
            "for batch5737 loss is 0.0991746112704277\n",
            "for batch5738 loss is 0.011846483685076237\n",
            "for batch5739 loss is 0.5733099579811096\n",
            "for batch5740 loss is 0.006443035788834095\n",
            "for batch5741 loss is 0.8224412798881531\n",
            "for batch5742 loss is 0.059071339666843414\n",
            "for batch5743 loss is 0.0013717336114495993\n",
            "for batch5744 loss is 0.006366165820509195\n",
            "for batch5745 loss is 0.41611242294311523\n",
            "for batch5746 loss is 0.008560421876609325\n",
            "for batch5747 loss is 0.057747386395931244\n",
            "for batch5748 loss is 0.0013206698931753635\n",
            "for batch5749 loss is 2.598750597826438e-06\n",
            "for batch5750 loss is 0.07932321727275848\n",
            "for batch5751 loss is 0.037134818732738495\n",
            "for batch5752 loss is 0.004656239412724972\n",
            "for batch5753 loss is 0.9528325200080872\n",
            "for batch5754 loss is 0.45354169607162476\n",
            "for batch5755 loss is 0.0\n",
            "for batch5756 loss is 0.009041126817464828\n",
            "for batch5757 loss is 0.20282313227653503\n",
            "for batch5758 loss is 0.040046315640211105\n",
            "for batch5759 loss is 0.07759227603673935\n",
            "for batch5760 loss is 0.08369483053684235\n",
            "for batch5761 loss is 0.009101688861846924\n",
            "for batch5762 loss is 0.013160528615117073\n",
            "for batch5763 loss is 0.06703533232212067\n",
            "for batch5764 loss is 0.01126650720834732\n",
            "for batch5765 loss is 0.04481856897473335\n",
            "for batch5766 loss is 0.0007969945436343551\n",
            "for batch5767 loss is 0.01692584529519081\n",
            "for batch5768 loss is 0.07900877296924591\n",
            "for batch5769 loss is 0.03659399226307869\n",
            "for batch5770 loss is 0.006311121396720409\n",
            "for batch5771 loss is 0.12185851484537125\n",
            "for batch5772 loss is 0.40892505645751953\n",
            "for batch5773 loss is 0.09905349463224411\n",
            "for batch5774 loss is 0.23579688370227814\n",
            "for batch5775 loss is 0.15001429617404938\n",
            "for batch5776 loss is 0.00265715760178864\n",
            "for batch5777 loss is 0.05213570594787598\n",
            "for batch5778 loss is 0.10477064549922943\n",
            "for batch5779 loss is 0.19778069853782654\n",
            "for batch5780 loss is 0.12721934914588928\n",
            "for batch5781 loss is 0.2135532647371292\n",
            "for batch5782 loss is 0.005753375124186277\n",
            "for batch5783 loss is 0.0031598478090018034\n",
            "for batch5784 loss is 0.0037351008504629135\n",
            "for batch5785 loss is 0.000168841885169968\n",
            "for batch5786 loss is 0.004015244543552399\n",
            "for batch5787 loss is 0.08169355243444443\n",
            "for batch5788 loss is 7.301820733118802e-05\n",
            "for batch5789 loss is 0.0008559006964787841\n",
            "for batch5790 loss is 0.07259513437747955\n",
            "for batch5791 loss is 0.008358225226402283\n",
            "for batch5792 loss is 0.0068894485011696815\n",
            "for batch5793 loss is 0.0073632365092635155\n",
            "for batch5794 loss is 0.0812818855047226\n",
            "for batch5795 loss is 0.036894649267196655\n",
            "for batch5796 loss is 0.01547639537602663\n",
            "for batch5797 loss is 0.08260912448167801\n",
            "for batch5798 loss is 1.5996949672698975\n",
            "for batch5799 loss is 0.000718053721357137\n",
            "for batch5800 loss is 0.1798955798149109\n",
            "for batch5801 loss is 0.2860850691795349\n",
            "for batch5802 loss is 0.06177405267953873\n",
            "for batch5803 loss is 0.00011432291648816317\n",
            "for batch5804 loss is 0.009893272072076797\n",
            "for batch5805 loss is 0.016721582040190697\n",
            "for batch5806 loss is 0.3286321759223938\n",
            "for batch5807 loss is 0.005656114779412746\n",
            "for batch5808 loss is 0.286149799823761\n",
            "for batch5809 loss is 0.010995305143296719\n",
            "for batch5810 loss is 0.09879259765148163\n",
            "for batch5811 loss is 0.019815441220998764\n",
            "for batch5812 loss is 0.37064647674560547\n",
            "for batch5813 loss is 0.003440179629251361\n",
            "for batch5814 loss is 0.0005440820823423564\n",
            "for batch5815 loss is 0.024549774825572968\n",
            "for batch5816 loss is 0.0018686320399865508\n",
            "for batch5817 loss is 0.10876128822565079\n",
            "for batch5818 loss is 0.7988905310630798\n",
            "for batch5819 loss is 0.0010551370214670897\n",
            "for batch5820 loss is 0.16108913719654083\n",
            "for batch5821 loss is 0.17711079120635986\n",
            "for batch5822 loss is 0.26664331555366516\n",
            "for batch5823 loss is 0.38309186697006226\n",
            "for batch5824 loss is 0.06256212294101715\n",
            "for batch5825 loss is 0.0032758121378719807\n",
            "for batch5826 loss is 1.3074266910552979\n",
            "for batch5827 loss is 0.26441633701324463\n",
            "for batch5828 loss is 0.10891483724117279\n",
            "for batch5829 loss is 0.004478036891669035\n",
            "for batch5830 loss is 0.0055653685703873634\n",
            "for batch5831 loss is 0.006155954208225012\n",
            "for batch5832 loss is 0.1642056703567505\n",
            "for batch5833 loss is 0.11396552622318268\n",
            "for batch5834 loss is 0.003471809672191739\n",
            "for batch5835 loss is 0.00023951972252689302\n",
            "for batch5836 loss is 5.197458904149244e-06\n",
            "for batch5837 loss is 0.9904084205627441\n",
            "for batch5838 loss is 0.031164903193712234\n",
            "for batch5839 loss is 0.06513116508722305\n",
            "for batch5840 loss is 0.003012880450114608\n",
            "for batch5841 loss is 0.15358440577983856\n",
            "for batch5842 loss is 0.40046077966690063\n",
            "for batch5843 loss is 0.02976948395371437\n",
            "for batch5844 loss is 0.004202116280794144\n",
            "for batch5845 loss is 0.009729084558784962\n",
            "for batch5846 loss is 0.10040949285030365\n",
            "for batch5847 loss is 0.1699548065662384\n",
            "for batch5848 loss is 2.7980237007141113\n",
            "for batch5849 loss is 0.3064681589603424\n",
            "for batch5850 loss is 0.10922712087631226\n",
            "for batch5851 loss is 0.17289096117019653\n",
            "for batch5852 loss is 0.23870372772216797\n",
            "for batch5853 loss is 0.032882679253816605\n",
            "for batch5854 loss is 0.007304440252482891\n",
            "for batch5855 loss is 0.006956305354833603\n",
            "for batch5856 loss is 0.07297005504369736\n",
            "for batch5857 loss is 0.5015523433685303\n",
            "for batch5858 loss is 0.6401076912879944\n",
            "for batch5859 loss is 0.6005167365074158\n",
            "for batch5860 loss is 0.03502347320318222\n",
            "for batch5861 loss is 0.18895399570465088\n",
            "for batch5862 loss is 0.003115301951766014\n",
            "for batch5863 loss is 0.1332729011774063\n",
            "for batch5864 loss is 0.10637275874614716\n",
            "for batch5865 loss is 0.1597582995891571\n",
            "for batch5866 loss is 0.33698195219039917\n",
            "for batch5867 loss is 0.007235903292894363\n",
            "for batch5868 loss is 0.034245532006025314\n",
            "for batch5869 loss is 0.07134215533733368\n",
            "for batch5870 loss is 0.007915137335658073\n",
            "for batch5871 loss is 0.0012967655202373862\n",
            "for batch5872 loss is 0.08978914469480515\n",
            "for batch5873 loss is 0.715664267539978\n",
            "for batch5874 loss is 0.10091546922922134\n",
            "for batch5875 loss is 0.21119911968708038\n",
            "for batch5876 loss is 0.015245625749230385\n",
            "for batch5877 loss is 0.14403589069843292\n",
            "for batch5878 loss is 0.02164245769381523\n",
            "for batch5879 loss is 0.11691279709339142\n",
            "for batch5880 loss is 0.011453822255134583\n",
            "for batch5881 loss is 4.274616367183626e-05\n",
            "for batch5882 loss is 0.2398443967103958\n",
            "for batch5883 loss is 0.010376041755080223\n",
            "for batch5884 loss is 0.0012979578459635377\n",
            "for batch5885 loss is 0.0006152067799121141\n",
            "for batch5886 loss is 0.6596301198005676\n",
            "for batch5887 loss is 0.986127495765686\n",
            "for batch5888 loss is 0.24140572547912598\n",
            "for batch5889 loss is 0.003203162457793951\n",
            "for batch5890 loss is 0.029627278447151184\n",
            "for batch5891 loss is 0.010860586538910866\n",
            "for batch5892 loss is 0.10996506363153458\n",
            "for batch5893 loss is 0.01831027865409851\n",
            "for batch5894 loss is 0.16406485438346863\n",
            "for batch5895 loss is 0.0036067061591893435\n",
            "for batch5896 loss is 0.00498189264908433\n",
            "for batch5897 loss is 4.631994670489803e-05\n",
            "for batch5898 loss is 0.030848030000925064\n",
            "for batch5899 loss is 0.001122468849644065\n",
            "for batch5900 loss is 0.16051016747951508\n",
            "for batch5901 loss is 0.0028732740320265293\n",
            "for batch5902 loss is 0.037155598402023315\n",
            "for batch5903 loss is 0.0035217218101024628\n",
            "for batch5904 loss is 0.015866326168179512\n",
            "for batch5905 loss is 0.16867691278457642\n",
            "for batch5906 loss is 0.02654115855693817\n",
            "for batch5907 loss is 0.12435295432806015\n",
            "for batch5908 loss is 0.029135581105947495\n",
            "for batch5909 loss is 0.0014729740796610713\n",
            "for batch5910 loss is 0.00012659179628826678\n",
            "for batch5911 loss is 0.030263666063547134\n",
            "for batch5912 loss is 0.0027233161963522434\n",
            "for batch5913 loss is 0.11416138708591461\n",
            "for batch5914 loss is 0.00024137148284353316\n",
            "for batch5915 loss is 0.030856823548674583\n",
            "for batch5916 loss is 0.008187067694962025\n",
            "for batch5917 loss is 0.0025131194852292538\n",
            "for batch5918 loss is 0.4031356871128082\n",
            "for batch5919 loss is 5.5312348195002414e-06\n",
            "for batch5920 loss is 0.049331922084093094\n",
            "for batch5921 loss is 0.000869413954205811\n",
            "for batch5922 loss is 0.7391068339347839\n",
            "for batch5923 loss is 0.04020165279507637\n",
            "for batch5924 loss is 1.123830795288086\n",
            "for batch5925 loss is 0.00014538133109454066\n",
            "for batch5926 loss is 0.03370808809995651\n",
            "for batch5927 loss is 0.1008761078119278\n",
            "for batch5928 loss is 0.43665313720703125\n",
            "for batch5929 loss is 0.0674770325422287\n",
            "for batch5930 loss is 0.010697358287870884\n",
            "for batch5931 loss is 0.05429833009839058\n",
            "for batch5932 loss is 0.13579972088336945\n",
            "for batch5933 loss is 0.008998237550258636\n",
            "for batch5934 loss is 0.08573494106531143\n",
            "for batch5935 loss is 0.08589787781238556\n",
            "for batch5936 loss is 0.2577468752861023\n",
            "for batch5937 loss is 0.05812157317996025\n",
            "for batch5938 loss is 0.25042927265167236\n",
            "for batch5939 loss is 0.4794100224971771\n",
            "for batch5940 loss is 6.02927066211123e-05\n",
            "for batch5941 loss is 0.3904113173484802\n",
            "for batch5942 loss is 0.00985022820532322\n",
            "for batch5943 loss is 0.09016688168048859\n",
            "for batch5944 loss is 3.1397867132909596e-05\n",
            "for batch5945 loss is 0.003931083716452122\n",
            "for batch5946 loss is 0.023798692971467972\n",
            "for batch5947 loss is 0.00022942486975807697\n",
            "for batch5948 loss is 0.006505537778139114\n",
            "for batch5949 loss is 0.020823638886213303\n",
            "for batch5950 loss is 0.012482533231377602\n",
            "for batch5951 loss is 2.384185648907078e-08\n",
            "for batch5952 loss is 0.0016974706668406725\n",
            "for batch5953 loss is 0.005912075284868479\n",
            "for batch5954 loss is 0.26632755994796753\n",
            "for batch5955 loss is 0.01898270659148693\n",
            "for batch5956 loss is 0.1730661541223526\n",
            "for batch5957 loss is 0.20570428669452667\n",
            "for batch5958 loss is 0.30589818954467773\n",
            "for batch5959 loss is 0.2202928364276886\n",
            "for batch5960 loss is 0.022172214463353157\n",
            "for batch5961 loss is 0.0011556390672922134\n",
            "for batch5962 loss is 0.1520264446735382\n",
            "for batch5963 loss is 0.8720183372497559\n",
            "for batch5964 loss is 0.3159154951572418\n",
            "for batch5965 loss is 9.0112051111646e-05\n",
            "for batch5966 loss is 0.35806405544281006\n",
            "for batch5967 loss is 0.21755918860435486\n",
            "for batch5968 loss is 0.009653178043663502\n",
            "for batch5969 loss is 5.113424776936881e-05\n",
            "for batch5970 loss is 0.021673887968063354\n",
            "for batch5971 loss is 0.00798687431961298\n",
            "for batch5972 loss is 0.39645320177078247\n",
            "for batch5973 loss is 0.3205558955669403\n",
            "for batch5974 loss is 0.00026116002118214965\n",
            "for batch5975 loss is 0.027148496359586716\n",
            "for batch5976 loss is 0.020049819722771645\n",
            "for batch5977 loss is 0.025156069546937943\n",
            "for batch5978 loss is 0.32589980959892273\n",
            "for batch5979 loss is 0.01623035967350006\n",
            "for batch5980 loss is 0.002613634569570422\n",
            "for batch5981 loss is 0.04475192353129387\n",
            "for batch5982 loss is 0.011035546660423279\n",
            "for batch5983 loss is 0.007637762930244207\n",
            "for batch5984 loss is 0.023998303338885307\n",
            "for batch5985 loss is 0.0004732878878712654\n",
            "for batch5986 loss is 0.037030186504125595\n",
            "for batch5987 loss is 0.03452853858470917\n",
            "for batch5988 loss is 0.027432594448328018\n",
            "for batch5989 loss is 0.05821775645017624\n",
            "for batch5990 loss is 0.35743218660354614\n",
            "for batch5991 loss is 0.1175219863653183\n",
            "for batch5992 loss is 0.0766974613070488\n",
            "for batch5993 loss is 0.02473394013941288\n",
            "for batch5994 loss is 0.0017718911403790116\n",
            "for batch5995 loss is 0.7700979113578796\n",
            "for batch5996 loss is 0.615736186504364\n",
            "for batch5997 loss is 0.12519405782222748\n",
            "for batch5998 loss is 0.004914252553135157\n",
            "for batch5999 loss is 0.013906097039580345\n",
            "for batch6000 loss is 0.001029337290674448\n",
            "for batch6001 loss is 0.0004629773029591888\n",
            "for batch6002 loss is 0.0046856701374053955\n",
            "for batch6003 loss is 0.4722028374671936\n",
            "for batch6004 loss is 0.04815226048231125\n",
            "for batch6005 loss is 0.02640504576265812\n",
            "for batch6006 loss is 0.32091981172561646\n",
            "for batch6007 loss is 0.09297703206539154\n",
            "for batch6008 loss is 0.025449877604842186\n",
            "for batch6009 loss is 0.038497358560562134\n",
            "for batch6010 loss is 0.014799229800701141\n",
            "for batch6011 loss is 0.0073234462179243565\n",
            "for batch6012 loss is 0.02159484103322029\n",
            "for batch6013 loss is 0.7638748288154602\n",
            "for batch6014 loss is 0.15915103256702423\n",
            "for batch6015 loss is 0.004514389205724001\n",
            "for batch6016 loss is 0.008501994423568249\n",
            "for batch6017 loss is 0.0017926875734701753\n",
            "for batch6018 loss is 0.36920684576034546\n",
            "for batch6019 loss is 0.006962784565985203\n",
            "for batch6020 loss is 0.03957532346248627\n",
            "for batch6021 loss is 0.020175307989120483\n",
            "for batch6022 loss is 0.10545966774225235\n",
            "for batch6023 loss is 0.17750531435012817\n",
            "for batch6024 loss is 0.02412603422999382\n",
            "for batch6025 loss is 0.16279059648513794\n",
            "for batch6026 loss is 0.18454338610172272\n",
            "for batch6027 loss is 0.0016724786255508661\n",
            "for batch6028 loss is 0.7359204292297363\n",
            "for batch6029 loss is 0.06013137847185135\n",
            "for batch6030 loss is 0.09588874876499176\n",
            "for batch6031 loss is 0.5385937094688416\n",
            "for batch6032 loss is 0.659002423286438\n",
            "for batch6033 loss is 2.164768193324562e-05\n",
            "for batch6034 loss is 1.909644197439775e-05\n",
            "for batch6035 loss is 0.013743666931986809\n",
            "for batch6036 loss is 0.00033927112235687673\n",
            "for batch6037 loss is 0.8427578806877136\n",
            "for batch6038 loss is 0.04362998530268669\n",
            "for batch6039 loss is 0.010292712599039078\n",
            "for batch6040 loss is 0.0027672001160681248\n",
            "for batch6041 loss is 0.03489918261766434\n",
            "for batch6042 loss is 0.0011256372090429068\n",
            "for batch6043 loss is 0.1703786551952362\n",
            "for batch6044 loss is 0.006339038722217083\n",
            "for batch6045 loss is 0.003391677513718605\n",
            "for batch6046 loss is 0.0003781556151807308\n",
            "for batch6047 loss is 0.05564187094569206\n",
            "for batch6048 loss is 0.08949486166238785\n",
            "for batch6049 loss is 0.010972353629767895\n",
            "for batch6050 loss is 0.09427426010370255\n",
            "for batch6051 loss is 0.4967133402824402\n",
            "for batch6052 loss is 0.001137582235969603\n",
            "for batch6053 loss is 0.0017025265842676163\n",
            "for batch6054 loss is 0.190409317612648\n",
            "for batch6055 loss is 0.0009808592731133103\n",
            "for batch6056 loss is 0.1352488100528717\n",
            "for batch6057 loss is 0.029675405472517014\n",
            "for batch6058 loss is 0.37166544795036316\n",
            "for batch6059 loss is 0.004216199740767479\n",
            "for batch6060 loss is 0.0017030697781592607\n",
            "for batch6061 loss is 0.23767979443073273\n",
            "for batch6062 loss is 0.4053177237510681\n",
            "for batch6063 loss is 0.0636001005768776\n",
            "for batch6064 loss is 0.11347760260105133\n",
            "for batch6065 loss is 0.00017225595365744084\n",
            "for batch6066 loss is 0.048816535621881485\n",
            "for batch6067 loss is 0.004135844297707081\n",
            "for batch6068 loss is 0.0020231292583048344\n",
            "for batch6069 loss is 0.42256832122802734\n",
            "for batch6070 loss is 0.017852533608675003\n",
            "for batch6071 loss is 1.3177659511566162\n",
            "for batch6072 loss is 0.0858171135187149\n",
            "for batch6073 loss is 0.027134990319609642\n",
            "for batch6074 loss is 0.09368012845516205\n",
            "for batch6075 loss is 0.0021094949916005135\n",
            "for batch6076 loss is 0.01776522397994995\n",
            "for batch6077 loss is 0.00010610399476718158\n",
            "for batch6078 loss is 0.053942352533340454\n",
            "for batch6079 loss is 0.010859902948141098\n",
            "for batch6080 loss is 0.6117584109306335\n",
            "for batch6081 loss is 0.12217137962579727\n",
            "for batch6082 loss is 0.011023029685020447\n",
            "for batch6083 loss is 0.13855299353599548\n",
            "for batch6084 loss is 0.0010806701611727476\n",
            "for batch6085 loss is 0.04801054671406746\n",
            "for batch6086 loss is 2.217280325567117e-06\n",
            "for batch6087 loss is 0.334769070148468\n",
            "for batch6088 loss is 0.00618180725723505\n",
            "for batch6089 loss is 0.02320072054862976\n",
            "for batch6090 loss is 2.9157052267692052e-05\n",
            "for batch6091 loss is 0.9751162528991699\n",
            "for batch6092 loss is 0.04950463026762009\n",
            "for batch6093 loss is 0.01411234401166439\n",
            "for batch6094 loss is 0.07680250704288483\n",
            "for batch6095 loss is 0.23399345576763153\n",
            "for batch6096 loss is 0.0003067675861530006\n",
            "for batch6097 loss is 0.03309129923582077\n",
            "for batch6098 loss is 0.00037720243562944233\n",
            "for batch6099 loss is 0.37027066946029663\n",
            "for batch6100 loss is 0.07147068530321121\n",
            "for batch6101 loss is 0.7261507511138916\n",
            "for batch6102 loss is 0.014517766423523426\n",
            "for batch6103 loss is 0.001918362220749259\n",
            "for batch6104 loss is 0.06282062083482742\n",
            "for batch6105 loss is 0.14022105932235718\n",
            "for batch6106 loss is 0.016162943094968796\n",
            "for batch6107 loss is 0.11127428710460663\n",
            "for batch6108 loss is 0.0030656582675874233\n",
            "for batch6109 loss is 0.00820175651460886\n",
            "for batch6110 loss is 0.0004054964520037174\n",
            "for batch6111 loss is 0.20576032996177673\n",
            "for batch6112 loss is 0.0047111595049500465\n",
            "for batch6113 loss is 0.019760023802518845\n",
            "for batch6114 loss is 0.0657653659582138\n",
            "for batch6115 loss is 0.0024881805293262005\n",
            "for batch6116 loss is 0.5674233436584473\n",
            "for batch6117 loss is 0.05986328050494194\n",
            "for batch6118 loss is 0.032642755657434464\n",
            "for batch6119 loss is 0.10839947313070297\n",
            "for batch6120 loss is 0.0010134249459952116\n",
            "for batch6121 loss is 0.40964165329933167\n",
            "for batch6122 loss is 1.069723129272461\n",
            "for batch6123 loss is 0.223183274269104\n",
            "for batch6124 loss is 2.384185648907078e-08\n",
            "for batch6125 loss is 1.720322608947754\n",
            "for batch6126 loss is 0.006504648830741644\n",
            "for batch6127 loss is 0.2864498198032379\n",
            "for batch6128 loss is 0.20885148644447327\n",
            "for batch6129 loss is 0.17118598520755768\n",
            "for batch6130 loss is 1.0476741790771484\n",
            "for batch6131 loss is 0.17495983839035034\n",
            "for batch6132 loss is 0.25794416666030884\n",
            "for batch6133 loss is 0.1745431125164032\n",
            "for batch6134 loss is 0.073915496468544\n",
            "for batch6135 loss is 0.26378360390663147\n",
            "for batch6136 loss is 0.14837676286697388\n",
            "for batch6137 loss is 0.00864599458873272\n",
            "for batch6138 loss is 0.05333714932203293\n",
            "for batch6139 loss is 0.002101816702634096\n",
            "for batch6140 loss is 0.13322001695632935\n",
            "for batch6141 loss is 0.10227452218532562\n",
            "for batch6142 loss is 0.002068720990791917\n",
            "for batch6143 loss is 0.21987859904766083\n",
            "for batch6144 loss is 0.0015486904885619879\n",
            "for batch6145 loss is 0.0004965573898516595\n",
            "for batch6146 loss is 6.00809426032356e-06\n",
            "for batch6147 loss is 0.3998405337333679\n",
            "for batch6148 loss is 0.010373391211032867\n",
            "for batch6149 loss is 0.02431463450193405\n",
            "for batch6150 loss is 0.9278255701065063\n",
            "for batch6151 loss is 0.033108241856098175\n",
            "for batch6152 loss is 0.022137310355901718\n",
            "for batch6153 loss is 0.1268235146999359\n",
            "for batch6154 loss is 0.01530875451862812\n",
            "for batch6155 loss is 0.04009702056646347\n",
            "for batch6156 loss is 0.06522752344608307\n",
            "for batch6157 loss is 0.11062955856323242\n",
            "for batch6158 loss is 9.769191092345864e-05\n",
            "for batch6159 loss is 0.4356868863105774\n",
            "for batch6160 loss is 0.037795912474393845\n",
            "for batch6161 loss is 0.07628242671489716\n",
            "for batch6162 loss is 0.002156984293833375\n",
            "for batch6163 loss is 0.020249072462320328\n",
            "for batch6164 loss is 0.04985267296433449\n",
            "for batch6165 loss is 0.0010764168109744787\n",
            "for batch6166 loss is 0.3334903120994568\n",
            "for batch6167 loss is 0.6127821207046509\n",
            "for batch6168 loss is 1.9073476664743794e-07\n",
            "for batch6169 loss is 0.020352229475975037\n",
            "for batch6170 loss is 0.9197503924369812\n",
            "for batch6171 loss is 5.5711039749439806e-05\n",
            "for batch6172 loss is 0.00016846711514517665\n",
            "for batch6173 loss is 0.015116607770323753\n",
            "for batch6174 loss is 0.14167991280555725\n",
            "for batch6175 loss is 0.0232057124376297\n",
            "for batch6176 loss is 0.07046643644571304\n",
            "for batch6177 loss is 0.0004035932070109993\n",
            "for batch6178 loss is 0.07228405773639679\n",
            "for batch6179 loss is 0.008581905625760555\n",
            "for batch6180 loss is 4.2338659113738686e-05\n",
            "for batch6181 loss is 1.9836068531731144e-05\n",
            "for batch6182 loss is 0.0005261314800009131\n",
            "for batch6183 loss is 0.8418461680412292\n",
            "for batch6184 loss is 0.006816983222961426\n",
            "for batch6185 loss is 0.06882333010435104\n",
            "for batch6186 loss is 0.02965511754155159\n",
            "for batch6187 loss is 0.007147081196308136\n",
            "for batch6188 loss is 0.03339356929063797\n",
            "for batch6189 loss is 0.12346212565898895\n",
            "for batch6190 loss is 0.013469213619828224\n",
            "for batch6191 loss is 0.006104358471930027\n",
            "for batch6192 loss is 0.09602151811122894\n",
            "for batch6193 loss is 0.312893271446228\n",
            "for batch6194 loss is 0.012924253940582275\n",
            "for batch6195 loss is 0.020390212535858154\n",
            "for batch6196 loss is 0.005883886478841305\n",
            "for batch6197 loss is 0.00018514087423682213\n",
            "for batch6198 loss is 0.04219638556241989\n",
            "for batch6199 loss is 0.014552059583365917\n",
            "for batch6200 loss is 0.0163432564586401\n",
            "for batch6201 loss is 0.023580631241202354\n",
            "for batch6202 loss is 1.8429151168675162e-05\n",
            "for batch6203 loss is 0.013304012827575207\n",
            "for batch6204 loss is 0.0063539184629917145\n",
            "for batch6205 loss is 0.004618873819708824\n",
            "for batch6206 loss is 0.046312447637319565\n",
            "for batch6207 loss is 0.2186194658279419\n",
            "for batch6208 loss is 0.03264611214399338\n",
            "for batch6209 loss is 0.673109233379364\n",
            "for batch6210 loss is 0.018590401858091354\n",
            "for batch6211 loss is 0.0007871934212744236\n",
            "for batch6212 loss is 0.004754704888910055\n",
            "for batch6213 loss is 0.0020192416850477457\n",
            "for batch6214 loss is 0.038909368216991425\n",
            "for batch6215 loss is 0.23591935634613037\n",
            "for batch6216 loss is 0.12024418264627457\n",
            "for batch6217 loss is 0.006456433329731226\n",
            "for batch6218 loss is 0.06253373622894287\n",
            "for batch6219 loss is 0.02975325658917427\n",
            "for batch6220 loss is 0.18812592327594757\n",
            "for batch6221 loss is 1.7881347957882099e-06\n",
            "for batch6222 loss is 0.025660719722509384\n",
            "for batch6223 loss is 1.883423828985542e-05\n",
            "for batch6224 loss is 0.294116735458374\n",
            "for batch6225 loss is 0.05440882593393326\n",
            "for batch6226 loss is 0.0058754561468958855\n",
            "for batch6227 loss is 0.30716049671173096\n",
            "for batch6228 loss is 0.013324891217052937\n",
            "for batch6229 loss is 0.18170464038848877\n",
            "for batch6230 loss is 0.09331899881362915\n",
            "for batch6231 loss is 0.15050914883613586\n",
            "for batch6232 loss is 0.015489058569073677\n",
            "for batch6233 loss is 0.026927590370178223\n",
            "for batch6234 loss is 1.0544683933258057\n",
            "for batch6235 loss is 7.295524028450018e-06\n",
            "for batch6236 loss is 0.017522675916552544\n",
            "for batch6237 loss is 0.02957967482507229\n",
            "for batch6238 loss is 0.3481278419494629\n",
            "for batch6239 loss is 0.14962880313396454\n",
            "for batch6240 loss is 0.10886462032794952\n",
            "for batch6241 loss is 0.018954524770379066\n",
            "for batch6242 loss is 0.45017009973526\n",
            "for batch6243 loss is 0.21415790915489197\n",
            "for batch6244 loss is 0.7261982560157776\n",
            "for batch6245 loss is 0.009271037764847279\n",
            "for batch6246 loss is 0.44029927253723145\n",
            "for batch6247 loss is 0.0015815843362361193\n",
            "for batch6248 loss is 0.015467299148440361\n",
            "for batch6249 loss is 0.5438920259475708\n",
            "for batch6250 loss is 0.01680867001414299\n",
            "for batch6251 loss is 0.006775762885808945\n",
            "for batch6252 loss is 0.0007110461010597646\n",
            "for batch6253 loss is 0.07147692143917084\n",
            "for batch6254 loss is 0.03376253321766853\n",
            "for batch6255 loss is 0.002180044073611498\n",
            "for batch6256 loss is 0.05562669783830643\n",
            "for batch6257 loss is 0.0020999007392674685\n",
            "for batch6258 loss is 0.013719147071242332\n",
            "for batch6259 loss is 0.24292631447315216\n",
            "for batch6260 loss is 0.017464524134993553\n",
            "for batch6261 loss is 0.001836845651268959\n",
            "for batch6262 loss is 0.02378160133957863\n",
            "for batch6263 loss is 5.006785386285628e-07\n",
            "for batch6264 loss is 0.03963262587785721\n",
            "for batch6265 loss is 0.0023006603587418795\n",
            "for batch6266 loss is 0.059610478579998016\n",
            "for batch6267 loss is 0.0001857701427070424\n",
            "for batch6268 loss is 0.041844453662633896\n",
            "for batch6269 loss is 0.00882568396627903\n",
            "for batch6270 loss is 0.32911401987075806\n",
            "for batch6271 loss is 0.013211186043918133\n",
            "for batch6272 loss is 0.018844926729798317\n",
            "for batch6273 loss is 0.3561309576034546\n",
            "for batch6274 loss is 0.23788733780384064\n",
            "for batch6275 loss is 0.22525426745414734\n",
            "for batch6276 loss is 0.10454734414815903\n",
            "for batch6277 loss is 0.012835919857025146\n",
            "for batch6278 loss is 0.01788971945643425\n",
            "for batch6279 loss is 0.14558282494544983\n",
            "for batch6280 loss is 0.08931905776262283\n",
            "for batch6281 loss is 0.00016318386769853532\n",
            "for batch6282 loss is 0.2964082360267639\n",
            "for batch6283 loss is 0.0003718625521287322\n",
            "for batch6284 loss is 1.6725860834121704\n",
            "for batch6285 loss is 0.01858946494758129\n",
            "for batch6286 loss is 0.06842701882123947\n",
            "for batch6287 loss is 0.013099734671413898\n",
            "for batch6288 loss is 0.012242475524544716\n",
            "for batch6289 loss is 1.1920925402364446e-07\n",
            "for batch6290 loss is 0.15420004725456238\n",
            "for batch6291 loss is 0.09195610135793686\n",
            "for batch6292 loss is 0.002732597291469574\n",
            "for batch6293 loss is 1.045863151550293\n",
            "for batch6294 loss is 0.00431563425809145\n",
            "for batch6295 loss is 0.0011029539164155722\n",
            "for batch6296 loss is 0.12460323423147202\n",
            "for batch6297 loss is 0.011103005148470402\n",
            "for batch6298 loss is 0.008266297169029713\n",
            "for batch6299 loss is 0.029997965320944786\n",
            "for batch6300 loss is 0.07140232622623444\n",
            "for batch6301 loss is 0.23342633247375488\n",
            "for batch6302 loss is 1.814283496059943e-05\n",
            "for batch6303 loss is 0.05837257578969002\n",
            "for batch6304 loss is 0.07046233117580414\n",
            "for batch6305 loss is 0.010535362176597118\n",
            "for batch6306 loss is 0.025916466489434242\n",
            "for batch6307 loss is 0.32202398777008057\n",
            "for batch6308 loss is 0.05732356756925583\n",
            "for batch6309 loss is 0.09116773307323456\n",
            "for batch6310 loss is 0.028799939900636673\n",
            "for batch6311 loss is 1.0588375329971313\n",
            "for batch6312 loss is 0.29607468843460083\n",
            "for batch6313 loss is 0.1552887111902237\n",
            "for batch6314 loss is 0.028990935534238815\n",
            "for batch6315 loss is 0.01128314808011055\n",
            "for batch6316 loss is 2.8848439797002356e-06\n",
            "for batch6317 loss is 0.40503397583961487\n",
            "for batch6318 loss is 0.12478436529636383\n",
            "for batch6319 loss is 0.062151987105607986\n",
            "for batch6320 loss is 0.004061772488057613\n",
            "for batch6321 loss is 0.08833646774291992\n",
            "for batch6322 loss is 0.05934184789657593\n",
            "for batch6323 loss is 0.04001916944980621\n",
            "for batch6324 loss is 0.24974170327186584\n",
            "for batch6325 loss is 0.04827743023633957\n",
            "for batch6326 loss is 0.0017720118630677462\n",
            "for batch6327 loss is 1.5839825868606567\n",
            "for batch6328 loss is 0.4651103913784027\n",
            "for batch6329 loss is 0.5509606599807739\n",
            "for batch6330 loss is 0.08167649060487747\n",
            "for batch6331 loss is 0.7395989298820496\n",
            "for batch6332 loss is 0.0005703142378479242\n",
            "for batch6333 loss is 0.0035572699271142483\n",
            "for batch6334 loss is 0.004883700516074896\n",
            "for batch6335 loss is 0.721996545791626\n",
            "for batch6336 loss is 0.03764615207910538\n",
            "for batch6337 loss is 9.53674117454284e-08\n",
            "for batch6338 loss is 0.013491024263203144\n",
            "for batch6339 loss is 0.08063850551843643\n",
            "for batch6340 loss is 0.14096811413764954\n",
            "for batch6341 loss is 0.0007787069771438837\n",
            "for batch6342 loss is 0.07678325474262238\n",
            "for batch6343 loss is 0.11501141637563705\n",
            "for batch6344 loss is 0.17028819024562836\n",
            "for batch6345 loss is 0.04041687771677971\n",
            "for batch6346 loss is 0.001622696639969945\n",
            "for batch6347 loss is 0.0006687091663479805\n",
            "for batch6348 loss is 0.0482228621840477\n",
            "for batch6349 loss is 0.9803175926208496\n",
            "for batch6350 loss is 0.0981966108083725\n",
            "for batch6351 loss is 0.04472192004323006\n",
            "for batch6352 loss is 0.03481627255678177\n",
            "for batch6353 loss is 0.0009849222842603922\n",
            "for batch6354 loss is 8.46368038764922e-06\n",
            "for batch6355 loss is 0.17051230370998383\n",
            "for batch6356 loss is 0.16629831492900848\n",
            "for batch6357 loss is 8.028325828490779e-05\n",
            "for batch6358 loss is 0.003510401351377368\n",
            "for batch6359 loss is 0.19720515608787537\n",
            "for batch6360 loss is 0.002852858742699027\n",
            "for batch6361 loss is 0.11501723527908325\n",
            "for batch6362 loss is 0.08497407287359238\n",
            "for batch6363 loss is 0.00045487802708521485\n",
            "for batch6364 loss is 0.018835391849279404\n",
            "for batch6365 loss is 7.890098640928045e-05\n",
            "for batch6366 loss is 0.09073473513126373\n",
            "for batch6367 loss is 0.0012147653615102172\n",
            "for batch6368 loss is 0.62774658203125\n",
            "for batch6369 loss is 0.005431869067251682\n",
            "for batch6370 loss is 0.0005967284087091684\n",
            "for batch6371 loss is 0.13120335340499878\n",
            "for batch6372 loss is 3.6716205613629427e-06\n",
            "for batch6373 loss is 0.006254115141928196\n",
            "for batch6374 loss is 0.19590318202972412\n",
            "for batch6375 loss is 0.002175719942897558\n",
            "for batch6376 loss is 1.0609534978866577\n",
            "for batch6377 loss is 0.0058117457665503025\n",
            "for batch6378 loss is 0.06941793113946915\n",
            "for batch6379 loss is 0.005178393330425024\n",
            "for batch6380 loss is 0.014352257363498211\n",
            "for batch6381 loss is 0.6023572683334351\n",
            "for batch6382 loss is 0.3299925923347473\n",
            "for batch6383 loss is 0.09803704917430878\n",
            "for batch6384 loss is 0.6250487565994263\n",
            "for batch6385 loss is 0.2179323434829712\n",
            "for batch6386 loss is 0.10981430113315582\n",
            "for batch6387 loss is 0.021567313000559807\n",
            "for batch6388 loss is 0.03740854933857918\n",
            "for batch6389 loss is 0.058330096304416656\n",
            "for batch6390 loss is 0.035219594836235046\n",
            "for batch6391 loss is 0.0037007175851613283\n",
            "for batch6392 loss is 0.03567471355199814\n",
            "for batch6393 loss is 0.0018133057747036219\n",
            "for batch6394 loss is 0.0063234069384634495\n",
            "for batch6395 loss is 0.01816660165786743\n",
            "for batch6396 loss is 0.00185440294444561\n",
            "for batch6397 loss is 0.1136779636144638\n",
            "for batch6398 loss is 1.125326156616211\n",
            "for batch6399 loss is 0.07170510292053223\n",
            "for batch6400 loss is 0.04698653891682625\n",
            "for batch6401 loss is 0.2632030248641968\n",
            "for batch6402 loss is 1.8476854165783152e-05\n",
            "for batch6403 loss is 0.00818035751581192\n",
            "for batch6404 loss is 0.2525734007358551\n",
            "for batch6405 loss is 0.003068742109462619\n",
            "for batch6406 loss is 0.008569584228098392\n",
            "for batch6407 loss is 0.05688834935426712\n",
            "for batch6408 loss is 0.003194597316905856\n",
            "for batch6409 loss is 0.1905997395515442\n",
            "for batch6410 loss is 0.024578595533967018\n",
            "for batch6411 loss is 0.11429951339960098\n",
            "for batch6412 loss is 0.08212688565254211\n",
            "for batch6413 loss is 0.19664566218852997\n",
            "for batch6414 loss is 0.002163294004276395\n",
            "for batch6415 loss is 0.015422324649989605\n",
            "for batch6416 loss is 0.8444247245788574\n",
            "for batch6417 loss is 0.5276145935058594\n",
            "for batch6418 loss is 0.014230027794837952\n",
            "for batch6419 loss is 0.019111594185233116\n",
            "for batch6420 loss is 0.0015904257306829095\n",
            "for batch6421 loss is 0.0014656985877081752\n",
            "for batch6422 loss is 0.3443223536014557\n",
            "for batch6423 loss is 0.29481589794158936\n",
            "for batch6424 loss is 0.003383918898180127\n",
            "for batch6425 loss is 0.5745211839675903\n",
            "for batch6426 loss is 0.09844259172677994\n",
            "for batch6427 loss is 0.7194586396217346\n",
            "for batch6428 loss is 1.4426991939544678\n",
            "for batch6429 loss is 0.024998562410473824\n",
            "for batch6430 loss is 0.007409336976706982\n",
            "for batch6431 loss is 0.1573602855205536\n",
            "for batch6432 loss is 0.007209478411823511\n",
            "for batch6433 loss is 0.30923399329185486\n",
            "for batch6434 loss is 0.01412358321249485\n",
            "for batch6435 loss is 0.005940052680671215\n",
            "for batch6436 loss is 0.44465717673301697\n",
            "for batch6437 loss is 0.00028704313444904983\n",
            "for batch6438 loss is 0.0019444527570158243\n",
            "for batch6439 loss is 2.7894870981981512e-06\n",
            "for batch6440 loss is 0.01899954304099083\n",
            "for batch6441 loss is 0.05455561354756355\n",
            "for batch6442 loss is 0.0187351293861866\n",
            "for batch6443 loss is 2.384185648907078e-08\n",
            "for batch6444 loss is 0.0030754762701690197\n",
            "for batch6445 loss is 0.08092758804559708\n",
            "for batch6446 loss is 0.03749976307153702\n",
            "for batch6447 loss is 0.062085993587970734\n",
            "for batch6448 loss is 0.5847466588020325\n",
            "for batch6449 loss is 0.00023047198192216456\n",
            "for batch6450 loss is 0.04118839651346207\n",
            "for batch6451 loss is 0.007577104959636927\n",
            "for batch6452 loss is 0.0037342156283557415\n",
            "for batch6453 loss is 0.044354476034641266\n",
            "for batch6454 loss is 0.02523430623114109\n",
            "for batch6455 loss is 0.7766502499580383\n",
            "for batch6456 loss is 0.19762958586215973\n",
            "for batch6457 loss is 0.5514190793037415\n",
            "for batch6458 loss is 0.11534459888935089\n",
            "for batch6459 loss is 0.004487678874284029\n",
            "for batch6460 loss is 0.09876998513936996\n",
            "for batch6461 loss is 0.0316954031586647\n",
            "for batch6462 loss is 0.0\n",
            "for batch6463 loss is 0.669366717338562\n",
            "for batch6464 loss is 0.04346184805035591\n",
            "for batch6465 loss is 0.0022164706606417894\n",
            "for batch6466 loss is 0.06711309403181076\n",
            "for batch6467 loss is 0.009746890515089035\n",
            "for batch6468 loss is 0.3781586289405823\n",
            "for batch6469 loss is 0.06356410682201385\n",
            "for batch6470 loss is 0.0509849488735199\n",
            "for batch6471 loss is 0.011662319302558899\n",
            "for batch6472 loss is 0.07628150284290314\n",
            "for batch6473 loss is 0.0413268581032753\n",
            "for batch6474 loss is 1.0437281131744385\n",
            "for batch6475 loss is 0.38430696725845337\n",
            "for batch6476 loss is 0.028466815128922462\n",
            "for batch6477 loss is 0.08403325080871582\n",
            "for batch6478 loss is 0.012411984615027905\n",
            "for batch6479 loss is 4.577591880661203e-06\n",
            "for batch6480 loss is 0.4267093241214752\n",
            "for batch6481 loss is 0.011563083156943321\n",
            "for batch6482 loss is 3.147110419376986e-06\n",
            "for batch6483 loss is 0.0015742348041385412\n",
            "for batch6484 loss is 0.003509854432195425\n",
            "for batch6485 loss is 0.5042254328727722\n",
            "for batch6486 loss is 0.000685216742567718\n",
            "for batch6487 loss is 0.41952162981033325\n",
            "for batch6488 loss is 0.016855692490935326\n",
            "for batch6489 loss is 0.0202731192111969\n",
            "for batch6490 loss is 0.2104450762271881\n",
            "for batch6491 loss is 0.07541422545909882\n",
            "for batch6492 loss is 0.42938345670700073\n",
            "for batch6493 loss is 0.053279727697372437\n",
            "for batch6494 loss is 0.00849617924541235\n",
            "for batch6495 loss is 0.033635783940553665\n",
            "for batch6496 loss is 0.020446116104722023\n",
            "for batch6497 loss is 0.012913884595036507\n",
            "for batch6498 loss is 2.4364921046071686e-05\n",
            "for batch6499 loss is 0.1974618285894394\n",
            "for batch6500 loss is 0.0007031136192381382\n",
            "for batch6501 loss is 0.14733697474002838\n",
            "for batch6502 loss is 1.3065075108897872e-05\n",
            "for batch6503 loss is 0.13151559233665466\n",
            "for batch6504 loss is 0.13088586926460266\n",
            "for batch6505 loss is 0.004947904497385025\n",
            "for batch6506 loss is 0.13481880724430084\n",
            "for batch6507 loss is 0.0008558131521567702\n",
            "for batch6508 loss is 0.009270265698432922\n",
            "for batch6509 loss is 6.961702638363931e-06\n",
            "for batch6510 loss is 0.08969305455684662\n",
            "for batch6511 loss is 0.19104993343353271\n",
            "for batch6512 loss is 0.11447183042764664\n",
            "for batch6513 loss is 0.002453484106808901\n",
            "for batch6514 loss is 0.08912566304206848\n",
            "for batch6515 loss is 1.9811790480162017e-05\n",
            "for batch6516 loss is 0.08640439808368683\n",
            "for batch6517 loss is 0.00789602380245924\n",
            "for batch6518 loss is 0.5045715570449829\n",
            "for batch6519 loss is 0.07884998619556427\n",
            "for batch6520 loss is 0.15902534127235413\n",
            "for batch6521 loss is 0.18005605041980743\n",
            "for batch6522 loss is 0.061382196843624115\n",
            "for batch6523 loss is 0.0031402718741446733\n",
            "for batch6524 loss is 0.04980800673365593\n",
            "for batch6525 loss is 1.0133326053619385\n",
            "for batch6526 loss is 0.0024169033858925104\n",
            "for batch6527 loss is 0.009791373275220394\n",
            "for batch6528 loss is 0.00040547270327806473\n",
            "for batch6529 loss is 0.2489815652370453\n",
            "for batch6530 loss is 0.0017585486639291048\n",
            "for batch6531 loss is 0.07286528497934341\n",
            "for batch6532 loss is 0.005659613758325577\n",
            "for batch6533 loss is 0.9573710560798645\n",
            "for batch6534 loss is 0.055252574384212494\n",
            "for batch6535 loss is 0.005917884409427643\n",
            "for batch6536 loss is 0.2182638943195343\n",
            "for batch6537 loss is 0.006980844773352146\n",
            "for batch6538 loss is 0.03919294476509094\n",
            "for batch6539 loss is 0.0002568779163993895\n",
            "for batch6540 loss is 0.03984510898590088\n",
            "for batch6541 loss is 0.21194013953208923\n",
            "for batch6542 loss is 0.010157590731978416\n",
            "for batch6543 loss is 0.06476515531539917\n",
            "for batch6544 loss is 0.002941673621535301\n",
            "for batch6545 loss is 0.0001953481259988621\n",
            "for batch6546 loss is 0.08698977530002594\n",
            "for batch6547 loss is 0.091737762093544\n",
            "for batch6548 loss is 0.00015661613724660128\n",
            "for batch6549 loss is 0.0004123052640352398\n",
            "for batch6550 loss is 0.00564030883833766\n",
            "for batch6551 loss is 0.688207745552063\n",
            "for batch6552 loss is 0.06790858507156372\n",
            "for batch6553 loss is 2.2744450689060614e-05\n",
            "for batch6554 loss is 0.4384768009185791\n",
            "for batch6555 loss is 0.00021383776038419455\n",
            "for batch6556 loss is 0.0030456390231847763\n",
            "for batch6557 loss is 0.004531348589807749\n",
            "for batch6558 loss is 0.013134462758898735\n",
            "for batch6559 loss is 0.01952539011836052\n",
            "for batch6560 loss is 0.18066683411598206\n",
            "for batch6561 loss is 0.013116732239723206\n",
            "for batch6562 loss is 0.023106418550014496\n",
            "for batch6563 loss is 0.046516720205545425\n",
            "for batch6564 loss is 8.813395106699318e-05\n",
            "for batch6565 loss is 0.002741685602813959\n",
            "for batch6566 loss is 0.037943098694086075\n",
            "for batch6567 loss is 0.004933263640850782\n",
            "for batch6568 loss is 0.00033467417233623564\n",
            "for batch6569 loss is 0.33305537700653076\n",
            "for batch6570 loss is 0.012069630436599255\n",
            "for batch6571 loss is 0.004405231215059757\n",
            "for batch6572 loss is 0.036607541143894196\n",
            "for batch6573 loss is 0.0032271489035338163\n",
            "for batch6574 loss is 0.0273439921438694\n",
            "for batch6575 loss is 0.27844706177711487\n",
            "for batch6576 loss is 0.001103725517168641\n",
            "for batch6577 loss is 0.007967489771544933\n",
            "for batch6578 loss is 0.28580281138420105\n",
            "for batch6579 loss is 0.0004977795179001987\n",
            "for batch6580 loss is 0.16451838612556458\n",
            "for batch6581 loss is 9.334295464213938e-05\n",
            "for batch6582 loss is 0.16388720273971558\n",
            "for batch6583 loss is 0.19225788116455078\n",
            "for batch6584 loss is 0.0007128852303139865\n",
            "for batch6585 loss is 0.01718020811676979\n",
            "for batch6586 loss is 0.029612604528665543\n",
            "for batch6587 loss is 0.19363035261631012\n",
            "for batch6588 loss is 0.2343081682920456\n",
            "for batch6589 loss is 0.08210166543722153\n",
            "for batch6590 loss is 0.08109509199857712\n",
            "for batch6591 loss is 0.0013673931825906038\n",
            "for batch6592 loss is 8.911094482755288e-05\n",
            "for batch6593 loss is 0.39498916268348694\n",
            "for batch6594 loss is 0.01095796748995781\n",
            "for batch6595 loss is 0.0043005794286727905\n",
            "for batch6596 loss is 0.4242720603942871\n",
            "for batch6597 loss is 0.0023740259930491447\n",
            "for batch6598 loss is 0.1394987404346466\n",
            "for batch6599 loss is 0.13511595129966736\n",
            "for batch6600 loss is 0.0031147173140197992\n",
            "for batch6601 loss is 3.9096827094908804e-05\n",
            "for batch6602 loss is 0.15489086508750916\n",
            "for batch6603 loss is 0.02143290266394615\n",
            "for batch6604 loss is 0.48677706718444824\n",
            "for batch6605 loss is 0.23186902701854706\n",
            "for batch6606 loss is 0.6454945206642151\n",
            "for batch6607 loss is 0.10819755494594574\n",
            "for batch6608 loss is 0.8127187490463257\n",
            "for batch6609 loss is 0.2193334549665451\n",
            "for batch6610 loss is 0.9276919364929199\n",
            "for batch6611 loss is 0.2612048387527466\n",
            "for batch6612 loss is 0.000230465637287125\n",
            "for batch6613 loss is 0.002157993149012327\n",
            "for batch6614 loss is 1.1134000487800222e-05\n",
            "for batch6615 loss is 0.03144093230366707\n",
            "for batch6616 loss is 0.3092905580997467\n",
            "for batch6617 loss is 0.015298995189368725\n",
            "for batch6618 loss is 0.03640999272465706\n",
            "for batch6619 loss is 8.225322744692676e-06\n",
            "for batch6620 loss is 0.36473023891448975\n",
            "for batch6621 loss is 0.044916942715644836\n",
            "for batch6622 loss is 0.011521512642502785\n",
            "for batch6623 loss is 0.009322059340775013\n",
            "for batch6624 loss is 0.02472338080406189\n",
            "for batch6625 loss is 0.29756659269332886\n",
            "for batch6626 loss is 0.21233665943145752\n",
            "for batch6627 loss is 0.01291891373693943\n",
            "for batch6628 loss is 0.007265270687639713\n",
            "for batch6629 loss is 0.005108530633151531\n",
            "for batch6630 loss is 0.03298775106668472\n",
            "for batch6631 loss is 0.04286482185125351\n",
            "for batch6632 loss is 0.022209951654076576\n",
            "for batch6633 loss is 0.022819142788648605\n",
            "for batch6634 loss is 0.0865587517619133\n",
            "for batch6635 loss is 0.03689452260732651\n",
            "for batch6636 loss is 0.36974796652793884\n",
            "for batch6637 loss is 0.017940500751137733\n",
            "for batch6638 loss is 0.006752571556717157\n",
            "for batch6639 loss is 0.0002507207100279629\n",
            "for batch6640 loss is 0.01698494330048561\n",
            "for batch6641 loss is 0.007765479385852814\n",
            "for batch6642 loss is 0.00019636048818938434\n",
            "for batch6643 loss is 0.10682356357574463\n",
            "for batch6644 loss is 5.865013008587994e-06\n",
            "for batch6645 loss is 0.2031475305557251\n",
            "for batch6646 loss is 0.7842143774032593\n",
            "for batch6647 loss is 3.7571840948658064e-05\n",
            "for batch6648 loss is 0.00015308178262785077\n",
            "for batch6649 loss is 4.789255763171241e-05\n",
            "for batch6650 loss is 0.021954338997602463\n",
            "for batch6651 loss is 0.03095703385770321\n",
            "for batch6652 loss is 0.7606313228607178\n",
            "for batch6653 loss is 0.006138024851679802\n",
            "for batch6654 loss is 0.04315739497542381\n",
            "for batch6655 loss is 0.05987962335348129\n",
            "for batch6656 loss is 0.30900925397872925\n",
            "for batch6657 loss is 0.10767658799886703\n",
            "for batch6658 loss is 0.5633834004402161\n",
            "for batch6659 loss is 0.35327765345573425\n",
            "for batch6660 loss is 0.016202079132199287\n",
            "for batch6661 loss is 5.3136442147661e-05\n",
            "for batch6662 loss is 0.05734945461153984\n",
            "for batch6663 loss is 4.314911348046735e-05\n",
            "for batch6664 loss is 0.0014849122380837798\n",
            "for batch6665 loss is 0.00933186523616314\n",
            "for batch6666 loss is 0.012801473960280418\n",
            "for batch6667 loss is 0.00024347472935914993\n",
            "for batch6668 loss is 0.11074210703372955\n",
            "for batch6669 loss is 1.2898590564727783\n",
            "for batch6670 loss is 0.0005840656813234091\n",
            "for batch6671 loss is 0.0001971229212358594\n",
            "for batch6672 loss is 0.012771882116794586\n",
            "for batch6673 loss is 0.1736498773097992\n",
            "for batch6674 loss is 0.021165383979678154\n",
            "for batch6675 loss is 0.039315346628427505\n",
            "for batch6676 loss is 0.31880176067352295\n",
            "for batch6677 loss is 0.0463075153529644\n",
            "for batch6678 loss is 0.016929911449551582\n",
            "for batch6679 loss is 0.26630643010139465\n",
            "for batch6680 loss is 1.4479255676269531\n",
            "for batch6681 loss is 0.20346125960350037\n",
            "for batch6682 loss is 0.010872562415897846\n",
            "for batch6683 loss is 1.1682393960654736e-05\n",
            "for batch6684 loss is 0.05957566574215889\n",
            "for batch6685 loss is 0.18342456221580505\n",
            "for batch6686 loss is 0.02369442768394947\n",
            "for batch6687 loss is 0.2199104279279709\n",
            "for batch6688 loss is 0.0008173744427040219\n",
            "for batch6689 loss is 0.012306963093578815\n",
            "for batch6690 loss is 0.3933497667312622\n",
            "for batch6691 loss is 0.09373903274536133\n",
            "for batch6692 loss is 3.18029779009521e-05\n",
            "for batch6693 loss is 0.17626039683818817\n",
            "for batch6694 loss is 0.1836545169353485\n",
            "for batch6695 loss is 0.04330633580684662\n",
            "for batch6696 loss is 0.006246196571737528\n",
            "for batch6697 loss is 0.01648741029202938\n",
            "for batch6698 loss is 0.10946303606033325\n",
            "for batch6699 loss is 0.000859500840306282\n",
            "for batch6700 loss is 0.0001875239104265347\n",
            "for batch6701 loss is 0.049277253448963165\n",
            "for batch6702 loss is 0.38118019700050354\n",
            "for batch6703 loss is 1.0669982433319092\n",
            "for batch6704 loss is 1.3971050975669641e-05\n",
            "for batch6705 loss is 0.3803018629550934\n",
            "for batch6706 loss is 0.8383505940437317\n",
            "for batch6707 loss is 0.023394914343953133\n",
            "for batch6708 loss is 0.15004603564739227\n",
            "for batch6709 loss is 0.07658813893795013\n",
            "for batch6710 loss is 0.006594897713512182\n",
            "for batch6711 loss is 0.7997705936431885\n",
            "for batch6712 loss is 0.13042305409908295\n",
            "for batch6713 loss is 0.027736011892557144\n",
            "for batch6714 loss is 0.0008076507365331054\n",
            "for batch6715 loss is 0.014405217953026295\n",
            "for batch6716 loss is 0.004134034272283316\n",
            "for batch6717 loss is 0.9555872082710266\n",
            "for batch6718 loss is 0.005052906461060047\n",
            "for batch6719 loss is 0.005051407031714916\n",
            "for batch6720 loss is 0.0027904012240469456\n",
            "for batch6721 loss is 0.0010930311400443316\n",
            "for batch6722 loss is 0.05811021476984024\n",
            "for batch6723 loss is 0.2927435040473938\n",
            "for batch6724 loss is 0.01778983324766159\n",
            "for batch6725 loss is 0.0817783996462822\n",
            "for batch6726 loss is 0.0009956236463040113\n",
            "for batch6727 loss is 0.5810586214065552\n",
            "for batch6728 loss is 0.07433757930994034\n",
            "for batch6729 loss is 0.18574868142604828\n",
            "for batch6730 loss is 0.02094857208430767\n",
            "for batch6731 loss is 0.02175338938832283\n",
            "for batch6732 loss is 0.011022338643670082\n",
            "for batch6733 loss is 0.012740629725158215\n",
            "for batch6734 loss is 0.036259204149246216\n",
            "for batch6735 loss is 0.013497436419129372\n",
            "for batch6736 loss is 0.24004356563091278\n",
            "for batch6737 loss is 0.008007614873349667\n",
            "for batch6738 loss is 0.16514113545417786\n",
            "for batch6739 loss is 6.324418063741177e-05\n",
            "for batch6740 loss is 0.7128430604934692\n",
            "for batch6741 loss is 1.657504677772522\n",
            "for batch6742 loss is 0.09517330676317215\n",
            "for batch6743 loss is 0.013837305828928947\n",
            "for batch6744 loss is 0.3085750937461853\n",
            "for batch6745 loss is 0.026240000501275063\n",
            "for batch6746 loss is 0.003032118082046509\n",
            "for batch6747 loss is 0.02493329718708992\n",
            "for batch6748 loss is 0.03250404819846153\n",
            "for batch6749 loss is 0.0005830366862937808\n",
            "for batch6750 loss is 0.3424857258796692\n",
            "for batch6751 loss is 0.004912080708891153\n",
            "for batch6752 loss is 0.8761787414550781\n",
            "for batch6753 loss is 4.5775841499562375e-06\n",
            "for batch6754 loss is 0.2760525345802307\n",
            "for batch6755 loss is 0.009174728766083717\n",
            "for batch6756 loss is 5.8499776059761643e-05\n",
            "for batch6757 loss is 0.006806348916143179\n",
            "for batch6758 loss is 0.00022937016910873353\n",
            "for batch6759 loss is 0.08415647596120834\n",
            "for batch6760 loss is 5.3112707973923534e-05\n",
            "for batch6761 loss is 3.320906398585066e-05\n",
            "for batch6762 loss is 0.05016053467988968\n",
            "for batch6763 loss is 0.1243508830666542\n",
            "for batch6764 loss is 0.003925045020878315\n",
            "for batch6765 loss is 0.5627936124801636\n",
            "for batch6766 loss is 0.011290060356259346\n",
            "for batch6767 loss is 0.0002842364483512938\n",
            "for batch6768 loss is 0.3288528621196747\n",
            "for batch6769 loss is 0.04041623696684837\n",
            "for batch6770 loss is 0.3399881422519684\n",
            "for batch6771 loss is 0.018909592181444168\n",
            "for batch6772 loss is 0.008446390740573406\n",
            "for batch6773 loss is 0.7835986018180847\n",
            "for batch6774 loss is 0.00202481122687459\n",
            "for batch6775 loss is 0.006091936491429806\n",
            "for batch6776 loss is 0.007356805261224508\n",
            "for batch6777 loss is 0.0038801853079348803\n",
            "for batch6778 loss is 0.3379293382167816\n",
            "for batch6779 loss is 0.007965446449816227\n",
            "for batch6780 loss is 0.0001380259927827865\n",
            "for batch6781 loss is 0.3312193751335144\n",
            "for batch6782 loss is 0.543327808380127\n",
            "for batch6783 loss is 8.863141556503251e-05\n",
            "for batch6784 loss is 5.006786523154005e-07\n",
            "for batch6785 loss is 0.11384210735559464\n",
            "for batch6786 loss is 0.004529121331870556\n",
            "for batch6787 loss is 0.047862112522125244\n",
            "for batch6788 loss is 0.010865114629268646\n",
            "for batch6789 loss is 0.001192607218399644\n",
            "for batch6790 loss is 0.048450928181409836\n",
            "for batch6791 loss is 0.08432801812887192\n",
            "for batch6792 loss is 0.0008603452006354928\n",
            "for batch6793 loss is 0.0014807390980422497\n",
            "for batch6794 loss is 0.13776488602161407\n",
            "for batch6795 loss is 0.012197902426123619\n",
            "for batch6796 loss is 9.53674117454284e-08\n",
            "for batch6797 loss is 0.0034542556386440992\n",
            "for batch6798 loss is 0.020141469314694405\n",
            "for batch6799 loss is 0.2730660140514374\n",
            "for batch6800 loss is 0.019604207947850227\n",
            "for batch6801 loss is 0.11903315782546997\n",
            "for batch6802 loss is 0.032484881579875946\n",
            "for batch6803 loss is 1.1730000551324338e-05\n",
            "for batch6804 loss is 0.000461414223536849\n",
            "for batch6805 loss is 0.3046674430370331\n",
            "for batch6806 loss is 0.045568183064460754\n",
            "for batch6807 loss is 0.0009667868725955486\n",
            "for batch6808 loss is 0.20588186383247375\n",
            "for batch6809 loss is 8.177668860298581e-06\n",
            "for batch6810 loss is 0.000319321290589869\n",
            "for batch6811 loss is 0.01623508147895336\n",
            "for batch6812 loss is 0.23286378383636475\n",
            "for batch6813 loss is 4.0531116951569857e-07\n",
            "for batch6814 loss is 9.201942157233134e-05\n",
            "for batch6815 loss is 0.17846174538135529\n",
            "for batch6816 loss is 0.017482537776231766\n",
            "for batch6817 loss is 0.036118265241384506\n",
            "for batch6818 loss is 0.03503691777586937\n",
            "for batch6819 loss is 0.0012585639487951994\n",
            "for batch6820 loss is 0.5366309881210327\n",
            "for batch6821 loss is 6.23129089944996e-05\n",
            "for batch6822 loss is 0.0008757634204812348\n",
            "for batch6823 loss is 1.022430419921875\n",
            "for batch6824 loss is 0.020503805950284004\n",
            "for batch6825 loss is 0.11455242335796356\n",
            "for batch6826 loss is 0.030942842364311218\n",
            "for batch6827 loss is 0.0005546441534534097\n",
            "for batch6828 loss is 0.005625487770885229\n",
            "for batch6829 loss is 0.1816735565662384\n",
            "for batch6830 loss is 0.4060543179512024\n",
            "for batch6831 loss is 0.29710692167282104\n",
            "for batch6832 loss is 0.21596050262451172\n",
            "for batch6833 loss is 0.15396282076835632\n",
            "for batch6834 loss is 0.14565548300743103\n",
            "for batch6835 loss is 0.002014805795624852\n",
            "for batch6836 loss is 0.07824134081602097\n",
            "for batch6837 loss is 0.003934126812964678\n",
            "for batch6838 loss is 0.0003479397273622453\n",
            "for batch6839 loss is 0.1128208264708519\n",
            "for batch6840 loss is 0.0013168791774660349\n",
            "for batch6841 loss is 2.312647211510921e-06\n",
            "for batch6842 loss is 0.004015921615064144\n",
            "for batch6843 loss is 0.5183966159820557\n",
            "for batch6844 loss is 0.020046841353178024\n",
            "for batch6845 loss is 0.03459801524877548\n",
            "for batch6846 loss is 0.0006921663298271596\n",
            "for batch6847 loss is 0.001850261352956295\n",
            "for batch6848 loss is 0.9347043037414551\n",
            "for batch6849 loss is 0.10601216554641724\n",
            "for batch6850 loss is 0.004905344918370247\n",
            "for batch6851 loss is 0.11857541650533676\n",
            "for batch6852 loss is 0.03717584162950516\n",
            "for batch6853 loss is 1.1920926823449918e-07\n",
            "for batch6854 loss is 0.08684219419956207\n",
            "for batch6855 loss is 0.045621417462825775\n",
            "for batch6856 loss is 0.12371788173913956\n",
            "for batch6857 loss is 0.7822384238243103\n",
            "for batch6858 loss is 2.3603304271091474e-06\n",
            "for batch6859 loss is 0.019712204113602638\n",
            "for batch6860 loss is 0.058444224298000336\n",
            "for batch6861 loss is 1.3208011296228506e-05\n",
            "for batch6862 loss is 0.006053623277693987\n",
            "for batch6863 loss is 0.2500317096710205\n",
            "for batch6864 loss is 0.016327541321516037\n",
            "for batch6865 loss is 0.010197212919592857\n",
            "for batch6866 loss is 2.88484625343699e-06\n",
            "for batch6867 loss is 0.0022010784596204758\n",
            "for batch6868 loss is 0.000489331257995218\n",
            "for batch6869 loss is 0.00730599369853735\n",
            "for batch6870 loss is 0.0003436473780311644\n",
            "for batch6871 loss is 0.26923808455467224\n",
            "for batch6872 loss is 0.0005602162564173341\n",
            "for batch6873 loss is 0.007526122033596039\n",
            "for batch6874 loss is 0.004085259977728128\n",
            "for batch6875 loss is 0.26960885524749756\n",
            "for batch6876 loss is 0.046742670238018036\n",
            "for batch6877 loss is 0.3431374430656433\n",
            "for batch6878 loss is 0.0016200302634388208\n",
            "for batch6879 loss is 1.4788793325424194\n",
            "for batch6880 loss is 0.00026706530479714274\n",
            "for batch6881 loss is 0.05106933042407036\n",
            "for batch6882 loss is 0.010486043989658356\n",
            "for batch6883 loss is 0.003945434000343084\n",
            "for batch6884 loss is 7.152556236178498e-08\n",
            "for batch6885 loss is 0.5329504013061523\n",
            "for batch6886 loss is 0.49973732233047485\n",
            "for batch6887 loss is 0.0016765696927905083\n",
            "for batch6888 loss is 0.03672711178660393\n",
            "for batch6889 loss is 0.47120293974876404\n",
            "for batch6890 loss is 0.006165007129311562\n",
            "for batch6891 loss is 0.08234259486198425\n",
            "for batch6892 loss is 8.400539809372276e-05\n",
            "for batch6893 loss is 0.4330623149871826\n",
            "for batch6894 loss is 0.000216637272387743\n",
            "for batch6895 loss is 0.126725971698761\n",
            "for batch6896 loss is 0.0\n",
            "for batch6897 loss is 0.06265351921319962\n",
            "for batch6898 loss is 0.07830770313739777\n",
            "for batch6899 loss is 0.056184738874435425\n",
            "for batch6900 loss is 0.12260954082012177\n",
            "for batch6901 loss is 0.0001892149302875623\n",
            "for batch6902 loss is 0.08299852907657623\n",
            "for batch6903 loss is 1.616444751562085e-05\n",
            "for batch6904 loss is 0.3043661117553711\n",
            "for batch6905 loss is 0.00011098744289483875\n",
            "for batch6906 loss is 0.0018859386909753084\n",
            "for batch6907 loss is 0.13992172479629517\n",
            "for batch6908 loss is 0.08884496986865997\n",
            "for batch6909 loss is 0.825384259223938\n",
            "for batch6910 loss is 0.0001038799891830422\n",
            "for batch6911 loss is 2.121917987096822e-06\n",
            "for batch6912 loss is 0.010935798287391663\n",
            "for batch6913 loss is 0.007871278561651707\n",
            "for batch6914 loss is 1.6794809103012085\n",
            "for batch6915 loss is 0.012983334250748158\n",
            "for batch6916 loss is 0.03757920861244202\n",
            "for batch6917 loss is 0.01649363338947296\n",
            "for batch6918 loss is 0.001833211281336844\n",
            "for batch6919 loss is 0.020724378526210785\n",
            "for batch6920 loss is 0.001217360608279705\n",
            "for batch6921 loss is 0.13216465711593628\n",
            "for batch6922 loss is 0.265371710062027\n",
            "for batch6923 loss is 0.012013992294669151\n",
            "for batch6924 loss is 0.08573640882968903\n",
            "for batch6925 loss is 0.03688462823629379\n",
            "for batch6926 loss is 0.04931427538394928\n",
            "for batch6927 loss is 0.09198638796806335\n",
            "for batch6928 loss is 0.6796108484268188\n",
            "for batch6929 loss is 0.005538328550755978\n",
            "for batch6930 loss is 0.41845273971557617\n",
            "for batch6931 loss is 0.07581654191017151\n",
            "for batch6932 loss is 0.011536357924342155\n",
            "for batch6933 loss is 0.016602613031864166\n",
            "for batch6934 loss is 0.06047540903091431\n",
            "for batch6935 loss is 0.15222284197807312\n",
            "for batch6936 loss is 0.023273399099707603\n",
            "for batch6937 loss is 0.09108676016330719\n",
            "for batch6938 loss is 0.06118981912732124\n",
            "for batch6939 loss is 0.3163500428199768\n",
            "for batch6940 loss is 0.010185999795794487\n",
            "for batch6941 loss is 0.06834084540605545\n",
            "for batch6942 loss is 0.016861189156770706\n",
            "for batch6943 loss is 0.20975252985954285\n",
            "for batch6944 loss is 0.18003146350383759\n",
            "for batch6945 loss is 0.0036713420413434505\n",
            "for batch6946 loss is 0.2791387736797333\n",
            "for batch6947 loss is 0.07880939543247223\n",
            "for batch6948 loss is 0.10213835537433624\n",
            "for batch6949 loss is 6.643963570240885e-05\n",
            "for batch6950 loss is 0.5780797004699707\n",
            "for batch6951 loss is 0.015427382662892342\n",
            "for batch6952 loss is 0.0002185038465540856\n",
            "for batch6953 loss is 0.36321982741355896\n",
            "for batch6954 loss is 0.1268042027950287\n",
            "for batch6955 loss is 0.03839428350329399\n",
            "for batch6956 loss is 0.02757403254508972\n",
            "for batch6957 loss is 0.03452112525701523\n",
            "for batch6958 loss is 0.001098930835723877\n",
            "for batch6959 loss is 4.7683664661235525e-07\n",
            "for batch6960 loss is 0.03343891724944115\n",
            "for batch6961 loss is 0.09463918209075928\n",
            "for batch6962 loss is 0.0008761502685956657\n",
            "for batch6963 loss is 0.08362738788127899\n",
            "for batch6964 loss is 0.008205873891711235\n",
            "for batch6965 loss is 0.42739295959472656\n",
            "for batch6966 loss is 0.22747871279716492\n",
            "for batch6967 loss is 0.002040388761088252\n",
            "for batch6968 loss is 0.03689875826239586\n",
            "for batch6969 loss is 0.0003217968042008579\n",
            "for batch6970 loss is 0.026218298822641373\n",
            "for batch6971 loss is 0.5296391844749451\n",
            "for batch6972 loss is 0.21879854798316956\n",
            "for batch6973 loss is 0.0011031556641682982\n",
            "for batch6974 loss is 0.22029206156730652\n",
            "for batch6975 loss is 0.8116990923881531\n",
            "for batch6976 loss is 0.07261943072080612\n",
            "for batch6977 loss is 0.06419816613197327\n",
            "for batch6978 loss is 0.06474288552999496\n",
            "for batch6979 loss is 0.08805239945650101\n",
            "for batch6980 loss is 0.005531882867217064\n",
            "for batch6981 loss is 0.972024142742157\n",
            "for batch6982 loss is 0.02371455729007721\n",
            "for batch6983 loss is 0.07653908431529999\n",
            "for batch6984 loss is 0.6639379858970642\n",
            "for batch6985 loss is 0.04238424822688103\n",
            "for batch6986 loss is 0.12815168499946594\n",
            "for batch6987 loss is 0.06130523234605789\n",
            "for batch6988 loss is 0.00948103703558445\n",
            "for batch6989 loss is 0.629320502281189\n",
            "for batch6990 loss is 0.0008701765909790993\n",
            "for batch6991 loss is 0.3113807439804077\n",
            "for batch6992 loss is 0.06561938673257828\n",
            "for batch6993 loss is 0.016722075641155243\n",
            "for batch6994 loss is 0.00502749253064394\n",
            "for batch6995 loss is 0.001083199167624116\n",
            "for batch6996 loss is 0.008368868380784988\n",
            "for batch6997 loss is 0.13608035445213318\n",
            "for batch6998 loss is 0.010207656770944595\n",
            "for batch6999 loss is 0.1715715378522873\n",
            "for batch7000 loss is 0.03207055479288101\n",
            "for batch7001 loss is 0.176198810338974\n",
            "for batch7002 loss is 0.006007485091686249\n",
            "for batch7003 loss is 0.1890198290348053\n",
            "for batch7004 loss is 0.19882768392562866\n",
            "for batch7005 loss is 0.0010195018257945776\n",
            "for batch7006 loss is 3.7429210351547226e-05\n",
            "for batch7007 loss is 0.06450046598911285\n",
            "for batch7008 loss is 0.6877670884132385\n",
            "for batch7009 loss is 1.0928281545639038\n",
            "for batch7010 loss is 0.23328471183776855\n",
            "for batch7011 loss is 0.0006633758312091231\n",
            "for batch7012 loss is 0.003366860095411539\n",
            "for batch7013 loss is 0.0010457177413627505\n",
            "for batch7014 loss is 0.1824563592672348\n",
            "for batch7015 loss is 0.004240440670400858\n",
            "for batch7016 loss is 5.487882299348712e-05\n",
            "for batch7017 loss is 0.06653879582881927\n",
            "for batch7018 loss is 0.03602609783411026\n",
            "for batch7019 loss is 3.309093153802678e-05\n",
            "for batch7020 loss is 0.11722533404827118\n",
            "for batch7021 loss is 0.3073441982269287\n",
            "for batch7022 loss is 0.01649690791964531\n",
            "for batch7023 loss is 7.390967766696122e-07\n",
            "for batch7024 loss is 0.12477557361125946\n",
            "for batch7025 loss is 0.22415697574615479\n",
            "for batch7026 loss is 0.005917602218687534\n",
            "for batch7027 loss is 0.0013098138151690364\n",
            "for batch7028 loss is 0.002855111612007022\n",
            "for batch7029 loss is 0.0017874168697744608\n",
            "for batch7030 loss is 0.012633177451789379\n",
            "for batch7031 loss is 0.4938744902610779\n",
            "for batch7032 loss is 0.03047015145421028\n",
            "for batch7033 loss is 0.6619296073913574\n",
            "for batch7034 loss is 0.0015548455994576216\n",
            "for batch7035 loss is 0.001778649166226387\n",
            "for batch7036 loss is 1.6855499779921956e-05\n",
            "for batch7037 loss is 0.254164457321167\n",
            "for batch7038 loss is 1.7642935290496098e-06\n",
            "for batch7039 loss is 0.3878706693649292\n",
            "for batch7040 loss is 0.031615424901247025\n",
            "for batch7041 loss is 0.3439241349697113\n",
            "for batch7042 loss is 0.18096797168254852\n",
            "for batch7043 loss is 0.0017617978155612946\n",
            "for batch7044 loss is 1.842437505722046\n",
            "for batch7045 loss is 0.0011953151552006602\n",
            "for batch7046 loss is 0.011397577822208405\n",
            "for batch7047 loss is 0.0003605271631386131\n",
            "for batch7048 loss is 0.046747542917728424\n",
            "for batch7049 loss is 0.0007946894620545208\n",
            "for batch7050 loss is 0.009496341459453106\n",
            "for batch7051 loss is 0.2429393082857132\n",
            "for batch7052 loss is 0.0002977775875478983\n",
            "for batch7053 loss is 0.03605774790048599\n",
            "for batch7054 loss is 0.00984618254005909\n",
            "for batch7055 loss is 0.1832754760980606\n",
            "for batch7056 loss is 0.05866878107190132\n",
            "for batch7057 loss is 0.021562810987234116\n",
            "for batch7058 loss is 0.1768127828836441\n",
            "for batch7059 loss is 0.003234077710658312\n",
            "for batch7060 loss is 0.11819352954626083\n",
            "for batch7061 loss is 0.19507953524589539\n",
            "for batch7062 loss is 0.001704003312624991\n",
            "for batch7063 loss is 0.01142814476042986\n",
            "for batch7064 loss is 0.13944517076015472\n",
            "for batch7065 loss is 0.004853334743529558\n",
            "for batch7066 loss is 0.5217810273170471\n",
            "for batch7067 loss is 0.6616339087486267\n",
            "for batch7068 loss is 0.16803549230098724\n",
            "for batch7069 loss is 2.241127276647603e-06\n",
            "for batch7070 loss is 0.012465598993003368\n",
            "for batch7071 loss is 0.10114902257919312\n",
            "for batch7072 loss is 0.05738704651594162\n",
            "for batch7073 loss is 0.08443208038806915\n",
            "for batch7074 loss is 4.808327503269538e-05\n",
            "for batch7075 loss is 0.14690236747264862\n",
            "for batch7076 loss is 0.06833866238594055\n",
            "for batch7077 loss is 2.1837950043845922e-05\n",
            "for batch7078 loss is 0.026995360851287842\n",
            "for batch7079 loss is 0.4898337423801422\n",
            "for batch7080 loss is 0.012737405486404896\n",
            "for batch7081 loss is 0.028627362102270126\n",
            "for batch7082 loss is 0.7149603366851807\n",
            "for batch7083 loss is 0.19613030552864075\n",
            "for batch7084 loss is 0.06675717234611511\n",
            "for batch7085 loss is 0.0037122953217476606\n",
            "for batch7086 loss is 0.07949749380350113\n",
            "for batch7087 loss is 0.0016735552344471216\n",
            "for batch7088 loss is 0.382661372423172\n",
            "for batch7089 loss is 0.058708615601062775\n",
            "for batch7090 loss is 2.2649635411653435e-06\n",
            "for batch7091 loss is 0.019163740798830986\n",
            "for batch7092 loss is 0.0021100989542901516\n",
            "for batch7093 loss is 0.0008453275077044964\n",
            "for batch7094 loss is 0.8123313784599304\n",
            "for batch7095 loss is 0.062315940856933594\n",
            "for batch7096 loss is 0.12898039817810059\n",
            "for batch7097 loss is 0.23564596474170685\n",
            "for batch7098 loss is 0.112143874168396\n",
            "for batch7099 loss is 0.00011814918980235234\n",
            "for batch7100 loss is 0.03279720991849899\n",
            "for batch7101 loss is 0.42598262429237366\n",
            "for batch7102 loss is 0.09676177054643631\n",
            "for batch7103 loss is 0.0006633949815295637\n",
            "for batch7104 loss is 0.8464022874832153\n",
            "for batch7105 loss is 0.005348737817257643\n",
            "for batch7106 loss is 0.0772973969578743\n",
            "for batch7107 loss is 0.0067551955580711365\n",
            "for batch7108 loss is 0.007387809455394745\n",
            "for batch7109 loss is 0.5263489484786987\n",
            "for batch7110 loss is 0.01723729632794857\n",
            "for batch7111 loss is 0.0026851887814700603\n",
            "for batch7112 loss is 0.023741628974676132\n",
            "for batch7113 loss is 0.004151699598878622\n",
            "for batch7114 loss is 1.945420262927655e-05\n",
            "for batch7115 loss is 0.0016678838292136788\n",
            "for batch7116 loss is 0.00425382237881422\n",
            "for batch7117 loss is 0.007711365818977356\n",
            "for batch7118 loss is 1.7005592584609985\n",
            "for batch7119 loss is 0.027910852804780006\n",
            "for batch7120 loss is 0.03884515166282654\n",
            "for batch7121 loss is 0.0003967437951359898\n",
            "for batch7122 loss is 0.0007248198380693793\n",
            "for batch7123 loss is 0.001835010014474392\n",
            "for batch7124 loss is 0.0015367453452199697\n",
            "for batch7125 loss is 0.6073414087295532\n",
            "for batch7126 loss is 0.059957973659038544\n",
            "for batch7127 loss is 0.3110331594944\n",
            "for batch7128 loss is 0.02171124331653118\n",
            "for batch7129 loss is 0.00324448524042964\n",
            "for batch7130 loss is 0.00014694014680571854\n",
            "for batch7131 loss is 0.0008745057275518775\n",
            "for batch7132 loss is 0.11815712600946426\n",
            "for batch7133 loss is 0.008872303180396557\n",
            "for batch7134 loss is 0.12409939616918564\n",
            "for batch7135 loss is 0.0009275261545553803\n",
            "for batch7136 loss is 0.01606784202158451\n",
            "for batch7137 loss is 0.19440628588199615\n",
            "for batch7138 loss is 0.0011307952227070928\n",
            "for batch7139 loss is 0.05188032239675522\n",
            "for batch7140 loss is 0.19602659344673157\n",
            "for batch7141 loss is 0.2468750774860382\n",
            "for batch7142 loss is 0.23854617774486542\n",
            "for batch7143 loss is 0.0010083845118060708\n",
            "for batch7144 loss is 0.006540813483297825\n",
            "for batch7145 loss is 0.05749625712633133\n",
            "for batch7146 loss is 2.384185648907078e-08\n",
            "for batch7147 loss is 0.1670224815607071\n",
            "for batch7148 loss is 0.022280190140008926\n",
            "for batch7149 loss is 0.007263964973390102\n",
            "for batch7150 loss is 0.4768245220184326\n",
            "for batch7151 loss is 0.0004939412465319037\n",
            "for batch7152 loss is 0.002019762760028243\n",
            "for batch7153 loss is 0.12462935596704483\n",
            "for batch7154 loss is 0.0026383833028376102\n",
            "for batch7155 loss is 0.04650789499282837\n",
            "for batch7156 loss is 0.46816128492355347\n",
            "for batch7157 loss is 0.3382273018360138\n",
            "for batch7158 loss is 0.0003891091328114271\n",
            "for batch7159 loss is 0.16220232844352722\n",
            "for batch7160 loss is 1.7585909366607666\n",
            "for batch7161 loss is 0.16750209033489227\n",
            "for batch7162 loss is 0.0019748483318835497\n",
            "for batch7163 loss is 0.07220709323883057\n",
            "for batch7164 loss is 0.02398003451526165\n",
            "for batch7165 loss is 0.12272032350301743\n",
            "for batch7166 loss is 0.0\n",
            "for batch7167 loss is 0.0005663004121743143\n",
            "for batch7168 loss is 0.07263844460248947\n",
            "for batch7169 loss is 0.4669221341609955\n",
            "for batch7170 loss is 0.04884355142712593\n",
            "for batch7171 loss is 0.36681872606277466\n",
            "for batch7172 loss is 0.022073155269026756\n",
            "for batch7173 loss is 0.07100944221019745\n",
            "for batch7174 loss is 0.01799999177455902\n",
            "for batch7175 loss is 0.20924802124500275\n",
            "for batch7176 loss is 0.02923852577805519\n",
            "for batch7177 loss is 0.49105149507522583\n",
            "for batch7178 loss is 1.0997508764266968\n",
            "for batch7179 loss is 0.941260814666748\n",
            "for batch7180 loss is 0.2681417465209961\n",
            "for batch7181 loss is 1.8771833181381226\n",
            "for batch7182 loss is 0.00011727465607691556\n",
            "for batch7183 loss is 0.001789973466657102\n",
            "for batch7184 loss is 0.6000196933746338\n",
            "for batch7185 loss is 0.007437877357006073\n",
            "for batch7186 loss is 0.042329028248786926\n",
            "for batch7187 loss is 0.001184677705168724\n",
            "for batch7188 loss is 0.0049821762368083\n",
            "for batch7189 loss is 0.016496581956744194\n",
            "for batch7190 loss is 0.11485196650028229\n",
            "for batch7191 loss is 0.0005286854575388134\n",
            "for batch7192 loss is 0.0182497501373291\n",
            "for batch7193 loss is 4.410720976011362e-06\n",
            "for batch7194 loss is 0.443673700094223\n",
            "for batch7195 loss is 0.004509417340159416\n",
            "for batch7196 loss is 0.0003628463309723884\n",
            "for batch7197 loss is 0.00013554314500652254\n",
            "for batch7198 loss is 0.4631589353084564\n",
            "for batch7199 loss is 0.20445339381694794\n",
            "for batch7200 loss is 0.010528854094445705\n",
            "for batch7201 loss is 0.0010150913149118423\n",
            "for batch7202 loss is 0.1029263287782669\n",
            "for batch7203 loss is 0.043136097490787506\n",
            "for batch7204 loss is 0.1554259955883026\n",
            "for batch7205 loss is 0.09653214365243912\n",
            "for batch7206 loss is 0.40349823236465454\n",
            "for batch7207 loss is 0.3320236802101135\n",
            "for batch7208 loss is 0.3954330086708069\n",
            "for batch7209 loss is 3.3378321404597955e-06\n",
            "for batch7210 loss is 0.18252553045749664\n",
            "for batch7211 loss is 0.005166732706129551\n",
            "for batch7212 loss is 0.08242873847484589\n",
            "for batch7213 loss is 0.005978073459118605\n",
            "for batch7214 loss is 0.0046980115585029125\n",
            "for batch7215 loss is 0.040255386382341385\n",
            "for batch7216 loss is 0.015337055549025536\n",
            "for batch7217 loss is 0.022918617352843285\n",
            "for batch7218 loss is 0.41335025429725647\n",
            "for batch7219 loss is 0.019719555974006653\n",
            "for batch7220 loss is 0.027440333738923073\n",
            "for batch7221 loss is 0.0005966722383163869\n",
            "for batch7222 loss is 0.012620965018868446\n",
            "for batch7223 loss is 0.002554015489295125\n",
            "for batch7224 loss is 0.7479673027992249\n",
            "for batch7225 loss is 0.6241730451583862\n",
            "for batch7226 loss is 0.05273802950978279\n",
            "for batch7227 loss is 0.4674499034881592\n",
            "for batch7228 loss is 0.12584437429904938\n",
            "for batch7229 loss is 1.4018520232639275e-05\n",
            "for batch7230 loss is 0.33097365498542786\n",
            "for batch7231 loss is 0.001797703793272376\n",
            "for batch7232 loss is 0.23187819123268127\n",
            "for batch7233 loss is 0.06807710975408554\n",
            "for batch7234 loss is 0.12079377472400665\n",
            "for batch7235 loss is 0.13924464583396912\n",
            "for batch7236 loss is 8.439840712526347e-06\n",
            "for batch7237 loss is 0.014581045135855675\n",
            "for batch7238 loss is 0.03814311325550079\n",
            "for batch7239 loss is 0.010809567756950855\n",
            "for batch7240 loss is 1.6211804904742166e-05\n",
            "for batch7241 loss is 0.1905689537525177\n",
            "for batch7242 loss is 0.12983164191246033\n",
            "for batch7243 loss is 0.864948570728302\n",
            "for batch7244 loss is 0.0003664239193312824\n",
            "for batch7245 loss is 0.035304956138134\n",
            "for batch7246 loss is 0.0014682362088933587\n",
            "for batch7247 loss is 0.19942769408226013\n",
            "for batch7248 loss is 0.05778459459543228\n",
            "for batch7249 loss is 0.0035472929012030363\n",
            "for batch7250 loss is 0.5656319260597229\n",
            "for batch7251 loss is 0.003153941361233592\n",
            "for batch7252 loss is 0.08002910017967224\n",
            "for batch7253 loss is 0.09931264817714691\n",
            "for batch7254 loss is 0.1275978982448578\n",
            "for batch7255 loss is 0.00331878662109375\n",
            "for batch7256 loss is 0.0466318354010582\n",
            "for batch7257 loss is 0.25380024313926697\n",
            "for batch7258 loss is 0.009487827308475971\n",
            "for batch7259 loss is 0.09740002453327179\n",
            "for batch7260 loss is 0.04694099351763725\n",
            "for batch7261 loss is 0.030396897345781326\n",
            "for batch7262 loss is 0.00017182601732201874\n",
            "for batch7263 loss is 0.1746263951063156\n",
            "for batch7264 loss is 0.00022148594143800437\n",
            "for batch7265 loss is 0.004644586704671383\n",
            "for batch7266 loss is 0.16184014081954956\n",
            "for batch7267 loss is 0.003844915423542261\n",
            "for batch7268 loss is 0.09843142330646515\n",
            "for batch7269 loss is 0.35702282190322876\n",
            "for batch7270 loss is 0.002347294706851244\n",
            "for batch7271 loss is 0.0\n",
            "for batch7272 loss is 0.2737325131893158\n",
            "for batch7273 loss is 0.04363495856523514\n",
            "for batch7274 loss is 0.009081211872398853\n",
            "for batch7275 loss is 0.2565383315086365\n",
            "for batch7276 loss is 0.002849196782335639\n",
            "for batch7277 loss is 0.0033614356070756912\n",
            "for batch7278 loss is 0.0244594756513834\n",
            "for batch7279 loss is 0.000674468174111098\n",
            "for batch7280 loss is 0.0016507285181432962\n",
            "for batch7281 loss is 0.014641393907368183\n",
            "for batch7282 loss is 0.031138882040977478\n",
            "for batch7283 loss is 0.13533681631088257\n",
            "for batch7284 loss is 0.01683770678937435\n",
            "for batch7285 loss is 0.031006217002868652\n",
            "for batch7286 loss is 0.03362269699573517\n",
            "for batch7287 loss is 2.0265476905478863e-06\n",
            "for batch7288 loss is 0.6767327189445496\n",
            "for batch7289 loss is 0.00015153196000028402\n",
            "for batch7290 loss is 0.00012034022802254185\n",
            "for batch7291 loss is 0.0003034441324416548\n",
            "for batch7292 loss is 0.7215142846107483\n",
            "for batch7293 loss is 0.7985670566558838\n",
            "for batch7294 loss is 0.0761106088757515\n",
            "for batch7295 loss is 0.03220093995332718\n",
            "for batch7296 loss is 0.0037731709890067577\n",
            "for batch7297 loss is 1.044975757598877\n",
            "for batch7298 loss is 0.3856319785118103\n",
            "for batch7299 loss is 0.007420494221150875\n",
            "for batch7300 loss is 0.04766392707824707\n",
            "for batch7301 loss is 0.0330343022942543\n",
            "for batch7302 loss is 0.2041967213153839\n",
            "for batch7303 loss is 0.011337198317050934\n",
            "for batch7304 loss is 0.4375024437904358\n",
            "for batch7305 loss is 0.13841381669044495\n",
            "for batch7306 loss is 0.025400197133421898\n",
            "for batch7307 loss is 0.027362946420907974\n",
            "for batch7308 loss is 0.2958143353462219\n",
            "for batch7309 loss is 0.000718354363925755\n",
            "for batch7310 loss is 0.013883611187338829\n",
            "for batch7311 loss is 0.00044842142960987985\n",
            "for batch7312 loss is 0.14223477244377136\n",
            "for batch7313 loss is 0.07018424570560455\n",
            "for batch7314 loss is 0.0025164943654090166\n",
            "for batch7315 loss is 0.5862998962402344\n",
            "for batch7316 loss is 1.37375009059906\n",
            "for batch7317 loss is 0.30613431334495544\n",
            "for batch7318 loss is 0.0005271228728815913\n",
            "for batch7319 loss is 0.009168270044028759\n",
            "for batch7320 loss is 0.07205231487751007\n",
            "for batch7321 loss is 0.0005246209329925478\n",
            "for batch7322 loss is 0.08571849763393402\n",
            "for batch7323 loss is 0.03105229139328003\n",
            "for batch7324 loss is 0.004622313193976879\n",
            "for batch7325 loss is 0.0017112490022554994\n",
            "for batch7326 loss is 0.0019526224350556731\n",
            "for batch7327 loss is 0.35327965021133423\n",
            "for batch7328 loss is 0.015251418575644493\n",
            "for batch7329 loss is 0.09044540673494339\n",
            "for batch7330 loss is 0.04663407802581787\n",
            "for batch7331 loss is 0.013657798990607262\n",
            "for batch7332 loss is 0.07281482964754105\n",
            "for batch7333 loss is 0.17615997791290283\n",
            "for batch7334 loss is 0.13217437267303467\n",
            "for batch7335 loss is 0.019145231693983078\n",
            "for batch7336 loss is 0.25047463178634644\n",
            "for batch7337 loss is 1.2917978763580322\n",
            "for batch7338 loss is 1.6927664319155156e-06\n",
            "for batch7339 loss is 0.2062184065580368\n",
            "for batch7340 loss is 0.01345843356102705\n",
            "for batch7341 loss is 0.1460074782371521\n",
            "for batch7342 loss is 0.014911646954715252\n",
            "for batch7343 loss is 0.012205745093524456\n",
            "for batch7344 loss is 0.0010270224884152412\n",
            "for batch7345 loss is 0.31307947635650635\n",
            "for batch7346 loss is 0.0010233402717858553\n",
            "for batch7347 loss is 0.0023963532876223326\n",
            "for batch7348 loss is 0.41709575057029724\n",
            "for batch7349 loss is 0.0023244728799909353\n",
            "for batch7350 loss is 0.2606894373893738\n",
            "for batch7351 loss is 0.00020781683269888163\n",
            "for batch7352 loss is 0.37323516607284546\n",
            "for batch7353 loss is 0.01316340547055006\n",
            "for batch7354 loss is 0.0007311599911190569\n",
            "for batch7355 loss is 0.0017072409391403198\n",
            "for batch7356 loss is 0.00655131321400404\n",
            "for batch7357 loss is 0.05152011662721634\n",
            "for batch7358 loss is 0.00035225984174758196\n",
            "for batch7359 loss is 0.4021851420402527\n",
            "for batch7360 loss is 0.09385249763727188\n",
            "for batch7361 loss is 0.014951253309845924\n",
            "for batch7362 loss is 0.0012675928883254528\n",
            "for batch7363 loss is 0.19327685236930847\n",
            "for batch7364 loss is 0.02505137398838997\n",
            "for batch7365 loss is 0.36443597078323364\n",
            "for batch7366 loss is 2.1094043254852295\n",
            "for batch7367 loss is 1.3275012969970703\n",
            "for batch7368 loss is 0.31026870012283325\n",
            "for batch7369 loss is 0.051000118255615234\n",
            "for batch7370 loss is 0.039489470422267914\n",
            "for batch7371 loss is 1.178365707397461\n",
            "for batch7372 loss is 2.384185648907078e-08\n",
            "for batch7373 loss is 0.00015820206317584962\n",
            "for batch7374 loss is 1.2177006006240845\n",
            "for batch7375 loss is 0.28777259588241577\n",
            "for batch7376 loss is 0.007285157684236765\n",
            "for batch7377 loss is 0.0127579839900136\n",
            "for batch7378 loss is 0.1211199164390564\n",
            "for batch7379 loss is 0.0013982013333588839\n",
            "for batch7380 loss is 0.013941635377705097\n",
            "for batch7381 loss is 0.017561564221978188\n",
            "for batch7382 loss is 0.1380317211151123\n",
            "for batch7383 loss is 0.01485647726804018\n",
            "for batch7384 loss is 0.048735611140728\n",
            "for batch7385 loss is 0.19007129967212677\n",
            "for batch7386 loss is 0.0001744615874486044\n",
            "for batch7387 loss is 0.08981559425592422\n",
            "for batch7388 loss is 0.1009533554315567\n",
            "for batch7389 loss is 0.0003713758196681738\n",
            "for batch7390 loss is 0.030818194150924683\n",
            "for batch7391 loss is 0.07577274739742279\n",
            "for batch7392 loss is 0.03880104050040245\n",
            "for batch7393 loss is 0.015592542476952076\n",
            "for batch7394 loss is 0.10902702808380127\n",
            "for batch7395 loss is 0.0004489496932365\n",
            "for batch7396 loss is 0.05456234887242317\n",
            "for batch7397 loss is 0.1297895312309265\n",
            "for batch7398 loss is 0.03221946954727173\n",
            "for batch7399 loss is 0.14194294810295105\n",
            "for batch7400 loss is 0.00011521983833517879\n",
            "for batch7401 loss is 0.029425621032714844\n",
            "for batch7402 loss is 0.02212080918252468\n",
            "for batch7403 loss is 0.0090066809207201\n",
            "for batch7404 loss is 0.9350603818893433\n",
            "for batch7405 loss is 0.09603680670261383\n",
            "for batch7406 loss is 0.30832287669181824\n",
            "for batch7407 loss is 0.3089577555656433\n",
            "for batch7408 loss is 0.0023834495805203915\n",
            "for batch7409 loss is 0.0037860784213989973\n",
            "for batch7410 loss is 0.08877833187580109\n",
            "for batch7411 loss is 0.006988705135881901\n",
            "for batch7412 loss is 0.10156313329935074\n",
            "for batch7413 loss is 0.19360609352588654\n",
            "for batch7414 loss is 0.01811770722270012\n",
            "for batch7415 loss is 0.20796477794647217\n",
            "for batch7416 loss is 0.0071394843980669975\n",
            "for batch7417 loss is 0.2259068489074707\n",
            "for batch7418 loss is 0.008567774668335915\n",
            "for batch7419 loss is 0.020625505596399307\n",
            "for batch7420 loss is 0.0032214659731835127\n",
            "for batch7421 loss is 0.02251574397087097\n",
            "for batch7422 loss is 0.001420309767127037\n",
            "for batch7423 loss is 0.060372114181518555\n",
            "for batch7424 loss is 0.00011676745634758845\n",
            "for batch7425 loss is 0.00838431715965271\n",
            "for batch7426 loss is 0.019185561686754227\n",
            "for batch7427 loss is 0.28838711977005005\n",
            "for batch7428 loss is 0.4037007689476013\n",
            "for batch7429 loss is 0.03561925143003464\n",
            "for batch7430 loss is 0.4173647463321686\n",
            "for batch7431 loss is 0.04950118809938431\n",
            "for batch7432 loss is 0.00016441896150354296\n",
            "for batch7433 loss is 0.01735595427453518\n",
            "for batch7434 loss is 9.53674117454284e-08\n",
            "for batch7435 loss is 0.005995344370603561\n",
            "for batch7436 loss is 0.0002711553534027189\n",
            "for batch7437 loss is 0.01026269979774952\n",
            "for batch7438 loss is 0.014626252464950085\n",
            "for batch7439 loss is 0.12254081666469574\n",
            "for batch7440 loss is 0.011484939604997635\n",
            "for batch7441 loss is 0.04346863925457001\n",
            "for batch7442 loss is 0.0018446177709847689\n",
            "for batch7443 loss is 0.0004481995420064777\n",
            "for batch7444 loss is 0.5606157183647156\n",
            "for batch7445 loss is 4.768370942542788e-08\n",
            "for batch7446 loss is 0.004411008674651384\n",
            "for batch7447 loss is 0.7246301174163818\n",
            "for batch7448 loss is 0.006180708762258291\n",
            "for batch7449 loss is 0.0014230373781174421\n",
            "for batch7450 loss is 0.26664426922798157\n",
            "for batch7451 loss is 0.09857652336359024\n",
            "for batch7452 loss is 0.6088899374008179\n",
            "for batch7453 loss is 0.0005135848768986762\n",
            "for batch7454 loss is 0.439654678106308\n",
            "for batch7455 loss is 0.0019725847523659468\n",
            "for batch7456 loss is 0.0011658644070848823\n",
            "for batch7457 loss is 0.00038756945286877453\n",
            "for batch7458 loss is 0.030452916398644447\n",
            "for batch7459 loss is 0.26659271121025085\n",
            "for batch7460 loss is 0.001761490711942315\n",
            "for batch7461 loss is 0.017705226317048073\n",
            "for batch7462 loss is 0.2988817095756531\n",
            "for batch7463 loss is 0.0008454350754618645\n",
            "for batch7464 loss is 0.40782833099365234\n",
            "for batch7465 loss is 0.016043251380324364\n",
            "for batch7466 loss is 0.004386520013213158\n",
            "for batch7467 loss is 7.423328497679904e-05\n",
            "for batch7468 loss is 0.033207058906555176\n",
            "for batch7469 loss is 0.0007601892575621605\n",
            "for batch7470 loss is 0.10481089353561401\n",
            "for batch7471 loss is 4.355562123237178e-05\n",
            "for batch7472 loss is 0.28403979539871216\n",
            "for batch7473 loss is 0.004184238612651825\n",
            "for batch7474 loss is 0.007273271679878235\n",
            "for batch7475 loss is 0.04093148186802864\n",
            "for batch7476 loss is 0.00026668113423511386\n",
            "for batch7477 loss is 0.0032614250667393208\n",
            "for batch7478 loss is 0.15211978554725647\n",
            "for batch7479 loss is 0.4753868579864502\n",
            "for batch7480 loss is 0.2610166668891907\n",
            "for batch7481 loss is 0.004190501756966114\n",
            "for batch7482 loss is 0.004054485820233822\n",
            "for batch7483 loss is 0.12283287197351456\n",
            "for batch7484 loss is 0.013134723529219627\n",
            "for batch7485 loss is 1.0548964738845825\n",
            "for batch7486 loss is 0.001406764960847795\n",
            "for batch7487 loss is 0.28166964650154114\n",
            "for batch7488 loss is 0.050819724798202515\n",
            "for batch7489 loss is 0.5313919186592102\n",
            "for batch7490 loss is 0.0039069862104952335\n",
            "for batch7491 loss is 0.008949377574026585\n",
            "for batch7492 loss is 0.05147501081228256\n",
            "for batch7493 loss is 0.3027505874633789\n",
            "for batch7494 loss is 0.18727049231529236\n",
            "for batch7495 loss is 0.009002563543617725\n",
            "for batch7496 loss is 0.3711002469062805\n",
            "for batch7497 loss is 0.21565327048301697\n",
            "for batch7498 loss is 0.000425786420237273\n",
            "for batch7499 loss is 0.0002012252080021426\n",
            "for batch7500 loss is 0.01755104586482048\n",
            "for batch7501 loss is 0.05597047135233879\n",
            "for batch7502 loss is 0.029528772458434105\n",
            "for batch7503 loss is 0.14315727353096008\n",
            "for batch7504 loss is 0.10387390851974487\n",
            "for batch7505 loss is 1.8566583395004272\n",
            "for batch7506 loss is 0.0004552278551273048\n",
            "for batch7507 loss is 0.23738041520118713\n",
            "for batch7508 loss is 0.028597688302397728\n",
            "for batch7509 loss is 0.007360475603491068\n",
            "for batch7510 loss is 0.1681578904390335\n",
            "for batch7511 loss is 0.05375124141573906\n",
            "for batch7512 loss is 0.4094139039516449\n",
            "for batch7513 loss is 0.002038970123976469\n",
            "for batch7514 loss is 2.161691188812256\n",
            "for batch7515 loss is 0.11061768233776093\n",
            "for batch7516 loss is 0.11195144802331924\n",
            "for batch7517 loss is 0.06402357667684555\n",
            "for batch7518 loss is 0.05849728733301163\n",
            "for batch7519 loss is 1.3994862456456758e-05\n",
            "for batch7520 loss is 0.0008580855210311711\n",
            "for batch7521 loss is 0.015384882688522339\n",
            "for batch7522 loss is 0.08466648310422897\n",
            "for batch7523 loss is 0.6032388806343079\n",
            "for batch7524 loss is 0.0014551110798493028\n",
            "for batch7525 loss is 0.073117695748806\n",
            "for batch7526 loss is 1.659982442855835\n",
            "for batch7527 loss is 0.0019840712193399668\n",
            "for batch7528 loss is 0.063901886343956\n",
            "for batch7529 loss is 0.14116714894771576\n",
            "for batch7530 loss is 0.004198385868221521\n",
            "for batch7531 loss is 0.1706233024597168\n",
            "for batch7532 loss is 0.03846367448568344\n",
            "for batch7533 loss is 0.033304568380117416\n",
            "for batch7534 loss is 0.000634611351415515\n",
            "for batch7535 loss is 0.1451532542705536\n",
            "for batch7536 loss is 0.002044346183538437\n",
            "for batch7537 loss is 0.020548274740576744\n",
            "for batch7538 loss is 0.0035852764267474413\n",
            "for batch7539 loss is 0.0053694709204137325\n",
            "for batch7540 loss is 1.31125925690867e-05\n",
            "for batch7541 loss is 0.028587713837623596\n",
            "for batch7542 loss is 0.039019010961055756\n",
            "for batch7543 loss is 0.004559005610644817\n",
            "for batch7544 loss is 0.0030486932955682278\n",
            "for batch7545 loss is 0.00023568530741613358\n",
            "for batch7546 loss is 0.04152487963438034\n",
            "for batch7547 loss is 0.04222625121474266\n",
            "for batch7548 loss is 4.672461000154726e-05\n",
            "for batch7549 loss is 0.05314898490905762\n",
            "for batch7550 loss is 0.0006646392866969109\n",
            "for batch7551 loss is 0.0002769460843410343\n",
            "for batch7552 loss is 0.01927337795495987\n",
            "for batch7553 loss is 0.005818903911858797\n",
            "for batch7554 loss is 9.794898505788296e-05\n",
            "for batch7555 loss is 0.009224193170666695\n",
            "for batch7556 loss is 0.0008088952163234353\n",
            "for batch7557 loss is 0.4583255648612976\n",
            "for batch7558 loss is 0.008115438744425774\n",
            "for batch7559 loss is 0.0019694461952894926\n",
            "for batch7560 loss is 0.002139350166544318\n",
            "for batch7561 loss is 0.0\n",
            "for batch7562 loss is 0.03484027832746506\n",
            "for batch7563 loss is 0.007014981005340815\n",
            "for batch7564 loss is 0.029394274577498436\n",
            "for batch7565 loss is 0.27934128046035767\n",
            "for batch7566 loss is 0.006232788320630789\n",
            "for batch7567 loss is 0.035429783165454865\n",
            "for batch7568 loss is 0.4767419397830963\n",
            "for batch7569 loss is 0.041503358632326126\n",
            "for batch7570 loss is 0.0009346678853034973\n",
            "for batch7571 loss is 0.1133190393447876\n",
            "for batch7572 loss is 0.00031310704071074724\n",
            "for batch7573 loss is 0.13708138465881348\n",
            "for batch7574 loss is 0.003257087664678693\n",
            "for batch7575 loss is 0.961033046245575\n",
            "for batch7576 loss is 0.010460643097758293\n",
            "for batch7577 loss is 0.13627061247825623\n",
            "for batch7578 loss is 0.045198626816272736\n",
            "for batch7579 loss is 0.0065836855210363865\n",
            "for batch7580 loss is 0.0013287633191794157\n",
            "for batch7581 loss is 0.03575150668621063\n",
            "for batch7582 loss is 0.5139098763465881\n",
            "for batch7583 loss is 0.002643831539899111\n",
            "for batch7584 loss is 0.8913944363594055\n",
            "for batch7585 loss is 0.11548399925231934\n",
            "for batch7586 loss is 0.05638847500085831\n",
            "for batch7587 loss is 0.5463036298751831\n",
            "for batch7588 loss is 0.011716551147401333\n",
            "for batch7589 loss is 0.0002672337868716568\n",
            "for batch7590 loss is 0.09227307885885239\n",
            "for batch7591 loss is 0.00010223178833257407\n",
            "for batch7592 loss is 0.7951132655143738\n",
            "for batch7593 loss is 0.9036991000175476\n",
            "for batch7594 loss is 0.004837582819163799\n",
            "for batch7595 loss is 0.013864275999367237\n",
            "for batch7596 loss is 0.007119843270629644\n",
            "for batch7597 loss is 0.08910539001226425\n",
            "for batch7598 loss is 0.0016480091726407409\n",
            "for batch7599 loss is 0.0034622694365680218\n",
            "for batch7600 loss is 0.0168050117790699\n",
            "for batch7601 loss is 0.0015781974652782083\n",
            "for batch7602 loss is 0.08698757737874985\n",
            "for batch7603 loss is 0.0006364947184920311\n",
            "for batch7604 loss is 0.045254696160554886\n",
            "for batch7605 loss is 6.438653508666903e-05\n",
            "for batch7606 loss is 0.013498185202479362\n",
            "for batch7607 loss is 0.03515871986746788\n",
            "for batch7608 loss is 0.8003300428390503\n",
            "for batch7609 loss is 0.07790061086416245\n",
            "for batch7610 loss is 0.8212107419967651\n",
            "for batch7611 loss is 0.006685932632535696\n",
            "for batch7612 loss is 0.0315316766500473\n",
            "for batch7613 loss is 0.4546118676662445\n",
            "for batch7614 loss is 0.10430391132831573\n",
            "for batch7615 loss is 0.05154644697904587\n",
            "for batch7616 loss is 0.32746222615242004\n",
            "for batch7617 loss is 0.0003010754007846117\n",
            "for batch7618 loss is 0.03114866279065609\n",
            "for batch7619 loss is 0.026918817311525345\n",
            "for batch7620 loss is 0.9217303991317749\n",
            "for batch7621 loss is 0.07394634932279587\n",
            "for batch7622 loss is 0.0026253527030348778\n",
            "for batch7623 loss is 0.003919612150639296\n",
            "for batch7624 loss is 0.4299759864807129\n",
            "for batch7625 loss is 0.03963197395205498\n",
            "for batch7626 loss is 0.3061401844024658\n",
            "for batch7627 loss is 0.013223139569163322\n",
            "for batch7628 loss is 0.08402873575687408\n",
            "for batch7629 loss is 0.3323218524456024\n",
            "for batch7630 loss is 0.00011395744513720274\n",
            "for batch7631 loss is 0.5639208555221558\n",
            "for batch7632 loss is 0.01912204548716545\n",
            "for batch7633 loss is 0.12756498157978058\n",
            "for batch7634 loss is 1.6450815110147232e-06\n",
            "for batch7635 loss is 0.10382568836212158\n",
            "for batch7636 loss is 0.0020149259362369776\n",
            "for batch7637 loss is 0.4759180545806885\n",
            "for batch7638 loss is 0.009728248231112957\n",
            "for batch7639 loss is 0.08507902920246124\n",
            "for batch7640 loss is 0.2284497767686844\n",
            "for batch7641 loss is 0.0016071576392278075\n",
            "for batch7642 loss is 0.04829490929841995\n",
            "for batch7643 loss is 0.004309604875743389\n",
            "for batch7644 loss is 0.017095770686864853\n",
            "for batch7645 loss is 0.011907413601875305\n",
            "for batch7646 loss is 0.00010180265962844715\n",
            "for batch7647 loss is 0.00014297747111413628\n",
            "for batch7648 loss is 4.9394227971788496e-05\n",
            "for batch7649 loss is 0.18968608975410461\n",
            "for batch7650 loss is 0.11828505992889404\n",
            "for batch7651 loss is 0.0049298228695988655\n",
            "for batch7652 loss is 0.008909898810088634\n",
            "for batch7653 loss is 0.02258729562163353\n",
            "for batch7654 loss is 0.2598707377910614\n",
            "for batch7655 loss is 0.01844147779047489\n",
            "for batch7656 loss is 0.15618163347244263\n",
            "for batch7657 loss is 0.03607574850320816\n",
            "for batch7658 loss is 0.025250712409615517\n",
            "for batch7659 loss is 0.0004007980169262737\n",
            "for batch7660 loss is 0.01840410754084587\n",
            "for batch7661 loss is 0.003554685739800334\n",
            "for batch7662 loss is 0.9001285433769226\n",
            "for batch7663 loss is 0.0001937177439685911\n",
            "for batch7664 loss is 0.9528416395187378\n",
            "for batch7665 loss is 0.00025756462127901614\n",
            "for batch7666 loss is 0.05350493639707565\n",
            "for batch7667 loss is 0.34801769256591797\n",
            "for batch7668 loss is 0.25867682695388794\n",
            "for batch7669 loss is 0.06876369565725327\n",
            "for batch7670 loss is 0.0027641316410154104\n",
            "for batch7671 loss is 0.07121294736862183\n",
            "for batch7672 loss is 0.011957361362874508\n",
            "for batch7673 loss is 0.024811675772070885\n",
            "for batch7674 loss is 0.21258747577667236\n",
            "for batch7675 loss is 0.026262497529387474\n",
            "for batch7676 loss is 0.011883380822837353\n",
            "for batch7677 loss is 0.0009895192924886942\n",
            "for batch7678 loss is 0.0012100744061172009\n",
            "for batch7679 loss is 0.24700598418712616\n",
            "for batch7680 loss is 0.008303685113787651\n",
            "for batch7681 loss is 0.06712888181209564\n",
            "for batch7682 loss is 0.1589856892824173\n",
            "for batch7683 loss is 0.10145749896764755\n",
            "for batch7684 loss is 0.048566363751888275\n",
            "for batch7685 loss is 0.10972027480602264\n",
            "for batch7686 loss is 0.017147060483694077\n",
            "for batch7687 loss is 3.2186248972720932e-06\n",
            "for batch7688 loss is 0.011870265938341618\n",
            "for batch7689 loss is 0.005538304802030325\n",
            "for batch7690 loss is 0.07225286215543747\n",
            "for batch7691 loss is 0.11792613565921783\n",
            "for batch7692 loss is 0.02472134493291378\n",
            "for batch7693 loss is 0.03405764698982239\n",
            "for batch7694 loss is 0.2514503300189972\n",
            "for batch7695 loss is 0.424513578414917\n",
            "for batch7696 loss is 0.03968495875597\n",
            "for batch7697 loss is 0.033991582691669464\n",
            "for batch7698 loss is 0.056058239191770554\n",
            "for batch7699 loss is 0.10336010158061981\n",
            "for batch7700 loss is 0.07063757628202438\n",
            "for batch7701 loss is 0.012674013152718544\n",
            "for batch7702 loss is 6.417271652026102e-05\n",
            "for batch7703 loss is 2.287297248840332\n",
            "for batch7704 loss is 0.5853620767593384\n",
            "for batch7705 loss is 0.06763384491205215\n",
            "for batch7706 loss is 2.0145782400504686e-05\n",
            "for batch7707 loss is 0.19739511609077454\n",
            "for batch7708 loss is 0.03745109960436821\n",
            "for batch7709 loss is 0.0015556474681943655\n",
            "for batch7710 loss is 0.08580578863620758\n",
            "for batch7711 loss is 0.0017802000511437654\n",
            "for batch7712 loss is 0.017074551433324814\n",
            "for batch7713 loss is 0.024553516879677773\n",
            "for batch7714 loss is 0.10735567659139633\n",
            "for batch7715 loss is 0.20007546246051788\n",
            "for batch7716 loss is 0.00932695809751749\n",
            "for batch7717 loss is 0.08909108489751816\n",
            "for batch7718 loss is 0.0037307932507246733\n",
            "for batch7719 loss is 0.003703110618516803\n",
            "for batch7720 loss is 0.7098758816719055\n",
            "for batch7721 loss is 0.01646615006029606\n",
            "for batch7722 loss is 0.024516571313142776\n",
            "for batch7723 loss is 0.07895132154226303\n",
            "for batch7724 loss is 0.01855952851474285\n",
            "for batch7725 loss is 0.0009543641353957355\n",
            "for batch7726 loss is 0.3148159086704254\n",
            "for batch7727 loss is 0.21147246658802032\n",
            "for batch7728 loss is 0.10377762466669083\n",
            "for batch7729 loss is 0.27580422163009644\n",
            "for batch7730 loss is 0.050734974443912506\n",
            "for batch7731 loss is 0.008540320210158825\n",
            "for batch7732 loss is 0.003170053707435727\n",
            "for batch7733 loss is 0.09056637436151505\n",
            "for batch7734 loss is 0.015717722475528717\n",
            "for batch7735 loss is 0.00893905945122242\n",
            "for batch7736 loss is 0.0004563842376228422\n",
            "for batch7737 loss is 0.6125190258026123\n",
            "for batch7738 loss is 0.03931272774934769\n",
            "for batch7739 loss is 0.6007150411605835\n",
            "for batch7740 loss is 0.0008085455046966672\n",
            "for batch7741 loss is 0.047414857894182205\n",
            "for batch7742 loss is 0.03406563401222229\n",
            "for batch7743 loss is 0.11976432800292969\n",
            "for batch7744 loss is 0.0799408107995987\n",
            "for batch7745 loss is 0.39262932538986206\n",
            "for batch7746 loss is 0.12484463304281235\n",
            "for batch7747 loss is 0.0008106264285743237\n",
            "for batch7748 loss is 0.0069234007969498634\n",
            "for batch7749 loss is 0.5703534483909607\n",
            "for batch7750 loss is 0.0001280912256333977\n",
            "for batch7751 loss is 1.0054320096969604\n",
            "for batch7752 loss is 0.42853087186813354\n",
            "for batch7753 loss is 0.6832542419433594\n",
            "for batch7754 loss is 0.07720120996236801\n",
            "for batch7755 loss is 0.24661199748516083\n",
            "for batch7756 loss is 0.2641679644584656\n",
            "for batch7757 loss is 0.007565683219581842\n",
            "for batch7758 loss is 0.01624726876616478\n",
            "for batch7759 loss is 0.009699799120426178\n",
            "for batch7760 loss is 0.01784605160355568\n",
            "for batch7761 loss is 0.46350765228271484\n",
            "for batch7762 loss is 0.028133487328886986\n",
            "for batch7763 loss is 0.10893023014068604\n",
            "for batch7764 loss is 0.11908292770385742\n",
            "for batch7765 loss is 0.1886584311723709\n",
            "for batch7766 loss is 0.0005935717490501702\n",
            "for batch7767 loss is 0.1842484027147293\n",
            "for batch7768 loss is 0.00026632711524143815\n",
            "for batch7769 loss is 0.00013022369239479303\n",
            "for batch7770 loss is 0.18417426943778992\n",
            "for batch7771 loss is 0.1331862509250641\n",
            "for batch7772 loss is 0.05484338849782944\n",
            "for batch7773 loss is 0.0018340761307626963\n",
            "for batch7774 loss is 0.0019308505579829216\n",
            "for batch7775 loss is 0.0011701282346621156\n",
            "for batch7776 loss is 0.03635997325181961\n",
            "for batch7777 loss is 0.1970159411430359\n",
            "for batch7778 loss is 0.0026796685997396708\n",
            "for batch7779 loss is 0.028004001826047897\n",
            "for batch7780 loss is 0.04649437963962555\n",
            "for batch7781 loss is 0.16851690411567688\n",
            "for batch7782 loss is 0.0009812829084694386\n",
            "for batch7783 loss is 0.0488317497074604\n",
            "for batch7784 loss is 0.01857752352952957\n",
            "for batch7785 loss is 0.06210852786898613\n",
            "for batch7786 loss is 0.13517917692661285\n",
            "for batch7787 loss is 0.02069288119673729\n",
            "for batch7788 loss is 0.00011200270091649145\n",
            "for batch7789 loss is 0.04313021898269653\n",
            "for batch7790 loss is 0.0\n",
            "for batch7791 loss is 0.04730885103344917\n",
            "for batch7792 loss is 0.10351650416851044\n",
            "for batch7793 loss is 0.0005802142550237477\n",
            "for batch7794 loss is 0.04644188657402992\n",
            "for batch7795 loss is 0.0031017388682812452\n",
            "for batch7796 loss is 0.013425287790596485\n",
            "for batch7797 loss is 0.011821312829852104\n",
            "for batch7798 loss is 0.046389807015657425\n",
            "for batch7799 loss is 0.0026952780317515135\n",
            "for batch7800 loss is 0.033359408378601074\n",
            "for batch7801 loss is 0.001652848208323121\n",
            "for batch7802 loss is 0.03258834034204483\n",
            "for batch7803 loss is 0.006646923720836639\n",
            "for batch7804 loss is 0.05679624155163765\n",
            "for batch7805 loss is 2.9325278774194885e-06\n",
            "for batch7806 loss is 1.0525331497192383\n",
            "for batch7807 loss is 6.174946065584663e-06\n",
            "for batch7808 loss is 0.04911937192082405\n",
            "for batch7809 loss is 0.6902686953544617\n",
            "for batch7810 loss is 0.16617938876152039\n",
            "for batch7811 loss is 0.2880844175815582\n",
            "for batch7812 loss is 0.40550360083580017\n",
            "for batch7813 loss is 1.473397423978895e-05\n",
            "for batch7814 loss is 3.187403126503341e-05\n",
            "for batch7815 loss is 0.5839711427688599\n",
            "for batch7816 loss is 0.02439882792532444\n",
            "for batch7817 loss is 0.0038893416058272123\n",
            "for batch7818 loss is 0.3595535159111023\n",
            "for batch7819 loss is 0.14066149294376373\n",
            "for batch7820 loss is 0.004761358257383108\n",
            "for batch7821 loss is 0.040759094059467316\n",
            "for batch7822 loss is 0.01174490712583065\n",
            "for batch7823 loss is 0.17077401280403137\n",
            "for batch7824 loss is 0.00011169190111104399\n",
            "for batch7825 loss is 0.6922087073326111\n",
            "for batch7826 loss is 0.019124681130051613\n",
            "for batch7827 loss is 0.22544939815998077\n",
            "for batch7828 loss is 0.07271708548069\n",
            "for batch7829 loss is 0.00860467366874218\n",
            "for batch7830 loss is 0.00019647376029752195\n",
            "for batch7831 loss is 0.08757852017879486\n",
            "for batch7832 loss is 0.00454930542036891\n",
            "for batch7833 loss is 0.28269052505493164\n",
            "for batch7834 loss is 0.012313926592469215\n",
            "for batch7835 loss is 0.0071462346240878105\n",
            "for batch7836 loss is 0.006330077536404133\n",
            "for batch7837 loss is 0.2814004719257355\n",
            "for batch7838 loss is 1.954867959022522\n",
            "for batch7839 loss is 0.5694782733917236\n",
            "for batch7840 loss is 0.018276114016771317\n",
            "for batch7841 loss is 0.38562846183776855\n",
            "for batch7842 loss is 0.055118609219789505\n",
            "for batch7843 loss is 6.753795605618507e-05\n",
            "for batch7844 loss is 0.2691088914871216\n",
            "for batch7845 loss is 0.00013921241043135524\n",
            "for batch7846 loss is 0.6793465614318848\n",
            "for batch7847 loss is 0.05116323381662369\n",
            "for batch7848 loss is 0.01577186957001686\n",
            "for batch7849 loss is 0.008253571577370167\n",
            "for batch7850 loss is 0.9379011988639832\n",
            "for batch7851 loss is 0.05655376985669136\n",
            "for batch7852 loss is 0.0004150319437030703\n",
            "for batch7853 loss is 0.00011510163312777877\n",
            "for batch7854 loss is 0.06137518212199211\n",
            "for batch7855 loss is 0.10303475707769394\n",
            "for batch7856 loss is 0.07326070964336395\n",
            "for batch7857 loss is 0.028895368799567223\n",
            "for batch7858 loss is 0.09161943197250366\n",
            "for batch7859 loss is 0.16157576441764832\n",
            "for batch7860 loss is 0.0005212145624682307\n",
            "for batch7861 loss is 0.00032435599132440984\n",
            "for batch7862 loss is 0.0031500167679041624\n",
            "for batch7863 loss is 0.495978981256485\n",
            "for batch7864 loss is 0.09214909374713898\n",
            "for batch7865 loss is 0.10562954097986221\n",
            "for batch7866 loss is 0.024171795696020126\n",
            "for batch7867 loss is 0.0018629112746566534\n",
            "for batch7868 loss is 0.0022870542015880346\n",
            "for batch7869 loss is 0.05265975743532181\n",
            "for batch7870 loss is 0.05733181908726692\n",
            "for batch7871 loss is 0.002306150970980525\n",
            "for batch7872 loss is 0.0002644747728481889\n",
            "for batch7873 loss is 0.5182267427444458\n",
            "for batch7874 loss is 0.28703269362449646\n",
            "for batch7875 loss is 0.005417637061327696\n",
            "for batch7876 loss is 0.09177976846694946\n",
            "for batch7877 loss is 0.004687962122261524\n",
            "for batch7878 loss is 0.024203062057495117\n",
            "for batch7879 loss is 0.003137029707431793\n",
            "for batch7880 loss is 0.2527139186859131\n",
            "for batch7881 loss is 0.21929380297660828\n",
            "for batch7882 loss is 0.3331034481525421\n",
            "for batch7883 loss is 3.381415843963623\n",
            "for batch7884 loss is 0.19343118369579315\n",
            "for batch7885 loss is 0.01033836044371128\n",
            "for batch7886 loss is 0.05246471241116524\n",
            "for batch7887 loss is 0.47904443740844727\n",
            "for batch7888 loss is 0.017887219786643982\n",
            "for batch7889 loss is 1.3351407233130885e-06\n",
            "for batch7890 loss is 0.10234314203262329\n",
            "for batch7891 loss is 0.03490196913480759\n",
            "for batch7892 loss is 0.0004440310294739902\n",
            "for batch7893 loss is 0.005376400426030159\n",
            "for batch7894 loss is 7.510080649808515e-06\n",
            "for batch7895 loss is 0.04676509276032448\n",
            "for batch7896 loss is 0.05404682829976082\n",
            "for batch7897 loss is 0.06382996588945389\n",
            "for batch7898 loss is 3.3689491748809814\n",
            "for batch7899 loss is 0.008472354151308537\n",
            "for batch7900 loss is 0.3548020124435425\n",
            "for batch7901 loss is 0.30591365694999695\n",
            "for batch7902 loss is 0.18457888066768646\n",
            "for batch7903 loss is 0.6159960627555847\n",
            "for batch7904 loss is 0.00020712314289994538\n",
            "for batch7905 loss is 0.49893617630004883\n",
            "for batch7906 loss is 0.549234926700592\n",
            "for batch7907 loss is 0.008178570307791233\n",
            "for batch7908 loss is 0.0004913037992082536\n",
            "for batch7909 loss is 0.07430414855480194\n",
            "for batch7910 loss is 0.17046186327934265\n",
            "for batch7911 loss is 0.20032088458538055\n",
            "for batch7912 loss is 0.8801059722900391\n",
            "for batch7913 loss is 0.2066730260848999\n",
            "for batch7914 loss is 0.003907557111233473\n",
            "for batch7915 loss is 0.40201419591903687\n",
            "for batch7916 loss is 0.0017088772729039192\n",
            "for batch7917 loss is 0.10307039320468903\n",
            "for batch7918 loss is 0.001687501324340701\n",
            "for batch7919 loss is 0.028084341436624527\n",
            "for batch7920 loss is 0.022099047899246216\n",
            "for batch7921 loss is 0.6081202030181885\n",
            "for batch7922 loss is 0.17237147688865662\n",
            "for batch7923 loss is 0.9273483157157898\n",
            "for batch7924 loss is 0.035822611302137375\n",
            "for batch7925 loss is 0.10525377094745636\n",
            "for batch7926 loss is 1.4278361797332764\n",
            "for batch7927 loss is 0.5740808844566345\n",
            "for batch7928 loss is 0.24721784889698029\n",
            "for batch7929 loss is 0.4744187295436859\n",
            "for batch7930 loss is 0.1763484627008438\n",
            "for batch7931 loss is 0.00020711307297460735\n",
            "for batch7932 loss is 0.3766130805015564\n",
            "for batch7933 loss is 0.2545744478702545\n",
            "for batch7934 loss is 0.16776424646377563\n",
            "for batch7935 loss is 0.11570564657449722\n",
            "for batch7936 loss is 0.015100307762622833\n",
            "for batch7937 loss is 0.043222058564424515\n",
            "for batch7938 loss is 4.386854016047437e-06\n",
            "for batch7939 loss is 0.02580978535115719\n",
            "for batch7940 loss is 0.09098995476961136\n",
            "for batch7941 loss is 0.02229279652237892\n",
            "for batch7942 loss is 0.009140195325016975\n",
            "for batch7943 loss is 0.0037335879169404507\n",
            "for batch7944 loss is 0.10924749076366425\n",
            "for batch7945 loss is 0.2794703543186188\n",
            "for batch7946 loss is 0.2916664183139801\n",
            "for batch7947 loss is 3.413874583202414e-05\n",
            "for batch7948 loss is 0.0030173021368682384\n",
            "for batch7949 loss is 0.07360749691724777\n",
            "for batch7950 loss is 4.7683664661235525e-07\n",
            "for batch7951 loss is 0.05726704001426697\n",
            "for batch7952 loss is 0.6738382577896118\n",
            "for batch7953 loss is 0.004743956029415131\n",
            "for batch7954 loss is 0.16097040474414825\n",
            "for batch7955 loss is 0.15200404822826385\n",
            "for batch7956 loss is 0.007787979207932949\n",
            "for batch7957 loss is 0.0018871694337576628\n",
            "for batch7958 loss is 8.886565046850592e-05\n",
            "for batch7959 loss is 0.2631163001060486\n",
            "for batch7960 loss is 0.018276097252964973\n",
            "for batch7961 loss is 0.12446626275777817\n",
            "for batch7962 loss is 0.042842764407396317\n",
            "for batch7963 loss is 0.028712481260299683\n",
            "for batch7964 loss is 0.01451790053397417\n",
            "for batch7965 loss is 0.028116490691900253\n",
            "for batch7966 loss is 0.023509666323661804\n",
            "for batch7967 loss is 0.08660552650690079\n",
            "for batch7968 loss is 0.17127998173236847\n",
            "for batch7969 loss is 0.0030523561872541904\n",
            "for batch7970 loss is 0.046059176325798035\n",
            "for batch7971 loss is 0.09803696721792221\n",
            "for batch7972 loss is 0.0013883128995075822\n",
            "for batch7973 loss is 0.19868066906929016\n",
            "for batch7974 loss is 1.4625165462493896\n",
            "for batch7975 loss is 0.144571453332901\n",
            "for batch7976 loss is 0.002089269459247589\n",
            "for batch7977 loss is 0.0012682242086157203\n",
            "for batch7978 loss is 0.5523449182510376\n",
            "for batch7979 loss is 7.699611887801439e-05\n",
            "for batch7980 loss is 0.31201595067977905\n",
            "for batch7981 loss is 0.00020892778411507607\n",
            "for batch7982 loss is 0.009998730383813381\n",
            "for batch7983 loss is 6.469654908869416e-05\n",
            "for batch7984 loss is 0.029891198500990868\n",
            "for batch7985 loss is 0.12212888896465302\n",
            "for batch7986 loss is 0.09628838300704956\n",
            "for batch7987 loss is 1.0776371709653176e-05\n",
            "for batch7988 loss is 0.02743365243077278\n",
            "for batch7989 loss is 0.010370143689215183\n",
            "for batch7990 loss is 0.45666784048080444\n",
            "for batch7991 loss is 0.19010137021541595\n",
            "for batch7992 loss is 0.317526638507843\n",
            "for batch7993 loss is 0.073707714676857\n",
            "for batch7994 loss is 0.12097710371017456\n",
            "for batch7995 loss is 0.012479267083108425\n",
            "for batch7996 loss is 0.04540608823299408\n",
            "for batch7997 loss is 0.02457718923687935\n",
            "for batch7998 loss is 0.29465311765670776\n",
            "for batch7999 loss is 0.0940224751830101\n",
            "for batch8000 loss is 0.009914485737681389\n",
            "for batch8001 loss is 4.768370942542788e-08\n",
            "for batch8002 loss is 0.032875098288059235\n",
            "for batch8003 loss is 1.3129583597183228\n",
            "for batch8004 loss is 0.4956192970275879\n",
            "for batch8005 loss is 0.0\n",
            "for batch8006 loss is 0.10685761272907257\n",
            "for batch8007 loss is 0.01153891533613205\n",
            "for batch8008 loss is 0.0004088610294274986\n",
            "for batch8009 loss is 0.967462420463562\n",
            "for batch8010 loss is 0.24656441807746887\n",
            "for batch8011 loss is 1.1063066720962524\n",
            "for batch8012 loss is 0.007830307818949223\n",
            "for batch8013 loss is 0.11826076358556747\n",
            "for batch8014 loss is 3.7977330066496506e-05\n",
            "for batch8015 loss is 1.1542211771011353\n",
            "for batch8016 loss is 0.011064122430980206\n",
            "for batch8017 loss is 0.07828637212514877\n",
            "for batch8018 loss is 0.01831376552581787\n",
            "for batch8019 loss is 0.09239360690116882\n",
            "for batch8020 loss is 0.08121196925640106\n",
            "for batch8021 loss is 0.19879920780658722\n",
            "for batch8022 loss is 0.029991820454597473\n",
            "for batch8023 loss is 0.0007551842136308551\n",
            "for batch8024 loss is 0.013914503157138824\n",
            "for batch8025 loss is 0.0017514813225716352\n",
            "for batch8026 loss is 0.02128594182431698\n",
            "for batch8027 loss is 1.8835013406715007e-06\n",
            "for batch8028 loss is 0.03809351474046707\n",
            "for batch8029 loss is 0.27408748865127563\n",
            "for batch8030 loss is 0.1544213593006134\n",
            "for batch8031 loss is 0.0001752328098518774\n",
            "for batch8032 loss is 0.0013577410718426108\n",
            "for batch8033 loss is 0.011892614886164665\n",
            "for batch8034 loss is 0.0005831274902448058\n",
            "for batch8035 loss is 0.001857526833191514\n",
            "for batch8036 loss is 0.00015277735656127334\n",
            "for batch8037 loss is 0.09714537858963013\n",
            "for batch8038 loss is 0.03995199501514435\n",
            "for batch8039 loss is 0.07222907245159149\n",
            "for batch8040 loss is 0.2850804030895233\n",
            "for batch8041 loss is 0.0365392304956913\n",
            "for batch8042 loss is 0.00031615118496119976\n",
            "for batch8043 loss is 0.1258053183555603\n",
            "for batch8044 loss is 0.09434902667999268\n",
            "for batch8045 loss is 0.004401895683258772\n",
            "for batch8046 loss is 0.0360763780772686\n",
            "for batch8047 loss is 0.0320163294672966\n",
            "for batch8048 loss is 0.0037701576948165894\n",
            "for batch8049 loss is 0.14618009328842163\n",
            "for batch8050 loss is 0.017405930906534195\n",
            "for batch8051 loss is 0.16394059360027313\n",
            "for batch8052 loss is 3.845506944344379e-05\n",
            "for batch8053 loss is 0.04060764238238335\n",
            "for batch8054 loss is 0.013191861100494862\n",
            "for batch8055 loss is 0.01400932390242815\n",
            "for batch8056 loss is 0.009484277106821537\n",
            "for batch8057 loss is 0.6151949167251587\n",
            "for batch8058 loss is 0.1297684907913208\n",
            "for batch8059 loss is 0.0032024644315242767\n",
            "for batch8060 loss is 0.0001308647042606026\n",
            "for batch8061 loss is 0.16553421318531036\n",
            "for batch8062 loss is 0.7833021879196167\n",
            "for batch8063 loss is 1.5998671054840088\n",
            "for batch8064 loss is 0.019916007295250893\n",
            "for batch8065 loss is 0.3364957273006439\n",
            "for batch8066 loss is 3.9859987737145275e-05\n",
            "for batch8067 loss is 0.29682210087776184\n",
            "for batch8068 loss is 0.06607062369585037\n",
            "for batch8069 loss is 0.014886906370520592\n",
            "for batch8070 loss is 9.608105756342411e-06\n",
            "for batch8071 loss is 0.24773864448070526\n",
            "for batch8072 loss is 0.018749121576547623\n",
            "for batch8073 loss is 0.043793756514787674\n",
            "for batch8074 loss is 0.11860749870538712\n",
            "for batch8075 loss is 0.02242407761514187\n",
            "for batch8076 loss is 0.05774018168449402\n",
            "for batch8077 loss is 0.038289569318294525\n",
            "for batch8078 loss is 0.5308791995048523\n",
            "for batch8079 loss is 0.00725137535482645\n",
            "for batch8080 loss is 0.2532994747161865\n",
            "for batch8081 loss is 0.00015849273768253624\n",
            "for batch8082 loss is 0.02111237123608589\n",
            "for batch8083 loss is 0.011340469121932983\n",
            "for batch8084 loss is 0.029513558372855186\n",
            "for batch8085 loss is 0.9177190065383911\n",
            "for batch8086 loss is 0.00027666782261803746\n",
            "for batch8087 loss is 0.021182630211114883\n",
            "for batch8088 loss is 0.01486227661371231\n",
            "for batch8089 loss is 0.029158663004636765\n",
            "for batch8090 loss is 0.03434478119015694\n",
            "for batch8091 loss is 0.0018748898291960359\n",
            "for batch8092 loss is 1.8835013406715007e-06\n",
            "for batch8093 loss is 0.34907639026641846\n",
            "for batch8094 loss is 0.00419897586107254\n",
            "for batch8095 loss is 0.12714262306690216\n",
            "for batch8096 loss is 0.011714782565832138\n",
            "for batch8097 loss is 1.0569698810577393\n",
            "for batch8098 loss is 0.000770826474763453\n",
            "for batch8099 loss is 0.05733614042401314\n",
            "for batch8100 loss is 0.6407564878463745\n",
            "for batch8101 loss is 0.22601541876792908\n",
            "for batch8102 loss is 0.020159434527158737\n",
            "for batch8103 loss is 0.007842166349291801\n",
            "for batch8104 loss is 0.0008682807674631476\n",
            "for batch8105 loss is 0.00021763917175121605\n",
            "for batch8106 loss is 0.0522063747048378\n",
            "for batch8107 loss is 0.21453876793384552\n",
            "for batch8108 loss is 0.051208145916461945\n",
            "for batch8109 loss is 1.435233025404159e-05\n",
            "for batch8110 loss is 0.09105341136455536\n",
            "for batch8111 loss is 0.00012143813364673406\n",
            "for batch8112 loss is 9.53674117454284e-08\n",
            "for batch8113 loss is 0.0001754008117131889\n",
            "for batch8114 loss is 0.19284756481647491\n",
            "for batch8115 loss is 0.4080048203468323\n",
            "for batch8116 loss is 0.0002534429950173944\n",
            "for batch8117 loss is 0.0624266043305397\n",
            "for batch8118 loss is 0.4163474142551422\n",
            "for batch8119 loss is 0.001924518495798111\n",
            "for batch8120 loss is 0.1358649730682373\n",
            "for batch8121 loss is 0.00047999509843066335\n",
            "for batch8122 loss is 0.05812365934252739\n",
            "for batch8123 loss is 0.036125704646110535\n",
            "for batch8124 loss is 0.029475126415491104\n",
            "for batch8125 loss is 4.672951945394743e-06\n",
            "for batch8126 loss is 4.52469612355344e-05\n",
            "for batch8127 loss is 0.11564955860376358\n",
            "for batch8128 loss is 1.3064911399851553e-05\n",
            "for batch8129 loss is 0.2319752722978592\n",
            "for batch8130 loss is 0.5055135488510132\n",
            "for batch8131 loss is 0.01918882131576538\n",
            "for batch8132 loss is 0.010862491093575954\n",
            "for batch8133 loss is 0.04654259234666824\n",
            "for batch8134 loss is 0.026195239275693893\n",
            "for batch8135 loss is 0.003354836953803897\n",
            "for batch8136 loss is 0.09337247908115387\n",
            "for batch8137 loss is 0.17586185038089752\n",
            "for batch8138 loss is 0.288949191570282\n",
            "for batch8139 loss is 0.001722800894640386\n",
            "for batch8140 loss is 0.01431107334792614\n",
            "for batch8141 loss is 0.016225088387727737\n",
            "for batch8142 loss is 0.012206444516777992\n",
            "for batch8143 loss is 0.007327289320528507\n",
            "for batch8144 loss is 2.913346907007508e-05\n",
            "for batch8145 loss is 0.3480355143547058\n",
            "for batch8146 loss is 0.0004035666643176228\n",
            "for batch8147 loss is 0.003618683200329542\n",
            "for batch8148 loss is 0.02728922665119171\n",
            "for batch8149 loss is 0.5186822414398193\n",
            "for batch8150 loss is 5.245204874881892e-07\n",
            "for batch8151 loss is 0.4790831506252289\n",
            "for batch8152 loss is 0.7264928817749023\n",
            "for batch8153 loss is 0.06042955443263054\n",
            "for batch8154 loss is 0.4504949450492859\n",
            "for batch8155 loss is 0.7820722460746765\n",
            "for batch8156 loss is 0.26881226897239685\n",
            "for batch8157 loss is 1.452858805656433\n",
            "for batch8158 loss is 0.06709737330675125\n",
            "for batch8159 loss is 0.0018026542384177446\n",
            "for batch8160 loss is 0.0011211186647415161\n",
            "for batch8161 loss is 0.0512612983584404\n",
            "for batch8162 loss is 0.4539649486541748\n",
            "for batch8163 loss is 0.0012503917096182704\n",
            "for batch8164 loss is 0.07961688935756683\n",
            "for batch8165 loss is 0.0430578738451004\n",
            "for batch8166 loss is 0.028843317180871964\n",
            "for batch8167 loss is 0.053505051881074905\n",
            "for batch8168 loss is 0.2083369791507721\n",
            "for batch8169 loss is 0.020928358659148216\n",
            "for batch8170 loss is 2.8298632969381288e-05\n",
            "for batch8171 loss is 6.818697329435963e-06\n",
            "for batch8172 loss is 0.0025969797279685736\n",
            "for batch8173 loss is 0.050314761698246\n",
            "for batch8174 loss is 0.002460489748045802\n",
            "for batch8175 loss is 0.010821794159710407\n",
            "for batch8176 loss is 0.0031365007162094116\n",
            "for batch8177 loss is 0.0002300131309311837\n",
            "for batch8178 loss is 0.01625237800180912\n",
            "for batch8179 loss is 0.14392498135566711\n",
            "for batch8180 loss is 0.0020426104310899973\n",
            "for batch8181 loss is 0.05816538259387016\n",
            "for batch8182 loss is 0.12296013534069061\n",
            "for batch8183 loss is 0.5117427110671997\n",
            "for batch8184 loss is 0.014495322480797768\n",
            "for batch8185 loss is 0.009295297786593437\n",
            "for batch8186 loss is 0.0\n",
            "for batch8187 loss is 0.5413590669631958\n",
            "for batch8188 loss is 0.0018766190623864532\n",
            "for batch8189 loss is 0.3963247239589691\n",
            "for batch8190 loss is 0.0029300900641828775\n",
            "for batch8191 loss is 0.4156956076622009\n",
            "for batch8192 loss is 0.323580265045166\n",
            "for batch8193 loss is 0.0184695515781641\n",
            "for batch8194 loss is 0.07545212656259537\n",
            "for batch8195 loss is 0.6129461526870728\n",
            "for batch8196 loss is 0.3564184904098511\n",
            "for batch8197 loss is 0.006812828592956066\n",
            "for batch8198 loss is 0.07287740707397461\n",
            "for batch8199 loss is 0.07655005902051926\n",
            "for batch8200 loss is 0.17804628610610962\n",
            "for batch8201 loss is 0.08691147714853287\n",
            "for batch8202 loss is 0.001574216061271727\n",
            "for batch8203 loss is 0.0010435711592435837\n",
            "for batch8204 loss is 0.014326770789921284\n",
            "for batch8205 loss is 0.0011584163876250386\n",
            "for batch8206 loss is 1.1761934757232666\n",
            "for batch8207 loss is 0.281810998916626\n",
            "for batch8208 loss is 0.004989425651729107\n",
            "for batch8209 loss is 0.1932283341884613\n",
            "for batch8210 loss is 0.00150543381460011\n",
            "for batch8211 loss is 0.02206810936331749\n",
            "for batch8212 loss is 0.000355411582859233\n",
            "for batch8213 loss is 0.15326592326164246\n",
            "for batch8214 loss is 0.0552566833794117\n",
            "for batch8215 loss is 0.1512259542942047\n",
            "for batch8216 loss is 0.12255284935235977\n",
            "for batch8217 loss is 0.0040787882171571255\n",
            "for batch8218 loss is 0.010521909222006798\n",
            "for batch8219 loss is 0.2902579605579376\n",
            "for batch8220 loss is 0.00010802801989484578\n",
            "for batch8221 loss is 0.01748136803507805\n",
            "for batch8222 loss is 0.2495729923248291\n",
            "for batch8223 loss is 0.1303979903459549\n",
            "for batch8224 loss is 0.6908515691757202\n",
            "for batch8225 loss is 0.0010713647352531552\n",
            "for batch8226 loss is 0.6515231728553772\n",
            "for batch8227 loss is 0.0007477118051610887\n",
            "for batch8228 loss is 0.072284996509552\n",
            "for batch8229 loss is 0.02512134239077568\n",
            "for batch8230 loss is 0.08007223904132843\n",
            "for batch8231 loss is 0.0025535959284752607\n",
            "for batch8232 loss is 0.011721056886017323\n",
            "for batch8233 loss is 0.06325425207614899\n",
            "for batch8234 loss is 0.03969389945268631\n",
            "for batch8235 loss is 0.002261520829051733\n",
            "for batch8236 loss is 0.006658780388534069\n",
            "for batch8237 loss is 0.1522877961397171\n",
            "for batch8238 loss is 0.1939268261194229\n",
            "for batch8239 loss is 0.3062923550605774\n",
            "for batch8240 loss is 0.0035202198196202517\n",
            "for batch8241 loss is 0.0002126759645761922\n",
            "for batch8242 loss is 0.041504666209220886\n",
            "for batch8243 loss is 0.0011419865768402815\n",
            "for batch8244 loss is 0.00015350653848145157\n",
            "for batch8245 loss is 0.20328518748283386\n",
            "for batch8246 loss is 0.12079284340143204\n",
            "for batch8247 loss is 0.004491456784307957\n",
            "for batch8248 loss is 0.023320529609918594\n",
            "for batch8249 loss is 0.2062300145626068\n",
            "for batch8250 loss is 0.04759034886956215\n",
            "for batch8251 loss is 0.004119342193007469\n",
            "for batch8252 loss is 0.0002533289371058345\n",
            "for batch8253 loss is 0.8598783612251282\n",
            "for batch8254 loss is 1.4705137014389038\n",
            "for batch8255 loss is 0.34399276971817017\n",
            "for batch8256 loss is 0.0825391635298729\n",
            "for batch8257 loss is 0.006200880743563175\n",
            "for batch8258 loss is 0.02952711284160614\n",
            "for batch8259 loss is 0.03439851105213165\n",
            "for batch8260 loss is 0.14979001879692078\n",
            "for batch8261 loss is 1.4947724342346191\n",
            "for batch8262 loss is 0.00044408938265405595\n",
            "for batch8263 loss is 0.8979047536849976\n",
            "for batch8264 loss is 0.03731931746006012\n",
            "for batch8265 loss is 0.06013214588165283\n",
            "for batch8266 loss is 0.08247175067663193\n",
            "for batch8267 loss is 0.055373210459947586\n",
            "for batch8268 loss is 0.3119855225086212\n",
            "for batch8269 loss is 0.0026810867711901665\n",
            "for batch8270 loss is 0.007207302842289209\n",
            "for batch8271 loss is 0.19340912997722626\n",
            "for batch8272 loss is 0.13049843907356262\n",
            "for batch8273 loss is 0.023744847625494003\n",
            "for batch8274 loss is 0.07159967720508575\n",
            "for batch8275 loss is 0.03298385813832283\n",
            "for batch8276 loss is 0.029351210221648216\n",
            "for batch8277 loss is 0.4609861373901367\n",
            "for batch8278 loss is 0.06481605023145676\n",
            "for batch8279 loss is 0.021301809698343277\n",
            "for batch8280 loss is 0.20423200726509094\n",
            "for batch8281 loss is 0.4922497868537903\n",
            "for batch8282 loss is 4.210271799820475e-05\n",
            "for batch8283 loss is 0.4380335807800293\n",
            "for batch8284 loss is 0.2724190354347229\n",
            "for batch8285 loss is 0.003383347764611244\n",
            "for batch8286 loss is 0.11509294807910919\n",
            "for batch8287 loss is 0.16269074380397797\n",
            "for batch8288 loss is 0.16590866446495056\n",
            "for batch8289 loss is 0.02280370332300663\n",
            "for batch8290 loss is 7.414714673359413e-06\n",
            "for batch8291 loss is 0.047737084329128265\n",
            "for batch8292 loss is 0.023128433153033257\n",
            "for batch8293 loss is 6.437293222916196e-07\n",
            "for batch8294 loss is 0.029415130615234375\n",
            "for batch8295 loss is 0.22748947143554688\n",
            "for batch8296 loss is 0.47792044281959534\n",
            "for batch8297 loss is 0.019713610410690308\n",
            "for batch8298 loss is 0.0003763338318094611\n",
            "for batch8299 loss is 0.4204675257205963\n",
            "for batch8300 loss is 0.012200002558529377\n",
            "for batch8301 loss is 0.07414541393518448\n",
            "for batch8302 loss is 0.041469622403383255\n",
            "for batch8303 loss is 0.4244931638240814\n",
            "for batch8304 loss is 0.014210596680641174\n",
            "for batch8305 loss is 0.03328370302915573\n",
            "for batch8306 loss is 1.0087705850601196\n",
            "for batch8307 loss is 0.05932776257395744\n",
            "for batch8308 loss is 0.6330419182777405\n",
            "for batch8309 loss is 0.012621678411960602\n",
            "for batch8310 loss is 0.06859298795461655\n",
            "for batch8311 loss is 0.15054206550121307\n",
            "for batch8312 loss is 0.021534189581871033\n",
            "for batch8313 loss is 0.2638922333717346\n",
            "for batch8314 loss is 0.04849559813737869\n",
            "for batch8315 loss is 0.08762945234775543\n",
            "for batch8316 loss is 0.045224979519844055\n",
            "for batch8317 loss is 0.07632323354482651\n",
            "for batch8318 loss is 0.660865068435669\n",
            "for batch8319 loss is 0.1449936181306839\n",
            "for batch8320 loss is 0.21129433810710907\n",
            "for batch8321 loss is 0.08995675295591354\n",
            "for batch8322 loss is 7.962394738569856e-05\n",
            "for batch8323 loss is 0.0005355142639018595\n",
            "for batch8324 loss is 2.446120197419077e-05\n",
            "for batch8325 loss is 0.001971856225281954\n",
            "for batch8326 loss is 0.05031551048159599\n",
            "for batch8327 loss is 0.0046102749183773994\n",
            "for batch8328 loss is 0.10827763378620148\n",
            "for batch8329 loss is 0.16720426082611084\n",
            "for batch8330 loss is 0.000897251651622355\n",
            "for batch8331 loss is 2.384185648907078e-08\n",
            "for batch8332 loss is 0.03535827621817589\n",
            "for batch8333 loss is 0.010332394391298294\n",
            "for batch8334 loss is 0.2550247311592102\n",
            "for batch8335 loss is 0.07903484255075455\n",
            "for batch8336 loss is 0.14921428263187408\n",
            "for batch8337 loss is 0.23028317093849182\n",
            "for batch8338 loss is 0.1311315894126892\n",
            "for batch8339 loss is 0.00018173540593124926\n",
            "for batch8340 loss is 0.047304555773735046\n",
            "for batch8341 loss is 0.00021848164033144712\n",
            "for batch8342 loss is 0.04319344833493233\n",
            "for batch8343 loss is 0.00018232411821372807\n",
            "for batch8344 loss is 0.17886462807655334\n",
            "for batch8345 loss is 0.033454492688179016\n",
            "for batch8346 loss is 0.0026990759652107954\n",
            "for batch8347 loss is 0.7883087396621704\n",
            "for batch8348 loss is 9.536741885085576e-08\n",
            "for batch8349 loss is 0.002305197063833475\n",
            "for batch8350 loss is 0.16529378294944763\n",
            "for batch8351 loss is 7.129830191843212e-05\n",
            "for batch8352 loss is 0.007813768461346626\n",
            "for batch8353 loss is 0.20057865977287292\n",
            "for batch8354 loss is 0.00056532520102337\n",
            "for batch8355 loss is 0.004956572782248259\n",
            "for batch8356 loss is 0.005011747591197491\n",
            "for batch8357 loss is 1.3260210752487183\n",
            "for batch8358 loss is 0.047121547162532806\n",
            "for batch8359 loss is 0.014362228102982044\n",
            "for batch8360 loss is 0.054862238466739655\n",
            "for batch8361 loss is 0.0001454566663596779\n",
            "for batch8362 loss is 7.089311839081347e-05\n",
            "for batch8363 loss is 0.02986624836921692\n",
            "for batch8364 loss is 0.1380920708179474\n",
            "for batch8365 loss is 0.1270705759525299\n",
            "for batch8366 loss is 0.06712707132101059\n",
            "for batch8367 loss is 0.00021343844127841294\n",
            "for batch8368 loss is 1.363741159439087\n",
            "for batch8369 loss is 0.02812604233622551\n",
            "for batch8370 loss is 0.21145756542682648\n",
            "for batch8371 loss is 0.11055030673742294\n",
            "for batch8372 loss is 0.08045898377895355\n",
            "for batch8373 loss is 0.027007779106497765\n",
            "for batch8374 loss is 0.05895431712269783\n",
            "for batch8375 loss is 0.02568414807319641\n",
            "for batch8376 loss is 0.09844225645065308\n",
            "for batch8377 loss is 0.05382062867283821\n",
            "for batch8378 loss is 0.02073548547923565\n",
            "for batch8379 loss is 0.02228420414030552\n",
            "for batch8380 loss is 0.003150759730488062\n",
            "for batch8381 loss is 0.0035112991463392973\n",
            "for batch8382 loss is 0.007231149822473526\n",
            "for batch8383 loss is 0.07784847170114517\n",
            "for batch8384 loss is 0.06609772890806198\n",
            "for batch8385 loss is 0.003238015342503786\n",
            "for batch8386 loss is 2.384185648907078e-08\n",
            "for batch8387 loss is 0.011615991592407227\n",
            "for batch8388 loss is 0.17433610558509827\n",
            "for batch8389 loss is 0.0367075614631176\n",
            "for batch8390 loss is 0.0018301999662071466\n",
            "for batch8391 loss is 0.09269475936889648\n",
            "for batch8392 loss is 0.005583073943853378\n",
            "for batch8393 loss is 3.0851662158966064\n",
            "for batch8394 loss is 0.00014695498975925148\n",
            "for batch8395 loss is 0.17117774486541748\n",
            "for batch8396 loss is 0.0006484018522314727\n",
            "for batch8397 loss is 0.005533577408641577\n",
            "for batch8398 loss is 0.17192231118679047\n",
            "for batch8399 loss is 0.07423368096351624\n",
            "for batch8400 loss is 0.6551058888435364\n",
            "for batch8401 loss is 0.306812584400177\n",
            "for batch8402 loss is 6.79140430293046e-05\n",
            "for batch8403 loss is 0.001603481126949191\n",
            "for batch8404 loss is 0.30859360098838806\n",
            "for batch8405 loss is 0.1391240656375885\n",
            "for batch8406 loss is 0.033281903713941574\n",
            "for batch8407 loss is 0.00015701449592597783\n",
            "for batch8408 loss is 9.298303780269634e-07\n",
            "for batch8409 loss is 0.1031915694475174\n",
            "for batch8410 loss is 1.311298774453462e-06\n",
            "for batch8411 loss is 0.014436056837439537\n",
            "for batch8412 loss is 0.01010504923760891\n",
            "for batch8413 loss is 0.00011198563879588619\n",
            "for batch8414 loss is 0.03887959569692612\n",
            "for batch8415 loss is 0.00026673165848478675\n",
            "for batch8416 loss is 0.0006570291006937623\n",
            "for batch8417 loss is 0.022809576243162155\n",
            "for batch8418 loss is 1.7642927332417457e-06\n",
            "for batch8419 loss is 0.04031030088663101\n",
            "for batch8420 loss is 0.0033210706897079945\n",
            "for batch8421 loss is 0.05033533647656441\n",
            "for batch8422 loss is 0.2008732557296753\n",
            "for batch8423 loss is 0.08841900527477264\n",
            "for batch8424 loss is 0.051992226392030716\n",
            "for batch8425 loss is 0.005752064753323793\n",
            "for batch8426 loss is 0.09816443175077438\n",
            "for batch8427 loss is 0.0016041869530454278\n",
            "for batch8428 loss is 0.13631990551948547\n",
            "for batch8429 loss is 0.02696833573281765\n",
            "for batch8430 loss is 0.47602421045303345\n",
            "for batch8431 loss is 0.011811716482043266\n",
            "for batch8432 loss is 9.347191371489316e-05\n",
            "for batch8433 loss is 0.27214735746383667\n",
            "for batch8434 loss is 0.015418295748531818\n",
            "for batch8435 loss is 2.384185648907078e-08\n",
            "for batch8436 loss is 0.2699633836746216\n",
            "for batch8437 loss is 0.0011640600860118866\n",
            "for batch8438 loss is 0.00016149402654264122\n",
            "for batch8439 loss is 0.04916473478078842\n",
            "for batch8440 loss is 0.07561253011226654\n",
            "for batch8441 loss is 0.45680898427963257\n",
            "for batch8442 loss is 0.8080860376358032\n",
            "for batch8443 loss is 1.4743177890777588\n",
            "for batch8444 loss is 0.5098773241043091\n",
            "for batch8445 loss is 0.005094381980597973\n",
            "for batch8446 loss is 0.0018135610735043883\n",
            "for batch8447 loss is 0.001004429766908288\n",
            "for batch8448 loss is 1.1804896593093872\n",
            "for batch8449 loss is 2.6354928016662598\n",
            "for batch8450 loss is 0.01916424371302128\n",
            "for batch8451 loss is 3.2279342121910304e-05\n",
            "for batch8452 loss is 0.01177279744297266\n",
            "for batch8453 loss is 0.014340323396027088\n",
            "for batch8454 loss is 0.310729444026947\n",
            "for batch8455 loss is 0.2856941819190979\n",
            "for batch8456 loss is 0.3010939359664917\n",
            "for batch8457 loss is 1.6486549377441406\n",
            "for batch8458 loss is 0.07939563691616058\n",
            "for batch8459 loss is 0.00545869255438447\n",
            "for batch8460 loss is 0.7305559515953064\n",
            "for batch8461 loss is 0.00364105892367661\n",
            "for batch8462 loss is 0.1869390904903412\n",
            "for batch8463 loss is 0.0006452976958826184\n",
            "for batch8464 loss is 0.02299092523753643\n",
            "for batch8465 loss is 1.907348234908568e-07\n",
            "for batch8466 loss is 0.40313273668289185\n",
            "for batch8467 loss is 0.41951560974121094\n",
            "for batch8468 loss is 0.2544082999229431\n",
            "for batch8469 loss is 0.11722272634506226\n",
            "for batch8470 loss is 0.0013694650260731578\n",
            "for batch8471 loss is 0.21932677924633026\n",
            "for batch8472 loss is 0.0027106301859021187\n",
            "for batch8473 loss is 0.5708262920379639\n",
            "for batch8474 loss is 0.050716888159513474\n",
            "for batch8475 loss is 1.3589817626780132e-06\n",
            "for batch8476 loss is 0.27995365858078003\n",
            "for batch8477 loss is 0.19676247239112854\n",
            "for batch8478 loss is 0.070286326110363\n",
            "for batch8479 loss is 2.4649271965026855\n",
            "for batch8480 loss is 0.27616041898727417\n",
            "for batch8481 loss is 0.0029099532403051853\n",
            "for batch8482 loss is 0.0063889725133776665\n",
            "for batch8483 loss is 0.0025375480763614178\n",
            "for batch8484 loss is 0.041810065507888794\n",
            "for batch8485 loss is 0.005556304007768631\n",
            "for batch8486 loss is 0.055436741560697556\n",
            "for batch8487 loss is 0.021045777946710587\n",
            "for batch8488 loss is 0.029904115945100784\n",
            "for batch8489 loss is 0.13085822761058807\n",
            "for batch8490 loss is 0.04633127525448799\n",
            "for batch8491 loss is 0.013581864535808563\n",
            "for batch8492 loss is 0.00010027850657934323\n",
            "for batch8493 loss is 0.012703384272754192\n",
            "for batch8494 loss is 1.5258743815138587e-06\n",
            "for batch8495 loss is 0.44707298278808594\n",
            "for batch8496 loss is 0.004882938228547573\n",
            "for batch8497 loss is 0.000638399098534137\n",
            "for batch8498 loss is 0.017527801916003227\n",
            "for batch8499 loss is 0.16331805288791656\n",
            "for batch8500 loss is 0.05265877768397331\n",
            "for batch8501 loss is 0.05564095452427864\n",
            "for batch8502 loss is 0.11870969831943512\n",
            "for batch8503 loss is 1.063318813976366e-05\n",
            "for batch8504 loss is 0.14679133892059326\n",
            "for batch8505 loss is 0.10443564504384995\n",
            "for batch8506 loss is 0.017212292179465294\n",
            "for batch8507 loss is 0.2519710659980774\n",
            "for batch8508 loss is 0.0018914904212579131\n",
            "for batch8509 loss is 0.04028493911027908\n",
            "for batch8510 loss is 0.00014580274000763893\n",
            "for batch8511 loss is 0.04440451040863991\n",
            "for batch8512 loss is 0.0006411693757399917\n",
            "for batch8513 loss is 1.3656220436096191\n",
            "for batch8514 loss is 0.02261827327311039\n",
            "for batch8515 loss is 0.3947044014930725\n",
            "for batch8516 loss is 0.22521671652793884\n",
            "for batch8517 loss is 0.0037329066544771194\n",
            "for batch8518 loss is 0.010945960879325867\n",
            "for batch8519 loss is 0.04478199779987335\n",
            "for batch8520 loss is 0.0033574216067790985\n",
            "for batch8521 loss is 0.03484132140874863\n",
            "for batch8522 loss is 0.4832211136817932\n",
            "for batch8523 loss is 0.21841058135032654\n",
            "for batch8524 loss is 8.236039866460487e-05\n",
            "for batch8525 loss is 0.06982069462537766\n",
            "for batch8526 loss is 0.0005675967549905181\n",
            "for batch8527 loss is 0.08677200973033905\n",
            "for batch8528 loss is 1.3848791122436523\n",
            "for batch8529 loss is 0.1188850849866867\n",
            "for batch8530 loss is 0.03588782250881195\n",
            "for batch8531 loss is 0.006895267870277166\n",
            "for batch8532 loss is 0.046349816024303436\n",
            "for batch8533 loss is 0.00013079810014460236\n",
            "for batch8534 loss is 1.2551846504211426\n",
            "for batch8535 loss is 0.028063837438821793\n",
            "for batch8536 loss is 0.06549777090549469\n",
            "for batch8537 loss is 0.010680717416107655\n",
            "for batch8538 loss is 0.0010446783853694797\n",
            "for batch8539 loss is 0.07213885337114334\n",
            "for batch8540 loss is 0.009419950656592846\n",
            "for batch8541 loss is 0.0006108530214987695\n",
            "for batch8542 loss is 0.058402739465236664\n",
            "for batch8543 loss is 0.427304208278656\n",
            "for batch8544 loss is 0.042372677475214005\n",
            "for batch8545 loss is 0.22992482781410217\n",
            "for batch8546 loss is 0.0024065240286290646\n",
            "for batch8547 loss is 0.30576616525650024\n",
            "for batch8548 loss is 0.37709560990333557\n",
            "for batch8549 loss is 0.008255896158516407\n",
            "for batch8550 loss is 0.1793876588344574\n",
            "for batch8551 loss is 0.07921634614467621\n",
            "for batch8552 loss is 0.6330069303512573\n",
            "for batch8553 loss is 0.09885537624359131\n",
            "for batch8554 loss is 1.8358181250732741e-06\n",
            "for batch8555 loss is 0.38042283058166504\n",
            "for batch8556 loss is 0.010263996198773384\n",
            "for batch8557 loss is 0.06556083261966705\n",
            "for batch8558 loss is 0.09586696326732635\n",
            "for batch8559 loss is 0.002313460921868682\n",
            "for batch8560 loss is 0.0002723122597672045\n",
            "for batch8561 loss is 0.015774140134453773\n",
            "for batch8562 loss is 0.012719148769974709\n",
            "for batch8563 loss is 0.016095779836177826\n",
            "for batch8564 loss is 0.1319989413022995\n",
            "for batch8565 loss is 0.047595031559467316\n",
            "for batch8566 loss is 0.03996431082487106\n",
            "for batch8567 loss is 0.16611185669898987\n",
            "for batch8568 loss is 0.0024150216486305\n",
            "for batch8569 loss is 0.009964710101485252\n",
            "for batch8570 loss is 0.04074380546808243\n",
            "for batch8571 loss is 0.01327479351311922\n",
            "for batch8572 loss is 0.4004184603691101\n",
            "for batch8573 loss is 0.024363722652196884\n",
            "for batch8574 loss is 1.638393759727478\n",
            "for batch8575 loss is 0.03597492724657059\n",
            "for batch8576 loss is 0.004577228333801031\n",
            "for batch8577 loss is 0.00040045735659077764\n",
            "for batch8578 loss is 0.4077073037624359\n",
            "for batch8579 loss is 0.06365390866994858\n",
            "for batch8580 loss is 0.025615787133574486\n",
            "for batch8581 loss is 0.4078170657157898\n",
            "for batch8582 loss is 0.10844551026821136\n",
            "for batch8583 loss is 0.059451960027217865\n",
            "for batch8584 loss is 0.20845815539360046\n",
            "for batch8585 loss is 0.1405089646577835\n",
            "for batch8586 loss is 0.24317607283592224\n",
            "for batch8587 loss is 0.00029186843312345445\n",
            "for batch8588 loss is 0.12238346040248871\n",
            "for batch8589 loss is 0.005271582398563623\n",
            "for batch8590 loss is 0.04369787499308586\n",
            "for batch8591 loss is 0.1751306653022766\n",
            "for batch8592 loss is 1.1724853515625\n",
            "for batch8593 loss is 0.028961334377527237\n",
            "for batch8594 loss is 0.4908056855201721\n",
            "for batch8595 loss is 3.628420745371841e-05\n",
            "for batch8596 loss is 0.07736764848232269\n",
            "for batch8597 loss is 0.1505483239889145\n",
            "for batch8598 loss is 0.04371463134884834\n",
            "for batch8599 loss is 0.7555865049362183\n",
            "for batch8600 loss is 0.36696261167526245\n",
            "for batch8601 loss is 0.14568065106868744\n",
            "for batch8602 loss is 0.4012201428413391\n",
            "for batch8603 loss is 0.04085410386323929\n",
            "for batch8604 loss is 4.768370942542788e-08\n",
            "for batch8605 loss is 0.17546357214450836\n",
            "for batch8606 loss is 0.04557974264025688\n",
            "for batch8607 loss is 0.04847562313079834\n",
            "for batch8608 loss is 0.0\n",
            "for batch8609 loss is 0.0010395306162536144\n",
            "for batch8610 loss is 0.03018972836434841\n",
            "for batch8611 loss is 0.0119413360953331\n",
            "for batch8612 loss is 0.19386547803878784\n",
            "for batch8613 loss is 0.10475047677755356\n",
            "for batch8614 loss is 0.09437751024961472\n",
            "for batch8615 loss is 0.18978922069072723\n",
            "for batch8616 loss is 0.02713124081492424\n",
            "for batch8617 loss is 0.014242877252399921\n",
            "for batch8618 loss is 0.3063346743583679\n",
            "for batch8619 loss is 0.39307141304016113\n",
            "for batch8620 loss is 0.35079309344291687\n",
            "for batch8621 loss is 0.08819358050823212\n",
            "for batch8622 loss is 0.03126559406518936\n",
            "for batch8623 loss is 0.012687260285019875\n",
            "for batch8624 loss is 0.07912005484104156\n",
            "for batch8625 loss is 0.0018463613232597709\n",
            "for batch8626 loss is 0.09705054014921188\n",
            "for batch8627 loss is 0.013756277970969677\n",
            "for batch8628 loss is 0.04847142472863197\n",
            "for batch8629 loss is 0.014986695721745491\n",
            "for batch8630 loss is 1.525844527350273e-05\n",
            "for batch8631 loss is 0.0014944844879209995\n",
            "for batch8632 loss is 0.0306512713432312\n",
            "for batch8633 loss is 0.01483825407922268\n",
            "for batch8634 loss is 0.07929117977619171\n",
            "for batch8635 loss is 0.12389928102493286\n",
            "for batch8636 loss is 0.17118816077709198\n",
            "for batch8637 loss is 0.021855365484952927\n",
            "for batch8638 loss is 0.15252137184143066\n",
            "for batch8639 loss is 0.08024802058935165\n",
            "for batch8640 loss is 0.00024759178631938994\n",
            "for batch8641 loss is 9.467752533964813e-05\n",
            "for batch8642 loss is 0.024832971394062042\n",
            "for batch8643 loss is 0.0014349641278386116\n",
            "for batch8644 loss is 0.031014248728752136\n",
            "for batch8645 loss is 0.0304134339094162\n",
            "for batch8646 loss is 0.3450701832771301\n",
            "for batch8647 loss is 1.686070203781128\n",
            "for batch8648 loss is 0.6491065621376038\n",
            "for batch8649 loss is 0.24723155796527863\n",
            "for batch8650 loss is 0.0020545567385852337\n",
            "for batch8651 loss is 0.34687429666519165\n",
            "for batch8652 loss is 0.32488468289375305\n",
            "for batch8653 loss is 0.04643452912569046\n",
            "for batch8654 loss is 0.17948554456233978\n",
            "for batch8655 loss is 0.10986609756946564\n",
            "for batch8656 loss is 0.0008411445887759328\n",
            "for batch8657 loss is 0.2113807648420334\n",
            "for batch8658 loss is 0.49690741300582886\n",
            "for batch8659 loss is 0.04522743821144104\n",
            "for batch8660 loss is 0.02724243327975273\n",
            "for batch8661 loss is 0.2521180212497711\n",
            "for batch8662 loss is 0.005844040308147669\n",
            "for batch8663 loss is 0.029085978865623474\n",
            "for batch8664 loss is 0.05164174363017082\n",
            "for batch8665 loss is 4.100757450942183e-06\n",
            "for batch8666 loss is 0.3056184947490692\n",
            "for batch8667 loss is 0.0776280090212822\n",
            "for batch8668 loss is 0.01955864205956459\n",
            "for batch8669 loss is 0.05221300199627876\n",
            "for batch8670 loss is 0.07455917447805405\n",
            "for batch8671 loss is 0.14760351181030273\n",
            "for batch8672 loss is 0.35374054312705994\n",
            "for batch8673 loss is 0.0009089579107239842\n",
            "for batch8674 loss is 0.23062773048877716\n",
            "for batch8675 loss is 0.44758519530296326\n",
            "for batch8676 loss is 0.044441141188144684\n",
            "for batch8677 loss is 0.4155082106590271\n",
            "for batch8678 loss is 0.4263364374637604\n",
            "for batch8679 loss is 0.01725010946393013\n",
            "for batch8680 loss is 0.1451876163482666\n",
            "for batch8681 loss is 0.04489314183592796\n",
            "for batch8682 loss is 0.2013905793428421\n",
            "for batch8683 loss is 0.20496633648872375\n",
            "for batch8684 loss is 0.3990429937839508\n",
            "for batch8685 loss is 0.013299478217959404\n",
            "for batch8686 loss is 0.2918684184551239\n",
            "for batch8687 loss is 0.03607655689120293\n",
            "for batch8688 loss is 0.0016238484531641006\n",
            "for batch8689 loss is 0.014522666111588478\n",
            "for batch8690 loss is 0.06796981394290924\n",
            "for batch8691 loss is 0.1366727352142334\n",
            "for batch8692 loss is 0.030794784426689148\n",
            "for batch8693 loss is 0.0011239834129810333\n",
            "for batch8694 loss is 0.004076586104929447\n",
            "for batch8695 loss is 0.15242668986320496\n",
            "for batch8696 loss is 0.00012403499567881227\n",
            "for batch8697 loss is 0.43545809388160706\n",
            "for batch8698 loss is 0.11466147005558014\n",
            "for batch8699 loss is 2.260157089040149e-05\n",
            "for batch8700 loss is 0.0001840254117269069\n",
            "for batch8701 loss is 0.3206223249435425\n",
            "for batch8702 loss is 0.06237981468439102\n",
            "for batch8703 loss is 0.5939153432846069\n",
            "for batch8704 loss is 0.15381832420825958\n",
            "for batch8705 loss is 0.263599693775177\n",
            "for batch8706 loss is 0.015563681721687317\n",
            "for batch8707 loss is 0.3187089264392853\n",
            "for batch8708 loss is 0.018395669758319855\n",
            "for batch8709 loss is 0.06221967190504074\n",
            "for batch8710 loss is 0.0023205021861940622\n",
            "for batch8711 loss is 0.10536347329616547\n",
            "for batch8712 loss is 0.3437647223472595\n",
            "for batch8713 loss is 0.1129106655716896\n",
            "for batch8714 loss is 1.1101897954940796\n",
            "for batch8715 loss is 0.002595965750515461\n",
            "for batch8716 loss is 0.01241291407495737\n",
            "for batch8717 loss is 0.45607882738113403\n",
            "for batch8718 loss is 0.16689176857471466\n",
            "for batch8719 loss is 0.03307633474469185\n",
            "for batch8720 loss is 0.022760212421417236\n",
            "for batch8721 loss is 0.0007793349795974791\n",
            "for batch8722 loss is 0.0007586721330881119\n",
            "for batch8723 loss is 0.004078486002981663\n",
            "for batch8724 loss is 0.20808939635753632\n",
            "for batch8725 loss is 0.00231202132999897\n",
            "for batch8726 loss is 0.014443745836615562\n",
            "for batch8727 loss is 0.0073080891743302345\n",
            "for batch8728 loss is 0.07717305421829224\n",
            "for batch8729 loss is 0.018973805010318756\n",
            "for batch8730 loss is 0.020812615752220154\n",
            "for batch8731 loss is 0.08776223659515381\n",
            "for batch8732 loss is 0.09823485463857651\n",
            "for batch8733 loss is 0.13147298991680145\n",
            "for batch8734 loss is 0.0018289871513843536\n",
            "for batch8735 loss is 0.006519777234643698\n",
            "for batch8736 loss is 0.009057017043232918\n",
            "for batch8737 loss is 0.05460641533136368\n",
            "for batch8738 loss is 0.012299862690269947\n",
            "for batch8739 loss is 0.11876678466796875\n",
            "for batch8740 loss is 0.0016843455377966166\n",
            "for batch8741 loss is 0.2696836590766907\n",
            "for batch8742 loss is 0.0069397734478116035\n",
            "for batch8743 loss is 0.00015191487909760326\n",
            "for batch8744 loss is 0.7463101744651794\n",
            "for batch8745 loss is 0.0005759758641943336\n",
            "for batch8746 loss is 0.03980375453829765\n",
            "for batch8747 loss is 0.2701164782047272\n",
            "for batch8748 loss is 0.594861626625061\n",
            "for batch8749 loss is 0.7380243539810181\n",
            "for batch8750 loss is 0.267147034406662\n",
            "for batch8751 loss is 0.01714957132935524\n",
            "for batch8752 loss is 0.08014605194330215\n",
            "for batch8753 loss is 0.01500186137855053\n",
            "for batch8754 loss is 0.10074460506439209\n",
            "for batch8755 loss is 0.06659220159053802\n",
            "for batch8756 loss is 0.00016613346815574914\n",
            "for batch8757 loss is 0.005219914019107819\n",
            "for batch8758 loss is 0.013548610731959343\n",
            "for batch8759 loss is 0.07828029245138168\n",
            "for batch8760 loss is 0.008329354226589203\n",
            "for batch8761 loss is 0.0005781601066701114\n",
            "for batch8762 loss is 0.00018774237832985818\n",
            "for batch8763 loss is 0.01766238734126091\n",
            "for batch8764 loss is 0.09752292931079865\n",
            "for batch8765 loss is 0.13963837921619415\n",
            "for batch8766 loss is 0.01449001394212246\n",
            "for batch8767 loss is 0.7441917657852173\n",
            "for batch8768 loss is 0.025019749999046326\n",
            "for batch8769 loss is 0.00030417731613852084\n",
            "for batch8770 loss is 0.11413111537694931\n",
            "for batch8771 loss is 0.0015268755378201604\n",
            "for batch8772 loss is 0.023614848032593727\n",
            "for batch8773 loss is 0.1861812025308609\n",
            "for batch8774 loss is 0.3680729568004608\n",
            "for batch8775 loss is 0.17797178030014038\n",
            "for batch8776 loss is 0.8237291574478149\n",
            "for batch8777 loss is 0.2316613495349884\n",
            "for batch8778 loss is 0.21271029114723206\n",
            "for batch8779 loss is 0.31426408886909485\n",
            "for batch8780 loss is 1.598995566368103\n",
            "for batch8781 loss is 0.007258879952132702\n",
            "for batch8782 loss is 0.00416866410523653\n",
            "for batch8783 loss is 0.0007192796329036355\n",
            "for batch8784 loss is 0.031637322157621384\n",
            "for batch8785 loss is 0.11692212522029877\n",
            "for batch8786 loss is 0.5172379612922668\n",
            "for batch8787 loss is 0.004083077423274517\n",
            "for batch8788 loss is 0.12494675070047379\n",
            "for batch8789 loss is 7.152556236178498e-08\n",
            "for batch8790 loss is 0.026079723611474037\n",
            "for batch8791 loss is 0.000353620300302282\n",
            "for batch8792 loss is 0.00206239172257483\n",
            "for batch8793 loss is 0.0012462114682421088\n",
            "for batch8794 loss is 0.014070090837776661\n",
            "for batch8795 loss is 0.0010624019196256995\n",
            "for batch8796 loss is 0.010577909648418427\n",
            "for batch8797 loss is 0.016069704666733742\n",
            "for batch8798 loss is 0.0022222022525966167\n",
            "for batch8799 loss is 0.004584037698805332\n",
            "for batch8800 loss is 2.1552017642534338e-05\n",
            "for batch8801 loss is 0.06813659518957138\n",
            "for batch8802 loss is 0.01570623740553856\n",
            "for batch8803 loss is 0.016835223883390427\n",
            "for batch8804 loss is 0.0018993092235177755\n",
            "for batch8805 loss is 0.00030574799166060984\n",
            "for batch8806 loss is 0.4707132875919342\n",
            "for batch8807 loss is 0.735225260257721\n",
            "for batch8808 loss is 0.13345149159431458\n",
            "for batch8809 loss is 0.0171138197183609\n",
            "for batch8810 loss is 6.624554953305051e-05\n",
            "for batch8811 loss is 0.0009419511770829558\n",
            "for batch8812 loss is 0.014679129235446453\n",
            "for batch8813 loss is 0.2660423219203949\n",
            "for batch8814 loss is 0.055492252111434937\n",
            "for batch8815 loss is 0.07758727669715881\n",
            "for batch8816 loss is 3.211298826499842e-05\n",
            "for batch8817 loss is 0.257572740316391\n",
            "for batch8818 loss is 0.03798817843198776\n",
            "for batch8819 loss is 0.0012929344084113836\n",
            "for batch8820 loss is 0.0037257603835314512\n",
            "for batch8821 loss is 0.003704880131408572\n",
            "for batch8822 loss is 0.02650739625096321\n",
            "for batch8823 loss is 0.15069709718227386\n",
            "for batch8824 loss is 0.03216353803873062\n",
            "for batch8825 loss is 0.4632503092288971\n",
            "for batch8826 loss is 0.047290608286857605\n",
            "for batch8827 loss is 0.011892445385456085\n",
            "for batch8828 loss is 0.37452274560928345\n",
            "for batch8829 loss is 0.5361044406890869\n",
            "for batch8830 loss is 0.0925024002790451\n",
            "for batch8831 loss is 0.0001690259377937764\n",
            "for batch8832 loss is 0.00012760140816681087\n",
            "for batch8833 loss is 0.0032515819184482098\n",
            "for batch8834 loss is 0.7298898696899414\n",
            "for batch8835 loss is 0.9712504148483276\n",
            "for batch8836 loss is 0.012718056328594685\n",
            "for batch8837 loss is 0.01950654946267605\n",
            "for batch8838 loss is 0.08682559430599213\n",
            "for batch8839 loss is 0.024571409448981285\n",
            "for batch8840 loss is 0.018370114266872406\n",
            "for batch8841 loss is 0.40208330750465393\n",
            "for batch8842 loss is 0.012678101658821106\n",
            "for batch8843 loss is 0.033271197229623795\n",
            "for batch8844 loss is 0.039979055523872375\n",
            "for batch8845 loss is 0.017787350341677666\n",
            "for batch8846 loss is 2.384185648907078e-08\n",
            "for batch8847 loss is 0.516880989074707\n",
            "for batch8848 loss is 0.00013418466551229358\n",
            "for batch8849 loss is 0.5005684494972229\n",
            "for batch8850 loss is 0.006733421236276627\n",
            "for batch8851 loss is 0.0007077606278471649\n",
            "for batch8852 loss is 0.16870523989200592\n",
            "for batch8853 loss is 0.0008785714162513614\n",
            "for batch8854 loss is 0.19100920855998993\n",
            "for batch8855 loss is 0.00047667190665379167\n",
            "for batch8856 loss is 0.0009508950752206147\n",
            "for batch8857 loss is 0.00025051989359781146\n",
            "for batch8858 loss is 0.13199034333229065\n",
            "for batch8859 loss is 0.039354950189590454\n",
            "for batch8860 loss is 0.010731030255556107\n",
            "for batch8861 loss is 0.08563194423913956\n",
            "for batch8862 loss is 0.0025258788373321295\n",
            "for batch8863 loss is 0.01285875029861927\n",
            "for batch8864 loss is 0.5196411609649658\n",
            "for batch8865 loss is 0.3060010075569153\n",
            "for batch8866 loss is 0.05062489956617355\n",
            "for batch8867 loss is 0.0006898603751324117\n",
            "for batch8868 loss is 0.03836793452501297\n",
            "for batch8869 loss is 0.000864133529830724\n",
            "for batch8870 loss is 0.04657728970050812\n",
            "for batch8871 loss is 1.2145519256591797\n",
            "for batch8872 loss is 0.35664528608322144\n",
            "for batch8873 loss is 0.12450379133224487\n",
            "for batch8874 loss is 0.001951943151652813\n",
            "for batch8875 loss is 0.04145781695842743\n",
            "for batch8876 loss is 0.024598661810159683\n",
            "for batch8877 loss is 0.0018015035893768072\n",
            "for batch8878 loss is 0.06188054755330086\n",
            "for batch8879 loss is 1.1920925402364446e-07\n",
            "for batch8880 loss is 5.9842241171281785e-06\n",
            "for batch8881 loss is 0.009764976799488068\n",
            "for batch8882 loss is 0.00012679854989983141\n",
            "for batch8883 loss is 1.3671623468399048\n",
            "for batch8884 loss is 0.00014916663349140435\n",
            "for batch8885 loss is 0.13078176975250244\n",
            "for batch8886 loss is 0.4963194727897644\n",
            "for batch8887 loss is 0.0006326690781861544\n",
            "for batch8888 loss is 0.018800681456923485\n",
            "for batch8889 loss is 0.0048484401777386665\n",
            "for batch8890 loss is 0.01987900771200657\n",
            "for batch8891 loss is 0.2513505816459656\n",
            "for batch8892 loss is 0.07956567406654358\n",
            "for batch8893 loss is 0.6010653972625732\n",
            "for batch8894 loss is 0.0011340633500367403\n",
            "for batch8895 loss is 0.012653231620788574\n",
            "for batch8896 loss is 0.45439139008522034\n",
            "for batch8897 loss is 0.0334404781460762\n",
            "for batch8898 loss is 0.0001490514405304566\n",
            "for batch8899 loss is 0.12230299413204193\n",
            "for batch8900 loss is 0.003221227554604411\n",
            "for batch8901 loss is 0.007629145868122578\n",
            "for batch8902 loss is 0.014283763244748116\n",
            "for batch8903 loss is 0.4364558160305023\n",
            "for batch8904 loss is 0.0004948712885379791\n",
            "for batch8905 loss is 0.0032357443124055862\n",
            "for batch8906 loss is 0.003633855376392603\n",
            "for batch8907 loss is 0.09719353914260864\n",
            "for batch8908 loss is 0.23290476202964783\n",
            "for batch8909 loss is 0.025891870260238647\n",
            "for batch8910 loss is 0.05034850910305977\n",
            "for batch8911 loss is 0.003652882296591997\n",
            "for batch8912 loss is 0.06431100517511368\n",
            "for batch8913 loss is 0.7809540033340454\n",
            "for batch8914 loss is 0.06550358235836029\n",
            "for batch8915 loss is 0.00020853236492257565\n",
            "for batch8916 loss is 0.025455299764871597\n",
            "for batch8917 loss is 0.29193058609962463\n",
            "for batch8918 loss is 0.4014050364494324\n",
            "for batch8919 loss is 0.03164329752326012\n",
            "for batch8920 loss is 0.4116900563240051\n",
            "for batch8921 loss is 0.028910795226693153\n",
            "for batch8922 loss is 0.09522511065006256\n",
            "for batch8923 loss is 0.691220760345459\n",
            "for batch8924 loss is 0.15435108542442322\n",
            "for batch8925 loss is 0.0006488744402304292\n",
            "for batch8926 loss is 2.4770157324383035e-05\n",
            "for batch8927 loss is 0.04941541701555252\n",
            "for batch8928 loss is 0.003444709349423647\n",
            "for batch8929 loss is 0.11013320833444595\n",
            "for batch8930 loss is 0.004313697572797537\n",
            "for batch8931 loss is 0.007557377219200134\n",
            "for batch8932 loss is 0.07614479213953018\n",
            "for batch8933 loss is 0.024205811321735382\n",
            "for batch8934 loss is 0.006791041698306799\n",
            "for batch8935 loss is 0.011069010011851788\n",
            "for batch8936 loss is 0.2347574234008789\n",
            "for batch8937 loss is 0.02742188796401024\n",
            "for batch8938 loss is 0.1798124462366104\n",
            "for batch8939 loss is 0.0914715975522995\n",
            "for batch8940 loss is 0.00812625139951706\n",
            "for batch8941 loss is 2.758377195277717e-05\n",
            "for batch8942 loss is 0.01644919253885746\n",
            "for batch8943 loss is 0.010168034583330154\n",
            "for batch8944 loss is 0.04314037039875984\n",
            "for batch8945 loss is 0.22632166743278503\n",
            "for batch8946 loss is 0.21469314396381378\n",
            "for batch8947 loss is 2.4580560420872644e-05\n",
            "for batch8948 loss is 0.051044873893260956\n",
            "for batch8949 loss is 0.004782216157764196\n",
            "for batch8950 loss is 0.23228208720684052\n",
            "for batch8951 loss is 0.04803084209561348\n",
            "for batch8952 loss is 1.1792199611663818\n",
            "for batch8953 loss is 0.15437310934066772\n",
            "for batch8954 loss is 0.020713986828923225\n",
            "for batch8955 loss is 0.008401799015700817\n",
            "for batch8956 loss is 0.0005903582205064595\n",
            "for batch8957 loss is 2.019125461578369\n",
            "for batch8958 loss is 0.0001902740477817133\n",
            "for batch8959 loss is 0.15207819640636444\n",
            "for batch8960 loss is 0.19604134559631348\n",
            "for batch8961 loss is 0.03302984684705734\n",
            "for batch8962 loss is 0.06777215003967285\n",
            "for batch8963 loss is 0.459827721118927\n",
            "for batch8964 loss is 0.05920378491282463\n",
            "for batch8965 loss is 0.0007837858865968883\n",
            "for batch8966 loss is 0.09794719517230988\n",
            "for batch8967 loss is 0.1682475507259369\n",
            "for batch8968 loss is 0.12572543323040009\n",
            "for batch8969 loss is 0.02535741962492466\n",
            "for batch8970 loss is 0.05944466590881348\n",
            "for batch8971 loss is 0.08757617324590683\n",
            "for batch8972 loss is 0.10188277065753937\n",
            "for batch8973 loss is 0.06843017786741257\n",
            "for batch8974 loss is 0.017406318336725235\n",
            "for batch8975 loss is 0.04450765997171402\n",
            "for batch8976 loss is 0.10729438066482544\n",
            "for batch8977 loss is 0.039255477488040924\n",
            "for batch8978 loss is 0.0014476589858531952\n",
            "for batch8979 loss is 0.12462000548839569\n",
            "for batch8980 loss is 0.3624620735645294\n",
            "for batch8981 loss is 0.00506172701716423\n",
            "for batch8982 loss is 0.819902777671814\n",
            "for batch8983 loss is 0.12457253038883209\n",
            "for batch8984 loss is 0.1297459751367569\n",
            "for batch8985 loss is 0.0904286801815033\n",
            "for batch8986 loss is 1.2765278816223145\n",
            "for batch8987 loss is 0.02565949223935604\n",
            "for batch8988 loss is 0.05265369266271591\n",
            "for batch8989 loss is 0.015679750591516495\n",
            "for batch8990 loss is 0.00012277938367333263\n",
            "for batch8991 loss is 5.1020924729527906e-06\n",
            "for batch8992 loss is 0.07282670587301254\n",
            "for batch8993 loss is 0.007366611156612635\n",
            "for batch8994 loss is 0.018957149237394333\n",
            "for batch8995 loss is 0.017444001510739326\n",
            "for batch8996 loss is 0.00010826545621966943\n",
            "for batch8997 loss is 0.00150644825771451\n",
            "for batch8998 loss is 0.0012975388672202826\n",
            "for batch8999 loss is 0.02421070635318756\n",
            "for batch9000 loss is 0.005032524932175875\n",
            "for batch9001 loss is 0.36260202527046204\n",
            "for batch9002 loss is 6.584405491594225e-05\n",
            "for batch9003 loss is 4.458378498384263e-06\n",
            "for batch9004 loss is 0.4994756281375885\n",
            "for batch9005 loss is 0.05062365531921387\n",
            "for batch9006 loss is 0.08674238622188568\n",
            "for batch9007 loss is 0.09705903381109238\n",
            "for batch9008 loss is 0.014354482293128967\n",
            "for batch9009 loss is 0.05516326427459717\n",
            "for batch9010 loss is 0.06654521822929382\n",
            "for batch9011 loss is 0.006543937139213085\n",
            "for batch9012 loss is 1.5091495697561186e-05\n",
            "for batch9013 loss is 0.1373348832130432\n",
            "for batch9014 loss is 0.00036284970701672137\n",
            "for batch9015 loss is 0.06722649931907654\n",
            "for batch9016 loss is 0.06786088645458221\n",
            "for batch9017 loss is 0.12994375824928284\n",
            "for batch9018 loss is 0.0018053343519568443\n",
            "for batch9019 loss is 0.008762378245592117\n",
            "for batch9020 loss is 0.7274338006973267\n",
            "for batch9021 loss is 0.0061851670034229755\n",
            "for batch9022 loss is 0.17384573817253113\n",
            "for batch9023 loss is 0.16719919443130493\n",
            "for batch9024 loss is 0.33561843633651733\n",
            "for batch9025 loss is 8.7516731582582e-05\n",
            "for batch9026 loss is 0.9666228294372559\n",
            "for batch9027 loss is 1.6689293147464923e-07\n",
            "for batch9028 loss is 0.0003561470075510442\n",
            "for batch9029 loss is 0.007478180341422558\n",
            "for batch9030 loss is 0.033340610563755035\n",
            "for batch9031 loss is 0.13423113524913788\n",
            "for batch9032 loss is 0.02679465152323246\n",
            "for batch9033 loss is 0.020362894982099533\n",
            "for batch9034 loss is 0.036965347826480865\n",
            "for batch9035 loss is 0.0014794182498008013\n",
            "for batch9036 loss is 0.0001405063085258007\n",
            "for batch9037 loss is 1.6986274719238281\n",
            "for batch9038 loss is 0.0030104166362434626\n",
            "for batch9039 loss is 0.0948486477136612\n",
            "for batch9040 loss is 0.10435882955789566\n",
            "for batch9041 loss is 0.6409643888473511\n",
            "for batch9042 loss is 0.21719269454479218\n",
            "for batch9043 loss is 0.31974321603775024\n",
            "for batch9044 loss is 0.05139927938580513\n",
            "for batch9045 loss is 0.14767195284366608\n",
            "for batch9046 loss is 0.0056056613102555275\n",
            "for batch9047 loss is 0.018064353615045547\n",
            "for batch9048 loss is 0.010538935661315918\n",
            "for batch9049 loss is 0.45800286531448364\n",
            "for batch9050 loss is 8.90391311259009e-05\n",
            "for batch9051 loss is 0.020977631211280823\n",
            "for batch9052 loss is 0.013916400261223316\n",
            "for batch9053 loss is 0.7091755867004395\n",
            "for batch9054 loss is 0.4766598343849182\n",
            "for batch9055 loss is 0.001247443724423647\n",
            "for batch9056 loss is 0.03786832466721535\n",
            "for batch9057 loss is 0.07381151616573334\n",
            "for batch9058 loss is 0.8719209432601929\n",
            "for batch9059 loss is 0.5968385934829712\n",
            "for batch9060 loss is 0.5430092811584473\n",
            "for batch9061 loss is 0.14204072952270508\n",
            "for batch9062 loss is 0.09977032989263535\n",
            "for batch9063 loss is 0.05293198302388191\n",
            "for batch9064 loss is 0.00018781411927193403\n",
            "for batch9065 loss is 0.012957031838595867\n",
            "for batch9066 loss is 0.004826555494219065\n",
            "for batch9067 loss is 0.006125998217612505\n",
            "for batch9068 loss is 0.15865448117256165\n",
            "for batch9069 loss is 3.506983193801716e-05\n",
            "for batch9070 loss is 0.005576492287218571\n",
            "for batch9071 loss is 0.0007858166354708374\n",
            "for batch9072 loss is 0.10604510456323624\n",
            "for batch9073 loss is 0.00036775696207769215\n",
            "for batch9074 loss is 0.30419886112213135\n",
            "for batch9075 loss is 1.1682296644721646e-05\n",
            "for batch9076 loss is 0.013007569126784801\n",
            "for batch9077 loss is 0.2926894724369049\n",
            "for batch9078 loss is 0.00027482720906846225\n",
            "for batch9079 loss is 0.037113357335329056\n",
            "for batch9080 loss is 0.0019615290220826864\n",
            "for batch9081 loss is 0.13219718635082245\n",
            "for batch9082 loss is 0.1678467094898224\n",
            "for batch9083 loss is 0.001216474105603993\n",
            "for batch9084 loss is 0.12141134589910507\n",
            "for batch9085 loss is 0.03689651936292648\n",
            "for batch9086 loss is 0.11092527955770493\n",
            "for batch9087 loss is 1.118156887969235e-05\n",
            "for batch9088 loss is 0.00728074973449111\n",
            "for batch9089 loss is 0.003397387219592929\n",
            "for batch9090 loss is 3.3378586294929846e-07\n",
            "for batch9091 loss is 0.00900875125080347\n",
            "for batch9092 loss is 0.0030141868628561497\n",
            "for batch9093 loss is 0.41423314809799194\n",
            "for batch9094 loss is 0.05745501071214676\n",
            "for batch9095 loss is 0.05876140668988228\n",
            "for batch9096 loss is 0.11278368532657623\n",
            "for batch9097 loss is 0.0032492843456566334\n",
            "for batch9098 loss is 0.0027141093742102385\n",
            "for batch9099 loss is 0.12900084257125854\n",
            "for batch9100 loss is 0.1781085878610611\n",
            "for batch9101 loss is 9.53674117454284e-08\n",
            "for batch9102 loss is 0.00040827965131029487\n",
            "for batch9103 loss is 0.0003312230110168457\n",
            "for batch9104 loss is 0.007477269973605871\n",
            "for batch9105 loss is 0.009356250055134296\n",
            "for batch9106 loss is 0.032233115285634995\n",
            "for batch9107 loss is 0.012684300541877747\n",
            "for batch9108 loss is 0.08448545634746552\n",
            "for batch9109 loss is 0.0012166848173364997\n",
            "for batch9110 loss is 0.2239067256450653\n",
            "for batch9111 loss is 0.0064314790070056915\n",
            "for batch9112 loss is 0.041325345635414124\n",
            "for batch9113 loss is 0.5043020844459534\n",
            "for batch9114 loss is 0.0013016177108511329\n",
            "for batch9115 loss is 0.9459807276725769\n",
            "for batch9116 loss is 0.23768576979637146\n",
            "for batch9117 loss is 0.01607571169734001\n",
            "for batch9118 loss is 0.43960341811180115\n",
            "for batch9119 loss is 0.003182394430041313\n",
            "for batch9120 loss is 0.0014292665291577578\n",
            "for batch9121 loss is 0.058781109750270844\n",
            "for batch9122 loss is 0.15692657232284546\n",
            "for batch9123 loss is 0.021131359040737152\n",
            "for batch9124 loss is 0.020936662331223488\n",
            "for batch9125 loss is 0.9649534225463867\n",
            "for batch9126 loss is 5.745805083279265e-06\n",
            "for batch9127 loss is 0.009685421362519264\n",
            "for batch9128 loss is 0.0011239328887313604\n",
            "for batch9129 loss is 0.4956185221672058\n",
            "for batch9130 loss is 4.772592365043238e-05\n",
            "for batch9131 loss is 0.00013070603017695248\n",
            "for batch9132 loss is 0.12626565992832184\n",
            "for batch9133 loss is 0.0034316678065806627\n",
            "for batch9134 loss is 0.11948095262050629\n",
            "for batch9135 loss is 0.0004347385838627815\n",
            "for batch9136 loss is 0.06829355657100677\n",
            "for batch9137 loss is 0.05192926526069641\n",
            "for batch9138 loss is 0.08782785385847092\n",
            "for batch9139 loss is 0.09873246401548386\n",
            "for batch9140 loss is 0.30337920784950256\n",
            "for batch9141 loss is 0.06453277170658112\n",
            "for batch9142 loss is 0.0006625710520893335\n",
            "for batch9143 loss is 0.00018514672410674393\n",
            "for batch9144 loss is 0.29955825209617615\n",
            "for batch9145 loss is 0.00015927842468954623\n",
            "for batch9146 loss is 0.5237318873405457\n",
            "for batch9147 loss is 1.235923409461975\n",
            "for batch9148 loss is 6.508073420263827e-05\n",
            "for batch9149 loss is 7.875968731241301e-05\n",
            "for batch9150 loss is 0.0002289514522999525\n",
            "for batch9151 loss is 0.9219328165054321\n",
            "for batch9152 loss is 0.1267402470111847\n",
            "for batch9153 loss is 0.011114787310361862\n",
            "for batch9154 loss is 0.0023171540815383196\n",
            "for batch9155 loss is 0.02219207212328911\n",
            "for batch9156 loss is 0.18267804384231567\n",
            "for batch9157 loss is 0.01848183386027813\n",
            "for batch9158 loss is 9.768046584213153e-05\n",
            "for batch9159 loss is 0.033357392996549606\n",
            "for batch9160 loss is 0.8747116923332214\n",
            "for batch9161 loss is 0.010756455361843109\n",
            "for batch9162 loss is 0.16048873960971832\n",
            "for batch9163 loss is 0.004077856428921223\n",
            "for batch9164 loss is 0.3231566548347473\n",
            "for batch9165 loss is 0.16983938217163086\n",
            "for batch9166 loss is 0.3366873562335968\n",
            "for batch9167 loss is 0.0558442659676075\n",
            "for batch9168 loss is 0.457355797290802\n",
            "for batch9169 loss is 0.019771113991737366\n",
            "for batch9170 loss is 0.03571105748414993\n",
            "for batch9171 loss is 0.018191296607255936\n",
            "for batch9172 loss is 0.021975353360176086\n",
            "for batch9173 loss is 0.18529300391674042\n",
            "for batch9174 loss is 0.004414747469127178\n",
            "for batch9175 loss is 0.010534392669796944\n",
            "for batch9176 loss is 0.00017756721354089677\n",
            "for batch9177 loss is 0.3162611722946167\n",
            "for batch9178 loss is 0.00926954485476017\n",
            "for batch9179 loss is 0.0014075430808588862\n",
            "for batch9180 loss is 0.28340238332748413\n",
            "for batch9181 loss is 0.2231900990009308\n",
            "for batch9182 loss is 0.43091678619384766\n",
            "for batch9183 loss is 0.06999625265598297\n",
            "for batch9184 loss is 0.3561631143093109\n",
            "for batch9185 loss is 0.22390775382518768\n",
            "for batch9186 loss is 0.1478106677532196\n",
            "for batch9187 loss is 0.0018193969735875726\n",
            "for batch9188 loss is 0.2588670253753662\n",
            "for batch9189 loss is 0.05809750407934189\n",
            "for batch9190 loss is 0.006470915861427784\n",
            "for batch9191 loss is 0.44625744223594666\n",
            "for batch9192 loss is 0.024452945217490196\n",
            "for batch9193 loss is 0.6833174824714661\n",
            "for batch9194 loss is 0.015254972502589226\n",
            "for batch9195 loss is 0.0123716089874506\n",
            "for batch9196 loss is 0.010478610172867775\n",
            "for batch9197 loss is 0.11848245561122894\n",
            "for batch9198 loss is 0.31860530376434326\n",
            "for batch9199 loss is 0.008832643739879131\n",
            "for batch9200 loss is 0.2004593312740326\n",
            "for batch9201 loss is 0.23866987228393555\n",
            "for batch9202 loss is 0.16766922175884247\n",
            "for batch9203 loss is 0.11128637939691544\n",
            "for batch9204 loss is 0.012129233218729496\n",
            "for batch9205 loss is 0.00012150082329753786\n",
            "for batch9206 loss is 0.15067829191684723\n",
            "for batch9207 loss is 0.014455372467637062\n",
            "for batch9208 loss is 0.02410496026277542\n",
            "for batch9209 loss is 0.0051942733116447926\n",
            "for batch9210 loss is 0.09172402322292328\n",
            "for batch9211 loss is 0.24609556794166565\n",
            "for batch9212 loss is 0.019967176020145416\n",
            "for batch9213 loss is 0.027302522212266922\n",
            "for batch9214 loss is 5.2451468945946544e-06\n",
            "for batch9215 loss is 0.006551699247211218\n",
            "for batch9216 loss is 0.1503952145576477\n",
            "for batch9217 loss is 0.2745746672153473\n",
            "for batch9218 loss is 0.31213122606277466\n",
            "for batch9219 loss is 8.9245819253847e-05\n",
            "for batch9220 loss is 0.5329548716545105\n",
            "for batch9221 loss is 0.03259003907442093\n",
            "for batch9222 loss is 0.014237563125789165\n",
            "for batch9223 loss is 0.014022399671375751\n",
            "for batch9224 loss is 0.13828900456428528\n",
            "for batch9225 loss is 0.011032520793378353\n",
            "for batch9226 loss is 0.006967560853809118\n",
            "for batch9227 loss is 0.051764000207185745\n",
            "for batch9228 loss is 0.06766165792942047\n",
            "for batch9229 loss is 0.7184220552444458\n",
            "for batch9230 loss is 0.13633042573928833\n",
            "for batch9231 loss is 0.018217789009213448\n",
            "for batch9232 loss is 0.13068854808807373\n",
            "for batch9233 loss is 0.24940896034240723\n",
            "for batch9234 loss is 0.00395117187872529\n",
            "for batch9235 loss is 0.0211506150662899\n",
            "for batch9236 loss is 0.19540569186210632\n",
            "for batch9237 loss is 0.16531726717948914\n",
            "for batch9238 loss is 0.009084406308829784\n",
            "for batch9239 loss is 0.0002533013466745615\n",
            "for batch9240 loss is 0.1237027645111084\n",
            "for batch9241 loss is 0.012929988093674183\n",
            "for batch9242 loss is 0.5022518038749695\n",
            "for batch9243 loss is 0.07085756957530975\n",
            "for batch9244 loss is 0.02299155294895172\n",
            "for batch9245 loss is 0.0007619787356816232\n",
            "for batch9246 loss is 0.004290428478270769\n",
            "for batch9247 loss is 0.0023662082385271788\n",
            "for batch9248 loss is 0.14273926615715027\n",
            "for batch9249 loss is 0.17316725850105286\n",
            "for batch9250 loss is 0.101385198533535\n",
            "for batch9251 loss is 0.03215033560991287\n",
            "for batch9252 loss is 1.2599852085113525\n",
            "for batch9253 loss is 0.0983525961637497\n",
            "for batch9254 loss is 1.4376373655977659e-05\n",
            "for batch9255 loss is 0.00045307778054848313\n",
            "for batch9256 loss is 0.009400892071425915\n",
            "for batch9257 loss is 0.004312475211918354\n",
            "for batch9258 loss is 7.152556946721234e-08\n",
            "for batch9259 loss is 0.0008754757000133395\n",
            "for batch9260 loss is 0.2965297996997833\n",
            "for batch9261 loss is 0.00014740755432285368\n",
            "for batch9262 loss is 0.6062349081039429\n",
            "for batch9263 loss is 0.33363181352615356\n",
            "for batch9264 loss is 0.13772358000278473\n",
            "for batch9265 loss is 7.36702077119844e-06\n",
            "for batch9266 loss is 8.320645065396093e-06\n",
            "for batch9267 loss is 0.008473126217722893\n",
            "for batch9268 loss is 0.1492900848388672\n",
            "for batch9269 loss is 0.03412842005491257\n",
            "for batch9270 loss is 0.9879283905029297\n",
            "for batch9271 loss is 0.37362030148506165\n",
            "for batch9272 loss is 6.675715553683403e-07\n",
            "for batch9273 loss is 0.06826788932085037\n",
            "for batch9274 loss is 0.00534720066934824\n",
            "for batch9275 loss is 0.2459225356578827\n",
            "for batch9276 loss is 0.08001432567834854\n",
            "for batch9277 loss is 0.0003473712713457644\n",
            "for batch9278 loss is 0.011418410576879978\n",
            "for batch9279 loss is 0.019887421280145645\n",
            "for batch9280 loss is 0.819790244102478\n",
            "for batch9281 loss is 0.000377001182641834\n",
            "for batch9282 loss is 1.8271621465682983\n",
            "for batch9283 loss is 0.07459606230258942\n",
            "for batch9284 loss is 0.0025684162974357605\n",
            "for batch9285 loss is 0.07842811197042465\n",
            "for batch9286 loss is 0.007665715180337429\n",
            "for batch9287 loss is 0.010720841586589813\n",
            "for batch9288 loss is 0.05950526148080826\n",
            "for batch9289 loss is 1.2920506000518799\n",
            "for batch9290 loss is 0.2995341420173645\n",
            "for batch9291 loss is 0.0009434829698875546\n",
            "for batch9292 loss is 0.0006820049020461738\n",
            "for batch9293 loss is 0.8484430313110352\n",
            "for batch9294 loss is 0.10248816013336182\n",
            "for batch9295 loss is 0.13083317875862122\n",
            "for batch9296 loss is 0.01792939379811287\n",
            "for batch9297 loss is 0.0033119923900812864\n",
            "for batch9298 loss is 0.19283004105091095\n",
            "for batch9299 loss is 1.7544934749603271\n",
            "for batch9300 loss is 0.25458139181137085\n",
            "for batch9301 loss is 0.0027049430646002293\n",
            "for batch9302 loss is 0.031098026782274246\n",
            "for batch9303 loss is 0.4553360044956207\n",
            "for batch9304 loss is 0.0001924781536217779\n",
            "for batch9305 loss is 0.0005222941981628537\n",
            "for batch9306 loss is 0.015026425942778587\n",
            "for batch9307 loss is 0.005163532681763172\n",
            "for batch9308 loss is 0.026149174198508263\n",
            "for batch9309 loss is 0.27699583768844604\n",
            "for batch9310 loss is 0.5390236377716064\n",
            "for batch9311 loss is 4.363012521935161e-06\n",
            "for batch9312 loss is 0.019294844940304756\n",
            "for batch9313 loss is 2.145766586636455e-07\n",
            "for batch9314 loss is 0.006381249986588955\n",
            "for batch9315 loss is 0.21907129883766174\n",
            "for batch9316 loss is 0.39652448892593384\n",
            "for batch9317 loss is 0.7987450957298279\n",
            "for batch9318 loss is 0.016059469431638718\n",
            "for batch9319 loss is 0.03982147201895714\n",
            "for batch9320 loss is 0.008472769521176815\n",
            "for batch9321 loss is 0.026350876316428185\n",
            "for batch9322 loss is 0.0382048562169075\n",
            "for batch9323 loss is 0.10168182849884033\n",
            "for batch9324 loss is 0.05352083966135979\n",
            "for batch9325 loss is 0.025041883811354637\n",
            "for batch9326 loss is 0.11840403079986572\n",
            "for batch9327 loss is 0.10236525535583496\n",
            "for batch9328 loss is 0.06484513729810715\n",
            "for batch9329 loss is 0.005379919894039631\n",
            "for batch9330 loss is 0.00016017045709304512\n",
            "for batch9331 loss is 0.06855211406946182\n",
            "for batch9332 loss is 0.06832176446914673\n",
            "for batch9333 loss is 0.03572612255811691\n",
            "for batch9334 loss is 0.004970368929207325\n",
            "for batch9335 loss is 0.0048153395764529705\n",
            "for batch9336 loss is 0.6510458588600159\n",
            "for batch9337 loss is 0.13955259323120117\n",
            "for batch9338 loss is 0.01007806695997715\n",
            "for batch9339 loss is 0.05338263511657715\n",
            "for batch9340 loss is 0.024760153144598007\n",
            "for batch9341 loss is 0.08292748779058456\n",
            "for batch9342 loss is 0.0004314008983783424\n",
            "for batch9343 loss is 2.1457667287450022e-07\n",
            "for batch9344 loss is 0.23729035258293152\n",
            "for batch9345 loss is 0.011213773861527443\n",
            "for batch9346 loss is 0.0007067587575875223\n",
            "for batch9347 loss is 1.500289797782898\n",
            "for batch9348 loss is 0.4901011884212494\n",
            "for batch9349 loss is 0.0015356187941506505\n",
            "for batch9350 loss is 0.33591121435165405\n",
            "for batch9351 loss is 1.642636561882682e-05\n",
            "for batch9352 loss is 8.448661537840962e-05\n",
            "for batch9353 loss is 1.0919305168499704e-05\n",
            "for batch9354 loss is 0.0011644758051261306\n",
            "for batch9355 loss is 0.0004095030599273741\n",
            "for batch9356 loss is 0.48430460691452026\n",
            "for batch9357 loss is 0.0034046382643282413\n",
            "for batch9358 loss is 0.10299545526504517\n",
            "for batch9359 loss is 2.4556993594160303e-06\n",
            "for batch9360 loss is 0.14503295719623566\n",
            "for batch9361 loss is 0.021024202927947044\n",
            "for batch9362 loss is 0.03208913654088974\n",
            "for batch9363 loss is 1.5170420408248901\n",
            "for batch9364 loss is 0.5835325121879578\n",
            "for batch9365 loss is 1.1920925402364446e-07\n",
            "for batch9366 loss is 0.25228747725486755\n",
            "for batch9367 loss is 0.0032372972927987576\n",
            "for batch9368 loss is 0.04853702709078789\n",
            "for batch9369 loss is 0.16267408430576324\n",
            "for batch9370 loss is 0.01225649006664753\n",
            "for batch9371 loss is 3.3661897759884596e-05\n",
            "for batch9372 loss is 0.023455137386918068\n",
            "for batch9373 loss is 0.42133936285972595\n",
            "for batch9374 loss is 0.06782639026641846\n",
            "for batch9375 loss is 0.08730009943246841\n",
            "for batch9376 loss is 0.006182165816426277\n",
            "for batch9377 loss is 0.7318064570426941\n",
            "for batch9378 loss is 0.24707086384296417\n",
            "for batch9379 loss is 0.0001443341898266226\n",
            "for batch9380 loss is 0.568392813205719\n",
            "for batch9381 loss is 0.38878554105758667\n",
            "for batch9382 loss is 0.005149192176759243\n",
            "for batch9383 loss is 0.5998567342758179\n",
            "for batch9384 loss is 2.384185648907078e-08\n",
            "for batch9385 loss is 0.2177891731262207\n",
            "for batch9386 loss is 0.16176018118858337\n",
            "for batch9387 loss is 0.39091557264328003\n",
            "for batch9388 loss is 0.007017822004854679\n",
            "for batch9389 loss is 0.3228650391101837\n",
            "for batch9390 loss is 0.005715919658541679\n",
            "for batch9391 loss is 0.046822138130664825\n",
            "for batch9392 loss is 0.3581711947917938\n",
            "for batch9393 loss is 0.06985972821712494\n",
            "for batch9394 loss is 0.8144299387931824\n",
            "for batch9395 loss is 0.1927023082971573\n",
            "for batch9396 loss is 0.011005816981196404\n",
            "for batch9397 loss is 0.1105918139219284\n",
            "for batch9398 loss is 0.00024024695449043065\n",
            "for batch9399 loss is 0.04283666983246803\n",
            "for batch9400 loss is 0.03077322617173195\n",
            "for batch9401 loss is 0.06115736439824104\n",
            "for batch9402 loss is 0.12341301143169403\n",
            "for batch9403 loss is 4.6512177505064756e-05\n",
            "for batch9404 loss is 0.05116262286901474\n",
            "for batch9405 loss is 0.039377711713314056\n",
            "for batch9406 loss is 0.03790808841586113\n",
            "for batch9407 loss is 0.0022093416191637516\n",
            "for batch9408 loss is 0.007981446571648121\n",
            "for batch9409 loss is 0.0557856485247612\n",
            "for batch9410 loss is 0.018841348588466644\n",
            "for batch9411 loss is 0.2405283898115158\n",
            "for batch9412 loss is 0.07629664242267609\n",
            "for batch9413 loss is 0.009215956553816795\n",
            "for batch9414 loss is 0.00028148372075520456\n",
            "for batch9415 loss is 0.09632015973329544\n",
            "for batch9416 loss is 0.06733997166156769\n",
            "for batch9417 loss is 0.08176927268505096\n",
            "for batch9418 loss is 0.0005093531217426062\n",
            "for batch9419 loss is 0.9606002569198608\n",
            "for batch9420 loss is 0.057185135781764984\n",
            "for batch9421 loss is 0.3639538586139679\n",
            "for batch9422 loss is 0.0003537196316756308\n",
            "for batch9423 loss is 0.21273525059223175\n",
            "for batch9424 loss is 0.021739494055509567\n",
            "for batch9425 loss is 0.07072149962186813\n",
            "for batch9426 loss is 0.011944035068154335\n",
            "for batch9427 loss is 0.2892424464225769\n",
            "for batch9428 loss is 0.09279076755046844\n",
            "for batch9429 loss is 0.009139643982052803\n",
            "for batch9430 loss is 0.6820134520530701\n",
            "for batch9431 loss is 0.005448757205158472\n",
            "for batch9432 loss is 0.09200077503919601\n",
            "for batch9433 loss is 0.04135320335626602\n",
            "for batch9434 loss is 1.2148425579071045\n",
            "for batch9435 loss is 0.04227259010076523\n",
            "for batch9436 loss is 0.025620754808187485\n",
            "for batch9437 loss is 0.6629024744033813\n",
            "for batch9438 loss is 0.014894386753439903\n",
            "for batch9439 loss is 0.30966314673423767\n",
            "for batch9440 loss is 0.007829675450921059\n",
            "for batch9441 loss is 0.4221464991569519\n",
            "for batch9442 loss is 0.6125480532646179\n",
            "for batch9443 loss is 0.017927201464772224\n",
            "for batch9444 loss is 0.2617925703525543\n",
            "for batch9445 loss is 0.0002774492895696312\n",
            "for batch9446 loss is 0.010846585966646671\n",
            "for batch9447 loss is 0.13330939412117004\n",
            "for batch9448 loss is 0.6587151288986206\n",
            "for batch9449 loss is 0.01498387474566698\n",
            "for batch9450 loss is 0.5066231489181519\n",
            "for batch9451 loss is 0.2820848822593689\n",
            "for batch9452 loss is 0.004460584372282028\n",
            "for batch9453 loss is 0.007022758014500141\n",
            "for batch9454 loss is 0.10773493349552155\n",
            "for batch9455 loss is 0.0022091951686888933\n",
            "for batch9456 loss is 0.022968631237745285\n",
            "for batch9457 loss is 0.06785278022289276\n",
            "for batch9458 loss is 0.04052751138806343\n",
            "for batch9459 loss is 0.03866582363843918\n",
            "for batch9460 loss is 0.5654944777488708\n",
            "for batch9461 loss is 1.4593055248260498\n",
            "for batch9462 loss is 0.22320273518562317\n",
            "for batch9463 loss is 0.001942139700986445\n",
            "for batch9464 loss is 0.4204476773738861\n",
            "for batch9465 loss is 1.1920588804059662e-05\n",
            "for batch9466 loss is 0.5480234026908875\n",
            "for batch9467 loss is 0.2413565218448639\n",
            "for batch9468 loss is 0.03527636453509331\n",
            "for batch9469 loss is 0.15538588166236877\n",
            "for batch9470 loss is 0.04078671336174011\n",
            "for batch9471 loss is 2.5127741537289694e-05\n",
            "for batch9472 loss is 0.129107266664505\n",
            "for batch9473 loss is 0.0653088390827179\n",
            "for batch9474 loss is 0.055529914796352386\n",
            "for batch9475 loss is 1.0524728298187256\n",
            "for batch9476 loss is 0.05212331563234329\n",
            "for batch9477 loss is 0.07043495029211044\n",
            "for batch9478 loss is 2.88473329419503e-05\n",
            "for batch9479 loss is 0.0641690120100975\n",
            "for batch9480 loss is 0.061428964138031006\n",
            "for batch9481 loss is 0.08911095559597015\n",
            "for batch9482 loss is 0.05480996519327164\n",
            "for batch9483 loss is 0.01898592710494995\n",
            "for batch9484 loss is 0.4588685631752014\n",
            "for batch9485 loss is 0.38217779994010925\n",
            "for batch9486 loss is 0.019323289394378662\n",
            "for batch9487 loss is 0.23978212475776672\n",
            "for batch9488 loss is 0.3032163083553314\n",
            "for batch9489 loss is 0.0015054758405312896\n",
            "for batch9490 loss is 0.04631427675485611\n",
            "for batch9491 loss is 0.07247411459684372\n",
            "for batch9492 loss is 1.2874564845333225e-06\n",
            "for batch9493 loss is 0.030742188915610313\n",
            "for batch9494 loss is 0.000515611027367413\n",
            "for batch9495 loss is 0.0\n",
            "for batch9496 loss is 7.39096833513031e-07\n",
            "for batch9497 loss is 0.00960239116102457\n",
            "for batch9498 loss is 5.885697828489356e-05\n",
            "for batch9499 loss is 0.025350486859679222\n",
            "for batch9500 loss is 0.04313898831605911\n",
            "for batch9501 loss is 0.0025127711705863476\n",
            "for batch9502 loss is 0.00027476795366965234\n",
            "for batch9503 loss is 0.16768614947795868\n",
            "for batch9504 loss is 0.000573317869566381\n",
            "for batch9505 loss is 0.05484061315655708\n",
            "for batch9506 loss is 0.0028492030687630177\n",
            "for batch9507 loss is 0.11585769802331924\n",
            "for batch9508 loss is 0.00015487853670492768\n",
            "for batch9509 loss is 0.004685850348323584\n",
            "for batch9510 loss is 0.02087629958987236\n",
            "for batch9511 loss is 0.18592488765716553\n",
            "for batch9512 loss is 0.005358857102692127\n",
            "for batch9513 loss is 0.2124975174665451\n",
            "for batch9514 loss is 0.3832724690437317\n",
            "for batch9515 loss is 0.1711646020412445\n",
            "for batch9516 loss is 0.04456210508942604\n",
            "for batch9517 loss is 0.013700952753424644\n",
            "for batch9518 loss is 0.0018471607472747564\n",
            "for batch9519 loss is 0.045144982635974884\n",
            "for batch9520 loss is 0.2125367820262909\n",
            "for batch9521 loss is 1.0649019479751587\n",
            "for batch9522 loss is 1.1643353700637817\n",
            "for batch9523 loss is 0.00015079241711646318\n",
            "for batch9524 loss is 0.013497011736035347\n",
            "for batch9525 loss is 0.002752965083345771\n",
            "for batch9526 loss is 0.0370870865881443\n",
            "for batch9527 loss is 0.0068290517665445805\n",
            "for batch9528 loss is 0.00043889967491850257\n",
            "for batch9529 loss is 0.23184625804424286\n",
            "for batch9530 loss is 0.21237942576408386\n",
            "for batch9531 loss is 0.3111949563026428\n",
            "for batch9532 loss is 0.5922683477401733\n",
            "for batch9533 loss is 0.030868638306856155\n",
            "for batch9534 loss is 5.245202032710949e-07\n",
            "for batch9535 loss is 0.07324610650539398\n",
            "for batch9536 loss is 0.0051872869953513145\n",
            "for batch9537 loss is 0.0006735482020303607\n",
            "for batch9538 loss is 0.0002589934738352895\n",
            "for batch9539 loss is 0.0010993535397574306\n",
            "for batch9540 loss is 0.5315898656845093\n",
            "for batch9541 loss is 0.3700207769870758\n",
            "for batch9542 loss is 0.23685142397880554\n",
            "for batch9543 loss is 0.00539521686732769\n",
            "for batch9544 loss is 0.19895383715629578\n",
            "for batch9545 loss is 0.09448017179965973\n",
            "for batch9546 loss is 0.04927844554185867\n",
            "for batch9547 loss is 0.03461502864956856\n",
            "for batch9548 loss is 0.0388890765607357\n",
            "for batch9549 loss is 0.12351590394973755\n",
            "for batch9550 loss is 0.05991092324256897\n",
            "for batch9551 loss is 0.002885848516598344\n",
            "for batch9552 loss is 0.001663874601945281\n",
            "for batch9553 loss is 0.05452590063214302\n",
            "for batch9554 loss is 0.0017687153303995728\n",
            "for batch9555 loss is 0.3195228576660156\n",
            "for batch9556 loss is 0.025546187534928322\n",
            "for batch9557 loss is 0.07222656905651093\n",
            "for batch9558 loss is 8.384004468098283e-05\n",
            "for batch9559 loss is 0.2038315236568451\n",
            "for batch9560 loss is 0.1258394420146942\n",
            "for batch9561 loss is 0.0700240284204483\n",
            "for batch9562 loss is 0.16369837522506714\n",
            "for batch9563 loss is 1.1352757215499878\n",
            "for batch9564 loss is 0.3139719069004059\n",
            "for batch9565 loss is 0.006872999016195536\n",
            "for batch9566 loss is 0.0018397606909275055\n",
            "for batch9567 loss is 0.04472925886511803\n",
            "for batch9568 loss is 0.036583565175533295\n",
            "for batch9569 loss is 0.4291571080684662\n",
            "for batch9570 loss is 0.004058370366692543\n",
            "for batch9571 loss is 0.39092618227005005\n",
            "for batch9572 loss is 0.0005265213549137115\n",
            "for batch9573 loss is 0.17875242233276367\n",
            "for batch9574 loss is 0.5714954137802124\n",
            "for batch9575 loss is 0.0032140444964170456\n",
            "for batch9576 loss is 0.12376506626605988\n",
            "for batch9577 loss is 0.005695736035704613\n",
            "for batch9578 loss is 0.29036006331443787\n",
            "for batch9579 loss is 0.0001042926887748763\n",
            "for batch9580 loss is 0.0024749194271862507\n",
            "for batch9581 loss is 0.008066698908805847\n",
            "for batch9582 loss is 0.00426266435533762\n",
            "for batch9583 loss is 0.2657163739204407\n",
            "for batch9584 loss is 0.003006108570843935\n",
            "for batch9585 loss is 3.445003676461056e-05\n",
            "for batch9586 loss is 0.00018360612739343196\n",
            "for batch9587 loss is 0.2077561914920807\n",
            "for batch9588 loss is 0.021802380681037903\n",
            "for batch9589 loss is 1.1590542793273926\n",
            "for batch9590 loss is 0.27256157994270325\n",
            "for batch9591 loss is 0.3495780825614929\n",
            "for batch9592 loss is 0.14870336651802063\n",
            "for batch9593 loss is 0.041713912039995193\n",
            "for batch9594 loss is 0.02289513684809208\n",
            "for batch9595 loss is 0.00010001550981542096\n",
            "for batch9596 loss is 1.2615599632263184\n",
            "for batch9597 loss is 0.47949567437171936\n",
            "for batch9598 loss is 0.03201543167233467\n",
            "for batch9599 loss is 0.6001427173614502\n",
            "for batch9600 loss is 0.001700447523035109\n",
            "for batch9601 loss is 0.3153358995914459\n",
            "for batch9602 loss is 9.369738108944148e-06\n",
            "for batch9603 loss is 0.0745285376906395\n",
            "for batch9604 loss is 5.812104791402817e-05\n",
            "for batch9605 loss is 0.05143700912594795\n",
            "for batch9606 loss is 0.17334067821502686\n",
            "for batch9607 loss is 0.004215585999190807\n",
            "for batch9608 loss is 0.14258801937103271\n",
            "for batch9609 loss is 0.017452297732234\n",
            "for batch9610 loss is 0.0008140212739817798\n",
            "for batch9611 loss is 0.48196712136268616\n",
            "for batch9612 loss is 0.04311322793364525\n",
            "for batch9613 loss is 0.0014173383824527264\n",
            "for batch9614 loss is 0.07267964631319046\n",
            "for batch9615 loss is 0.4497033953666687\n",
            "for batch9616 loss is 0.00042595923878252506\n",
            "for batch9617 loss is 2.357642889022827\n",
            "for batch9618 loss is 0.013483445160090923\n",
            "for batch9619 loss is 0.22393269836902618\n",
            "for batch9620 loss is 0.002132291905581951\n",
            "for batch9621 loss is 0.76614910364151\n",
            "for batch9622 loss is 0.12679846584796906\n",
            "for batch9623 loss is 0.036483801901340485\n",
            "for batch9624 loss is 0.581024169921875\n",
            "for batch9625 loss is 0.22772260010242462\n",
            "for batch9626 loss is 0.4299612045288086\n",
            "for batch9627 loss is 1.8025925159454346\n",
            "for batch9628 loss is 0.030568579211831093\n",
            "for batch9629 loss is 0.10761667788028717\n",
            "for batch9630 loss is 0.21837027370929718\n",
            "for batch9631 loss is 0.018959026783704758\n",
            "for batch9632 loss is 1.4303138256072998\n",
            "for batch9633 loss is 0.0768301859498024\n",
            "for batch9634 loss is 0.06938014924526215\n",
            "for batch9635 loss is 0.33880797028541565\n",
            "for batch9636 loss is 0.0037675879430025816\n",
            "for batch9637 loss is 0.010879961773753166\n",
            "for batch9638 loss is 0.001456720638088882\n",
            "for batch9639 loss is 0.02257470414042473\n",
            "for batch9640 loss is 0.17035171389579773\n",
            "for batch9641 loss is 0.6703211069107056\n",
            "for batch9642 loss is 0.044662732630968094\n",
            "for batch9643 loss is 0.00016702557331882417\n",
            "for batch9644 loss is 0.07940761744976044\n",
            "for batch9645 loss is 0.015048591420054436\n",
            "for batch9646 loss is 0.017620407044887543\n",
            "for batch9647 loss is 1.0967235084535787e-06\n",
            "for batch9648 loss is 0.5239462852478027\n",
            "for batch9649 loss is 0.0008160982397384942\n",
            "for batch9650 loss is 0.0002741133503150195\n",
            "for batch9651 loss is 0.00853167288005352\n",
            "for batch9652 loss is 0.09394638985395432\n",
            "for batch9653 loss is 0.1763509064912796\n",
            "for batch9654 loss is 0.00040202276431955397\n",
            "for batch9655 loss is 0.09054926782846451\n",
            "for batch9656 loss is 0.0050814514979720116\n",
            "for batch9657 loss is 0.018869686871767044\n",
            "for batch9658 loss is 0.011966907419264317\n",
            "for batch9659 loss is 0.025192826986312866\n",
            "for batch9660 loss is 0.00014185642066877335\n",
            "for batch9661 loss is 0.00039920624112710357\n",
            "for batch9662 loss is 0.006568876560777426\n",
            "for batch9663 loss is 0.022815311327576637\n",
            "for batch9664 loss is 0.03203331679105759\n",
            "for batch9665 loss is 0.05549393966794014\n",
            "for batch9666 loss is 0.04656774923205376\n",
            "for batch9667 loss is 0.0012709615984931588\n",
            "for batch9668 loss is 3.6786172131542116e-05\n",
            "for batch9669 loss is 1.8689180612564087\n",
            "for batch9670 loss is 0.00483357347548008\n",
            "for batch9671 loss is 0.06593310832977295\n",
            "for batch9672 loss is 0.01217477023601532\n",
            "for batch9673 loss is 0.041606757789850235\n",
            "for batch9674 loss is 0.0030021488200873137\n",
            "for batch9675 loss is 0.00018855063535738736\n",
            "for batch9676 loss is 0.16860419511795044\n",
            "for batch9677 loss is 0.019299985840916634\n",
            "for batch9678 loss is 0.6519384384155273\n",
            "for batch9679 loss is 0.04079237952828407\n",
            "for batch9680 loss is 0.11283731460571289\n",
            "for batch9681 loss is 0.15491876006126404\n",
            "for batch9682 loss is 0.015994258224964142\n",
            "for batch9683 loss is 0.0008215447887778282\n",
            "for batch9684 loss is 0.034560758620500565\n",
            "for batch9685 loss is 0.5972816944122314\n",
            "for batch9686 loss is 1.1036972999572754\n",
            "for batch9687 loss is 0.03344700485467911\n",
            "for batch9688 loss is 0.30636903643608093\n",
            "for batch9689 loss is 0.0011379949282854795\n",
            "for batch9690 loss is 0.0020889523439109325\n",
            "for batch9691 loss is 0.005730665288865566\n",
            "for batch9692 loss is 0.23200583457946777\n",
            "for batch9693 loss is 0.0243516955524683\n",
            "for batch9694 loss is 2.739273440965917e-05\n",
            "for batch9695 loss is 4.768371297814156e-08\n",
            "for batch9696 loss is 2.2580857276916504\n",
            "for batch9697 loss is 0.38107404112815857\n",
            "for batch9698 loss is 2.5866818759823218e-05\n",
            "for batch9699 loss is 0.0011818900238722563\n",
            "for batch9700 loss is 0.03481439873576164\n",
            "for batch9701 loss is 0.00150216871406883\n",
            "for batch9702 loss is 0.0027876305393874645\n",
            "for batch9703 loss is 0.0023537229280918837\n",
            "for batch9704 loss is 0.0\n",
            "for batch9705 loss is 0.0010845885844901204\n",
            "for batch9706 loss is 0.4445871412754059\n",
            "for batch9707 loss is 0.021440623328089714\n",
            "for batch9708 loss is 0.27365484833717346\n",
            "for batch9709 loss is 0.2862958014011383\n",
            "for batch9710 loss is 0.35583940148353577\n",
            "for batch9711 loss is 0.16191762685775757\n",
            "for batch9712 loss is 0.3096836507320404\n",
            "for batch9713 loss is 0.10503502190113068\n",
            "for batch9714 loss is 0.23109789192676544\n",
            "for batch9715 loss is 0.5984403491020203\n",
            "for batch9716 loss is 0.10381586849689484\n",
            "for batch9717 loss is 0.0141907362267375\n",
            "for batch9718 loss is 1.0028094053268433\n",
            "for batch9719 loss is 0.5215989351272583\n",
            "for batch9720 loss is 0.07106177508831024\n",
            "for batch9721 loss is 0.3057859539985657\n",
            "for batch9722 loss is 0.003993268124759197\n",
            "for batch9723 loss is 0.5150540471076965\n",
            "for batch9724 loss is 0.010012282058596611\n",
            "for batch9725 loss is 0.052076809108257294\n",
            "for batch9726 loss is 0.09314705431461334\n",
            "for batch9727 loss is 0.8164612650871277\n",
            "for batch9728 loss is 0.00363432546146214\n",
            "for batch9729 loss is 1.9192029867554083e-05\n",
            "for batch9730 loss is 0.1069754809141159\n",
            "for batch9731 loss is 0.7139846086502075\n",
            "for batch9732 loss is 0.0009578261524438858\n",
            "for batch9733 loss is 1.3874428272247314\n",
            "for batch9734 loss is 0.035107169300317764\n",
            "for batch9735 loss is 0.0005763553781434894\n",
            "for batch9736 loss is 0.00024315182236023247\n",
            "for batch9737 loss is 0.008301420137286186\n",
            "for batch9738 loss is 0.032644402235746384\n",
            "for batch9739 loss is 0.44972196221351624\n",
            "for batch9740 loss is 0.768208384513855\n",
            "for batch9741 loss is 0.63446044921875\n",
            "for batch9742 loss is 0.025249233469367027\n",
            "for batch9743 loss is 0.004183210898190737\n",
            "for batch9744 loss is 3.001504228450358e-05\n",
            "for batch9745 loss is 0.3393236994743347\n",
            "for batch9746 loss is 0.038958560675382614\n",
            "for batch9747 loss is 0.0451158806681633\n",
            "for batch9748 loss is 0.0008209124207496643\n",
            "for batch9749 loss is 0.11608383804559708\n",
            "for batch9750 loss is 0.9163721203804016\n",
            "for batch9751 loss is 0.0030543108005076647\n",
            "for batch9752 loss is 0.46215254068374634\n",
            "for batch9753 loss is 0.5266944766044617\n",
            "for batch9754 loss is 0.5576383471488953\n",
            "for batch9755 loss is 0.4471069872379303\n",
            "for batch9756 loss is 0.034572381526231766\n",
            "for batch9757 loss is 0.09014774858951569\n",
            "for batch9758 loss is 0.12914523482322693\n",
            "for batch9759 loss is 0.0027159727178514004\n",
            "for batch9760 loss is 0.24357834458351135\n",
            "for batch9761 loss is 0.10578672587871552\n",
            "for batch9762 loss is 1.5049254894256592\n",
            "for batch9763 loss is 0.04130055382847786\n",
            "for batch9764 loss is 0.25714999437332153\n",
            "for batch9765 loss is 0.003691165940836072\n",
            "for batch9766 loss is 0.007267121225595474\n",
            "for batch9767 loss is 1.410980463027954\n",
            "for batch9768 loss is 0.002323852851986885\n",
            "for batch9769 loss is 0.003708569798618555\n",
            "for batch9770 loss is 0.03901357203722\n",
            "for batch9771 loss is 0.030768748372793198\n",
            "for batch9772 loss is 0.033229243010282516\n",
            "for batch9773 loss is 0.04229302331805229\n",
            "for batch9774 loss is 0.0016587553545832634\n",
            "for batch9775 loss is 0.13494440913200378\n",
            "for batch9776 loss is 0.38963261246681213\n",
            "for batch9777 loss is 0.0015864407178014517\n",
            "for batch9778 loss is 0.02620621956884861\n",
            "for batch9779 loss is 0.0001980696979444474\n",
            "for batch9780 loss is 0.003247716696932912\n",
            "for batch9781 loss is 0.0004064732347615063\n",
            "for batch9782 loss is 0.001597112976014614\n",
            "for batch9783 loss is 0.025569995865225792\n",
            "for batch9784 loss is 0.7327985167503357\n",
            "for batch9785 loss is 0.032785795629024506\n",
            "for batch9786 loss is 0.040247298777103424\n",
            "for batch9787 loss is 0.3949282765388489\n",
            "for batch9788 loss is 0.0059761768206954\n",
            "for batch9789 loss is 0.020593082532286644\n",
            "for batch9790 loss is 0.6993073225021362\n",
            "for batch9791 loss is 0.385489284992218\n",
            "for batch9792 loss is 0.0012226381804794073\n",
            "for batch9793 loss is 6.751593900844455e-05\n",
            "for batch9794 loss is 2.0479106751736253e-05\n",
            "for batch9795 loss is 0.6088736653327942\n",
            "for batch9796 loss is 0.058574378490448\n",
            "for batch9797 loss is 0.010015412233769894\n",
            "for batch9798 loss is 0.12614010274410248\n",
            "for batch9799 loss is 3.3616765904298518e-06\n",
            "for batch9800 loss is 0.01851540617644787\n",
            "for batch9801 loss is 0.30760493874549866\n",
            "for batch9802 loss is 0.08548278361558914\n",
            "for batch9803 loss is 0.046391189098358154\n",
            "for batch9804 loss is 0.1464613378047943\n",
            "for batch9805 loss is 0.518494725227356\n",
            "for batch9806 loss is 0.15257896482944489\n",
            "for batch9807 loss is 0.002635990036651492\n",
            "for batch9808 loss is 0.0002651363902259618\n",
            "for batch9809 loss is 4.358012301963754e-05\n",
            "for batch9810 loss is 0.005147478077560663\n",
            "for batch9811 loss is 0.5585013628005981\n",
            "for batch9812 loss is 0.0003209505812264979\n",
            "for batch9813 loss is 0.09499166905879974\n",
            "for batch9814 loss is 0.005591400898993015\n",
            "for batch9815 loss is 0.023859329521656036\n",
            "for batch9816 loss is 0.20176371932029724\n",
            "for batch9817 loss is 3.073112020501867e-05\n",
            "for batch9818 loss is 0.03821590542793274\n",
            "for batch9819 loss is 0.14839646220207214\n",
            "for batch9820 loss is 2.0991785526275635\n",
            "for batch9821 loss is 0.009664610028266907\n",
            "for batch9822 loss is 0.001417956780642271\n",
            "for batch9823 loss is 1.7512588500976562\n",
            "for batch9824 loss is 0.0009751872275955975\n",
            "for batch9825 loss is 0.17077484726905823\n",
            "for batch9826 loss is 0.06163908168673515\n",
            "for batch9827 loss is 0.9023023843765259\n",
            "for batch9828 loss is 0.07365307956933975\n",
            "for batch9829 loss is 0.14824600517749786\n",
            "for batch9830 loss is 0.7167137265205383\n",
            "for batch9831 loss is 9.794680227059871e-05\n",
            "for batch9832 loss is 0.188798189163208\n",
            "for batch9833 loss is 0.005131208803504705\n",
            "for batch9834 loss is 0.0008703058701939881\n",
            "for batch9835 loss is 2.596284684841521e-05\n",
            "for batch9836 loss is 3.0994203825684963e-06\n",
            "for batch9837 loss is 0.8810404539108276\n",
            "for batch9838 loss is 0.023567374795675278\n",
            "for batch9839 loss is 0.0014366297982633114\n",
            "for batch9840 loss is 0.17264506220817566\n",
            "for batch9841 loss is 0.14257293939590454\n",
            "for batch9842 loss is 0.02848474681377411\n",
            "for batch9843 loss is 0.05964209884405136\n",
            "for batch9844 loss is 0.2127852886915207\n",
            "for batch9845 loss is 0.23440547287464142\n",
            "for batch9846 loss is 0.08292052894830704\n",
            "for batch9847 loss is 2.4318640043929918e-06\n",
            "for batch9848 loss is 0.000475404696771875\n",
            "for batch9849 loss is 0.0026716208085417747\n",
            "for batch9850 loss is 0.006155874114483595\n",
            "for batch9851 loss is 0.0014272838598117232\n",
            "for batch9852 loss is 0.027442550286650658\n",
            "for batch9853 loss is 6.675709300907329e-07\n",
            "for batch9854 loss is 1.6845271587371826\n",
            "for batch9855 loss is 3.5762758443524945e-07\n",
            "for batch9856 loss is 0.007575704716145992\n",
            "for batch9857 loss is 0.03811692073941231\n",
            "for batch9858 loss is 0.7989232540130615\n",
            "for batch9859 loss is 0.002247657161206007\n",
            "for batch9860 loss is 0.11708378791809082\n",
            "for batch9861 loss is 0.3738803267478943\n",
            "for batch9862 loss is 0.014275391586124897\n",
            "for batch9863 loss is 0.02258322946727276\n",
            "for batch9864 loss is 0.10569236427545547\n",
            "for batch9865 loss is 0.04136919602751732\n",
            "for batch9866 loss is 0.04459042102098465\n",
            "for batch9867 loss is 0.23293869197368622\n",
            "for batch9868 loss is 0.046599131077528\n",
            "for batch9869 loss is 0.00201911898329854\n",
            "for batch9870 loss is 0.15957382321357727\n",
            "for batch9871 loss is 1.2874572803411866e-06\n",
            "for batch9872 loss is 0.002893016906455159\n",
            "for batch9873 loss is 0.0013008634559810162\n",
            "for batch9874 loss is 1.8292052745819092\n",
            "for batch9875 loss is 0.593018651008606\n",
            "for batch9876 loss is 0.15847919881343842\n",
            "for batch9877 loss is 3.838429256575182e-05\n",
            "for batch9878 loss is 0.008172838017344475\n",
            "for batch9879 loss is 0.16834716498851776\n",
            "for batch9880 loss is 0.0005452823243103921\n",
            "for batch9881 loss is 0.0017878093058243394\n",
            "for batch9882 loss is 0.18652942776679993\n",
            "for batch9883 loss is 0.1144278421998024\n",
            "for batch9884 loss is 2.7988735382677987e-05\n",
            "for batch9885 loss is 0.8092666864395142\n",
            "for batch9886 loss is 0.06015615910291672\n",
            "for batch9887 loss is 0.0014966126764193177\n",
            "for batch9888 loss is 0.1814822256565094\n",
            "for batch9889 loss is 0.30070486664772034\n",
            "for batch9890 loss is 0.0003459830186329782\n",
            "for batch9891 loss is 0.11338675022125244\n",
            "for batch9892 loss is 0.0800277441740036\n",
            "for batch9893 loss is 0.008259560912847519\n",
            "for batch9894 loss is 0.1013006940484047\n",
            "for batch9895 loss is 0.0033123581670224667\n",
            "for batch9896 loss is 0.0002723848738241941\n",
            "for batch9897 loss is 0.00569408293813467\n",
            "for batch9898 loss is 0.004839456174522638\n",
            "for batch9899 loss is 0.04133377596735954\n",
            "for batch9900 loss is 0.4324992299079895\n",
            "for batch9901 loss is 0.01172754168510437\n",
            "for batch9902 loss is 0.7369875311851501\n",
            "for batch9903 loss is 0.12179426848888397\n",
            "for batch9904 loss is 0.31105440855026245\n",
            "for batch9905 loss is 0.1541970670223236\n",
            "for batch9906 loss is 0.03234364092350006\n",
            "for batch9907 loss is 8.292864367831498e-05\n",
            "for batch9908 loss is 3.3781059755710885e-05\n",
            "for batch9909 loss is 0.001302501535974443\n",
            "for batch9910 loss is 0.23899176716804504\n",
            "for batch9911 loss is 1.041793704032898\n",
            "for batch9912 loss is 0.8257572054862976\n",
            "for batch9913 loss is 0.04863422363996506\n",
            "for batch9914 loss is 0.24494752287864685\n",
            "for batch9915 loss is 0.002885192632675171\n",
            "for batch9916 loss is 0.49203816056251526\n",
            "for batch9917 loss is 0.07138369977474213\n",
            "for batch9918 loss is 0.011747503653168678\n",
            "for batch9919 loss is 0.1517712026834488\n",
            "for batch9920 loss is 0.0\n",
            "for batch9921 loss is 7.742988236714154e-05\n",
            "for batch9922 loss is 0.06823950260877609\n",
            "for batch9923 loss is 0.05456848070025444\n",
            "for batch9924 loss is 0.00011083858407801017\n",
            "for batch9925 loss is 0.23608942329883575\n",
            "for batch9926 loss is 0.23057985305786133\n",
            "for batch9927 loss is 0.08051981031894684\n",
            "for batch9928 loss is 0.08220003545284271\n",
            "for batch9929 loss is 0.01646188274025917\n",
            "for batch9930 loss is 0.03941971808671951\n",
            "for batch9931 loss is 0.26711076498031616\n",
            "for batch9932 loss is 0.10302482545375824\n",
            "for batch9933 loss is 0.3171267509460449\n",
            "for batch9934 loss is 0.11138508468866348\n",
            "for batch9935 loss is 0.01919516548514366\n",
            "for batch9936 loss is 0.3220251798629761\n",
            "for batch9937 loss is 0.0014791472349315882\n",
            "for batch9938 loss is 0.00024254713207483292\n",
            "for batch9939 loss is 0.7970006465911865\n",
            "for batch9940 loss is 0.008515942841768265\n",
            "for batch9941 loss is 0.0006663893582299352\n",
            "for batch9942 loss is 0.00363263045437634\n",
            "for batch9943 loss is 0.26300570368766785\n",
            "for batch9944 loss is 0.0021375019568949938\n",
            "for batch9945 loss is 0.2658378481864929\n",
            "for batch9946 loss is 0.054375339299440384\n",
            "for batch9947 loss is 0.152836412191391\n",
            "for batch9948 loss is 0.2983303964138031\n",
            "for batch9949 loss is 8.640806481707841e-05\n",
            "for batch9950 loss is 0.0051493095234036446\n",
            "for batch9951 loss is 0.24331140518188477\n",
            "for batch9952 loss is 0.009286440908908844\n",
            "for batch9953 loss is 0.13460364937782288\n",
            "for batch9954 loss is 0.311817467212677\n",
            "for batch9955 loss is 1.199735164642334\n",
            "for batch9956 loss is 0.0021909642964601517\n",
            "for batch9957 loss is 0.015134970657527447\n",
            "for batch9958 loss is 9.887562919175252e-05\n",
            "for batch9959 loss is 0.000550657685380429\n",
            "for batch9960 loss is 0.28744247555732727\n",
            "for batch9961 loss is 1.661738133407198e-05\n",
            "for batch9962 loss is 0.17700675129890442\n",
            "for batch9963 loss is 0.03842237591743469\n",
            "for batch9964 loss is 9.48889100982342e-06\n",
            "for batch9965 loss is 0.006648990325629711\n",
            "for batch9966 loss is 2.8215126991271973\n",
            "for batch9967 loss is 0.008959723636507988\n",
            "for batch9968 loss is 0.0028519732877612114\n",
            "for batch9969 loss is 0.00030645806691609323\n",
            "for batch9970 loss is 0.009451490826904774\n",
            "for batch9971 loss is 0.10431692749261856\n",
            "for batch9972 loss is 0.0817861258983612\n",
            "for batch9973 loss is 4.291508048481774e-06\n",
            "for batch9974 loss is 0.07273055613040924\n",
            "for batch9975 loss is 0.0007796756108291447\n",
            "for batch9976 loss is 0.06809458136558533\n",
            "for batch9977 loss is 0.12421043962240219\n",
            "for batch9978 loss is 0.15326222777366638\n",
            "for batch9979 loss is 0.06342019140720367\n",
            "for batch9980 loss is 2.8610219260372105e-07\n",
            "for batch9981 loss is 0.021903622895479202\n",
            "for batch9982 loss is 0.11314525455236435\n",
            "for batch9983 loss is 0.5350164175033569\n",
            "for batch9984 loss is 0.003522707847878337\n",
            "for batch9985 loss is 0.0012333601480349898\n",
            "for batch9986 loss is 4.327062197262421e-05\n",
            "for batch9987 loss is 0.004575012717396021\n",
            "for batch9988 loss is 0.18397216498851776\n",
            "for batch9989 loss is 1.3397845029830933\n",
            "for batch9990 loss is 1.1038522643502802e-05\n",
            "for batch9991 loss is 0.011070597916841507\n",
            "for batch9992 loss is 0.07324326783418655\n",
            "for batch9993 loss is 0.017958203330636024\n",
            "for batch9994 loss is 0.765584409236908\n",
            "for batch9995 loss is 0.2269906997680664\n",
            "for batch9996 loss is 0.032003216445446014\n",
            "for batch9997 loss is 0.22284097969532013\n",
            "for batch9998 loss is 0.015355666168034077\n",
            "for batch9999 loss is 0.0031316063832491636\n",
            "Finished Training\n"
          ]
        }
      ],
      "source": [
        "epoch_nums = 70\n",
        "trainloader = train_loader\n",
        "for epoch in range(epoch_nums):  # loop over the dataset multiple times\n",
        "\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        ## FILL HERE\n",
        "        images, labels = data\n",
        "        outputs = net(images)\n",
        "        loss_op = loss(outputs, labels)\n",
        "        optimizer.zero_grad()\n",
        "        loss_op.backward()\n",
        "        optimizer.step()\n",
        "        ## You should train the model and also print the running loss for each batch\n",
        "        print(f'for batch{i} loss is {loss_op}')\n",
        "print('Finished Training')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14558277",
      "metadata": {
        "id": "14558277"
      },
      "source": [
        "## Test the network on the test data\n",
        "\n",
        "In the following section, you should test your network and calculate the accuracy of your model on the test dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb0f4803",
      "metadata": {
        "id": "fb0f4803",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d578034d-61ad-4717-ce51-f0bd5bb8d864"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the 10000 test images: 71 %\n"
          ]
        }
      ],
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "# since we're not training, we don't need to calculate the gradients for our outputs\n",
        "testloader = test_loader\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        ## FILL HERE\n",
        "        images, labels = data\n",
        "        outputs = net(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a90c190",
      "metadata": {
        "id": "9a90c190"
      },
      "source": [
        "Here, we also calculate the model performance for each class as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8afd6a24",
      "metadata": {
        "id": "8afd6a24"
      },
      "outputs": [],
      "source": [
        "# prepare to count predictions for each class\n",
        "correct_pred = {classname: 0 for classname in classes}\n",
        "total_pred = {classname: 0 for classname in classes}\n",
        "\n",
        "# again no gradients needed\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        ## FILL HERE\n",
        "        images, labels = data\n",
        "        outputs = net(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "# print accuracy for each class\n",
        "for classname, correct_count in correct_pred.items():\n",
        "    accuracy = 100 * float(correct_count) / total_pred[classname]\n",
        "    print(f'Accuracy for class: {classname:5s} is {accuracy:.1f} %')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}